
@article{sharma_review_2023,
	title = {A {Review} of {Physics}-{Informed} {Machine} {Learning} in {Fluid} {Mechanics}},
	volume = {16},
	copyright = {Review},
	issn = {1996-1073},
	shorttitle = {{PINN} {Fluid} {Mechanics}},
	url = {https://www.mdpi.com/1996-1073/16/5/2343},
	doi = {10.3390/en16052343},
	abstract = {Physics-informed machine-learning (PIML) enables the integration of domain knowledge with machine learning (ML) algorithms, which results in higher data efﬁciency and more stable predictions. This provides opportunities for augmenting—and even replacing—high-ﬁdelity numerical simulations of complex turbulent ﬂows, which are often expensive due to the requirement of high temporal and spatial resolution. In this review, we (i) provide an introduction and historical perspective of ML methods, in particular neural networks (NN), (ii) examine existing PIML applications to ﬂuid mechanics problems, especially in complex high Reynolds number ﬂows, (iii) demonstrate the utility of PIML techniques through a case study, and (iv) discuss the challenges and opportunities of developing PIML for ﬂuid mechanics.},
	language = {en},
	number = {5},
	urldate = {2023-04-28},
	journal = {Energies},
	author = {Sharma, Pushan and Chung, Wai Tong and Akoush, Bassem and Ihme, Matthias},
	month = feb,
	year = {2023},
	pages = {2343},
	file = {Sharma et al. - 2023 - A Review of Physics-Informed Machine Learning in F.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/MJMFUEHX/Sharma et al. - 2023 - A Review of Physics-Informed Machine Learning in F.pdf:application/pdf},
}

@article{lawal_physics-informed_2022,
	title = {Physics-{Informed} {Neural} {Network} ({PINN}) {Evolution} and {Beyond}: {A} {Systematic} {Literature} {Review} and {Bibliometric} {Analysis}},
	volume = {6},
	copyright = {Evolution and Beyond},
	issn = {2504-2289},
	shorttitle = {{PINN} evolution},
	url = {https://www.mdpi.com/2504-2289/6/4/140},
	doi = {10.3390/bdcc6040140},
	abstract = {This research aims to study and assess state-of-the-art physics-informed neural networks (PINNs) from different researchers’ perspectives. The PRISMA framework was used for a systematic literature review, and 120 research articles from the computational sciences and engineering domain were speciﬁcally classiﬁed through a well-deﬁned keyword search in Scopus and Web of Science databases. Through bibliometric analyses, we have identiﬁed journal sources with the most publications, authors with high citations, and countries with many publications on PINNs. Some newly improved techniques developed to enhance PINN performance and reduce high training costs and slowness, among other limitations, have been highlighted. Different approaches have been introduced to overcome the limitations of PINNs. In this review, we categorized the newly proposed PINN methods into Extended PINNs, Hybrid PINNs, and Minimized Loss techniques. Various potential future research directions are outlined based on the limitations of the proposed solutions.},
	language = {en},
	number = {4},
	urldate = {2023-04-28},
	journal = {BDCC},
	author = {Lawal, Zaharaddeen Karami and Yassin, Hayati and Lai, Daphne Teck Ching and Che Idris, Azam},
	month = nov,
	year = {2022},
	pages = {140},
	file = {Lawal et al. - 2022 - Physics-Informed Neural Network (PINN) Evolution a.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/J952V88S/Lawal et al. - 2022 - Physics-Informed Neural Network (PINN) Evolution a.pdf:application/pdf},
}

@article{raissi_physics_2017,
	title = {Physics {Informed} {Deep} {Learning} ({Part} {I}): {Data}-driven {Solutions} of {Nonlinear} {Partial} {Diﬀerential} {Equations}},
	shorttitle = {{PINN} {I}},
	url = {https://arxiv.org/abs/1711.10561},
	language = {en},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	year = {2017},
	note = {Sources: https://github.com/maziarraissi/PINNs},
	file = { REM Raissi et al. - Physics Informed Deep Learning (Part I) Data-driv.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/D9K9B8Y4/ REM Raissi et al. - Physics Informed Deep Learning (Part I) Data-driv.pdf:application/pdf;Raissi et al. - 2017 - Physics Informed Deep Learning (Part I) Data-driv.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/NA3JBQH9/1711.10561.pdf:application/pdf},
}

@article{raissi_physics_2017-1,
	title = {Physics {Informed} {Deep} {Learning} ({Part} {II}): {Data}-driven {Discovery} of {Nonlinear} {Partial} {Diﬀerential} {Equations}},
	shorttitle = {{PINN} {II}},
	url = {https://arxiv.org/abs/1711.10566},
	language = {en},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	year = {2017},
	file = {Raissi et al. - Physics Informed Deep Learning (Part II) Data-dri.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/PX7MPUSK/Raissi et al. - Physics Informed Deep Learning (Part II) Data-dri.pdf:application/pdf},
}

@article{raissi_cs598_2021,
	title = {{CS598}: {Physics}-{Informed} {Neural} {Networks}:  {A} deep learning framework for solving forward and inverse problems involving nonlinear {PDEs}},
	shorttitle = {{PINN} slides},
	url = {https://arindam.cs.illinois.edu/courses/f21cs598/slides/pml11_598f21.pdf},
	language = {en},
	author = {Raissi, M and Perdikaris, P and Karniadakis, GE},
	year = {2021},
	file = {Raissi et al. - CS598 Physics-Informed Neural Networks  A deep l.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/Y7E7BZYH/Raissi et al. - CS598 Physics-Informed Neural Networks  A deep l.pdf:application/pdf},
}

@article{cuomo_scientific_2022,
	title = {Scientific {Machine} {Learning} {Through} {Physics}–{Informed} {Neural} {Networks}: {Where} we are and {What}’s {Next}},
	volume = {92},
	issn = {0885-7474, 1573-7691},
	shorttitle = {{PINN} review},
	url = {https://link.springer.com/10.1007/s10915-022-01939-z},
	doi = {10.1007/s10915-022-01939-z},
	abstract = {Physics-Informed Neural Networks (PINN) are neural networks (NNs) that encode model equations, like Partial Differential Equations (PDE), as a component of the neural network itself. PINNs are nowadays used to solve PDEs, fractional equations, integral-differential equations, and stochastic PDEs. This novel methodology has arisen as a multi-task learning framework in which a NN must ﬁt observed data while reducing a PDE residual. This article provides a comprehensive review of the literature on PINNs: while the primary goal of the study was to characterize these networks and their related advantages and disadvantages. The review also attempts to incorporate publications on a broader range of collocation-based physics informed neural networks, which stars form the vanilla PINN, as well as many other variants, such as physics-constrained neural networks (PCNN), variational hp-VPINN, and conservative PINN (CPINN). The study indicates that most research has focused on customizing the PINN through different activation functions, gradient optimization techniques, neural network structures, and loss function structures. Despite the wide range of applications for which PINNs have been used, by demonstrating their ability to be more feasible in some contexts than classical numerical techniques like Finite Element Method (FEM), advancements are still possible, most notably theoretical issues that remain unresolved.},
	language = {en},
	number = {3},
	urldate = {2023-04-28},
	journal = {J Sci Comput},
	author = {Cuomo, Salvatore and Di Cola, Vincenzo Schiano and Giampaolo, Fabio and Rozza, Gianluigi and Raissi, Maziar and Piccialli, Francesco},
	month = sep,
	year = {2022},
	pages = {88},
	file = {Cuomo et al. - 2022 - Scientific Machine Learning Through Physics–Inform.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/BCTXFUNL/Cuomo et al. - 2022 - Scientific Machine Learning Through Physics–Inform.pdf:application/pdf},
}

@article{sacchetti_neural_2022,
	title = {Neural {Networks} to {Solve} {Partial} {Differential} {Equations}: {A} {Comparison} {With} {Finite} {Elements}},
	volume = {10},
	copyright = {Heat 2D},
	issn = {2169-3536},
	shorttitle = {{ANN} {PDE} {FE} comparison},
	url = {https://ieeexplore.ieee.org/document/9737092/},
	doi = {10.1109/ACCESS.2022.3160186},
	abstract = {We compare the Finite Element Method (FEM) simulation of a standard Partial Differential Equation thermal problem of a plate with a hole with a Neural Network (NN) simulation. The largest deviation from the true solution obtained from FEM (0.015 for a solution on the order of unity) is easily achieved with NN too without much tuning of the hyperparameters. Accuracies below 0.01 instead require reﬁnement with an alternative optimizer to reach a similar performance with NN. A rough comparison between the Floating Point Operations values, as a machine-independent quantiﬁcation of the computational performance, suggests a signiﬁcant difference between FEM and NN in favour of the former. This also strongly holds for computation time: for an accuracy on the order of 10−5, FEM and NN require 54 and 1100 seconds, respectively. A detailed analysis of the effect of varying different hyperparameters shows that accuracy and computational time only weakly depend on the major part of them. Accuracies below 0.01 cannot be achieved with the ‘‘adam’’ optimizers and it looks as though accuracies below 10−5 cannot be achieved at all. In conclusion, the present work shows that for the concrete case of solving a steady-state 2D heat equation, the performance of a FEM algorithm is signiﬁcantly better than the solution via networks.},
	language = {en},
	urldate = {2023-04-28},
	journal = {IEEE Access},
	author = {Sacchetti, Andrea and Bachmann, Benjamin and Loffel, Kaspar and Kunzi, Urs-Martin and Paoli, Beatrice},
	year = {2022},
	pages = {32271--32279},
	file = {Sacchetti et al. - 2022 - Neural Networks to Solve Partial Differential Equa.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/KBYQEYSC/Sacchetti et al. - 2022 - Neural Networks to Solve Partial Differential Equa.pdf:application/pdf},
}

@misc{he_unsupervised_2020,
	title = {An unsupervised learning approach to solving heat equations on chip based on {Auto} {Encoder} and {Image} {Gradient}},
	copyright = {Heat 3D},
	shorttitle = {{PINN} on chip},
	url = {http://arxiv.org/abs/2007.09684},
	abstract = {Solving heat transfer equations on chip becomes very critical in the upcoming 5G and AI chip-package-systems. However, batches of simulations have to be performed for data driven supervised machine learning models. Data driven methods are data hungry, to address this, Physics Informed Neural Networks (PINN) have been proposed. However, vanilla PINN models solve one ﬁxed heat equation at a time, so the models have to be retrained for heat equations with diﬀerent source terms. Additionally, issues related to multi-objective optimization have to be resolved while using PINN to minimize the PDE residual, satisfy boundary conditions and ﬁt the observed data etc. Therefore, this paper investigates an unsupervised learning approach for solving heat transfer equations on chip without using solution data and generalizing the trained network for predicting solutions for heat equations with unseen source terms. Speciﬁcally, a hybrid framework of Auto Encoder (AE) and Image Gradient (IG) based network is designed. The AE is used to encode diﬀerent source terms of the heat equations. The IG based network implements a second order central diﬀerence algorithm for structured grids and minimizes the PDE residual. The eﬀectiveness of the designed network is evaluated by solving heat equations for various use cases. It is proved that with limited number of source terms to train the AE network, the framework can not only solve the given heat transfer problems with a single training process, but also make reasonable predictions for unseen cases (heat equations with new source terms) without retraining.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {He, Haiyang and Pathak, Jay},
	month = jul,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Physics - Applied Physics, Physics - Computational Physics, Statistics - Machine Learning},
	file = {He and Pathak - 2020 - An unsupervised learning approach to solving heat .pdf:/home/x/Documents/inpe/library-ml-zotero/storage/BJK29L52/He and Pathak - 2020 - An unsupervised learning approach to solving heat .pdf:application/pdf},
}

@misc{ma_heat_2022,
	title = {Heat {Conduction} {Plate} {Layout} {Optimization} using {Physics}-driven {Convolutional} {Neural} {Networks}},
	shorttitle = {Heat {PINN} {Convolutional}},
	url = {http://arxiv.org/abs/2201.10002},
	abstract = {The layout optimization of the heat conduction is essential during design in engineering, especially for thermal sensible products. When the optimization algorithm iteratively evaluates different loading cases, the traditional numerical simulation methods used usually lead to a substantial computational cost. To effectively reduce the computational effort, data-driven approaches are used to train a surrogate model as a mapping between the prescribed external loads and various geometry. However, the existing model are trained by data-driven methods which requires intensive training samples that from numerical simulations and not really effectively solve the problem. Choosing the steady heat conduction problems as examples, this paper proposes a Physics-driven Convolutional Neural Networks (PD-CNN) method to infer the physical field solutions for random varied loading cases. After that, the Particle Swarm Optimization (PSO) algorithm is used to optimize the sizes and the positions of the hole masks in the prescribed design domain, and the average temperature value of the entire heat conduction field is minimized, and the goal of minimizing heat transfer is achieved. Compared with the existing data-driven approaches, the proposed PD-CNN optimization framework not only predict field solutions that are highly consistent with conventional simulation results, but also generate the solution space with without any pre-obtained training data.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Ma, Hao and Sun, Yang and Chiarelli, Mario},
	month = jan,
	year = {2022},
	keywords = {Computer Science - Machine Learning, Computer Science - Computational Engineering, Finance, and Science},
	file = {Ma et al. - 2022 - Heat Conduction Plate Layout Optimization using Ph.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/DATU86B9/Ma et al. - 2022 - Heat Conduction Plate Layout Optimization using Ph.pdf:application/pdf},
}

@article{liu_novel_2022,
	title = {A novel meta-learning initialization method for physics-informed neural networks},
	volume = {34},
	copyright = {With heat sources},
	issn = {0941-0643, 1433-3058},
	shorttitle = {Initialization method for {PINN}},
	url = {https://link.springer.com/10.1007/s00521-022-07294-2},
	doi = {10.1007/s00521-022-07294-2},
	abstract = {Physics-informed neural networks (PINNs) have been widely used to solve various scientiﬁc computing problems. However, large training costs limit PINNs for some real-time applications. Although some works have been proposed to improve the training eﬃciency of PINNs, few consider the inﬂuence of initialization. To this end, we propose a New Reptile initialization based Physics-Informed Neural Network (NRPINN). The original Reptile algorithm is a meta-learning initialization method based on labeled data. PINNs can be trained with less labeled data or even without any labeled data by adding partial diﬀerential equations (PDEs) as a penalty term into the loss function. Inspired by this idea, we propose the new Reptile initialization to sample more tasks from the parameterized PDEs and adapt the penalty term of the loss. The new Reptile initialization can acquire initialization parameters from related tasks by supervised, unsupervised, and semi-supervised learning. Then, PINNs with initialization parameters can eﬃciently solve PDEs. Besides, the new Reptile initialization can also be used for the variants of PINNs. Finally, we demonstrate and verify the NRPINN considering both forward problems, including solving Poisson, Burgers, and Schr¨odinger equations, as well as inverse problems, where unknown parameters in the PDEs are estimated. Experimental results show that the NRPINN training is much faster and achieves higher accuracy than PINNs with other initialization methods.},
	language = {en},
	number = {17},
	urldate = {2023-04-28},
	journal = {Neural Comput \& Applic},
	author = {Liu, Xu and Zhang, Xiaoya and Peng, Wei and Zhou, Weien and Yao, Wen},
	month = sep,
	year = {2022},
	pages = {14511--14534},
	file = {Liu et al. - 2022 - A novel meta-learning initialization method for ph.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/H9JU9N4Y/Liu et al. - 2022 - A novel meta-learning initialization method for ph.pdf:application/pdf},
}

@article{krasnopolsky_new_2005,
	title = {New {Approach} to {Calculation} of {Atmospheric} {Model} {Physics}: {Accurate} and {Fast} {Neural} {Network} {Emulation} of {Longwave} {Radiation} in a {Climate} {Model}},
	volume = {133},
	copyright = {Atmospheric},
	issn = {1520-0493, 0027-0644},
	shorttitle = {{NN} for {Atmospheric} {Radiation}},
	url = {http://journals.ametsoc.org/doi/10.1175/MWR2923.1},
	doi = {10.1175/MWR2923.1},
	abstract = {A new approach based on a synergetic combination of statistical/machine learning and deterministic modeling within atmospheric models is presented. The approach uses neural networks as a statistical or machine learning technique for an accurate and fast emulation or statistical approximation of model physics parameterizations. It is applied to development of an accurate and fast approximation of an atmospheric longwave radiation parameterization for the NCAR Community Atmospheric Model, which is the most time consuming component of model physics. The developed neural network emulation is two orders of magnitude, 50–80 times, faster than the original parameterization. A comparison of the parallel 10-yr climate simulations performed with the original parameterization and its neural network emulations confirmed that these simulations produce almost identical results. The obtained results show the conceptual and practical possibility of an efficient synergetic combination of deterministic and statistical learning components within an atmospheric climate or forecast model. A developmental framework and practical validation criteria for neural network emulations of model physics components are outlined.},
	language = {en},
	number = {5},
	urldate = {2023-04-28},
	journal = {Monthly Weather Review},
	author = {Krasnopolsky, Vladimir M. and Fox-Rabinovitz, Michael S. and Chalikov, Dmitry V.},
	month = may,
	year = {2005},
	pages = {1370--1383},
	file = {Krasnopolsky et al. - 2005 - New Approach to Calculation of Atmospheric Model P.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/6UL498IT/Krasnopolsky et al. - 2005 - New Approach to Calculation of Atmospheric Model P.pdf:application/pdf},
}

@article{dueben_challenges_2018,
	title = {Challenges and design choices for global weather and climate models based on machine learning},
	volume = {11},
	issn = {1991-9603},
	shorttitle = {Climate model \& {ML}},
	url = {https://gmd.copernicus.org/articles/11/3999/2018/},
	doi = {10.5194/gmd-11-3999-2018},
	abstract = {Can models that are based on deep learning and trained on atmospheric data compete with weather and climate models that are based on physical principles and the basic equations of motion? This question has been asked often recently due to the boom in deep-learning techniques. The question is valid given the huge amount of data that are available, the computational efﬁciency of deep-learning techniques and the limitations of today’s weather and climate models in particular with respect to resolution and complexity.},
	language = {en},
	number = {10},
	urldate = {2023-04-28},
	journal = {Geosci. Model Dev.},
	author = {Dueben, Peter D. and Bauer, Peter},
	month = oct,
	year = {2018},
	pages = {3999--4009},
	file = {Dueben and Bauer - 2018 - Challenges and design choices for global weather a.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/UVGWDATR/Dueben and Bauer - 2018 - Challenges and design choices for global weather a.pdf:application/pdf},
}

@article{sonnewald_bridging_2021,
	title = {Bridging observations, theory and numerical simulation of the ocean using machine learning},
	volume = {16},
	issn = {1748-9326},
	shorttitle = {Ocean {ML}},
	url = {https://iopscience.iop.org/article/10.1088/1748-9326/ac0eb0},
	doi = {10.1088/1748-9326/ac0eb0},
	abstract = {Progress within physical oceanography has been concurrent with the increasing sophistication of tools available for its study. The incorporation of machine learning (ML) techniques offers exciting possibilities for advancing the capacity and speed of established methods and for making substantial and serendipitous discoveries. Beyond vast amounts of complex data ubiquitous in many modern scientific fields, the study of the ocean poses a combination of unique challenges that ML can help address. The observational data available is largely spatially sparse, limited to the surface, and with few time series spanning more than a handful of decades. Important timescales span seconds to millennia, with strong scale interactions and numerical modelling efforts complicated by details such as coastlines. This review covers the current scientific insight offered by applying ML and points to where there is imminent potential. We cover the main three branches of the field: observations, theory, and numerical modelling. Highlighting both challenges and opportunities, we discuss both the historical context and salient ML tools. We focus on the use of ML in situ sampling and satellite observations, and the extent to which ML applications can advance theoretical oceanographic exploration, as well as aid numerical simulations. Applications that are also covered include model error and bias correction and current and potential use within data assimilation. While not without risk, there is great interest in the potential benefits of oceanographic ML applications; this review caters to this interest within the research community.},
	language = {en},
	number = {7},
	urldate = {2023-04-28},
	journal = {Environ. Res. Lett.},
	author = {Sonnewald, Maike and Lguensat, Redouane and Jones, Daniel C and Dueben, Peter D and Brajard, Julien and Balaji, V},
	month = jul,
	year = {2021},
	pages = {073008},
	file = {Sonnewald et al. - 2021 - Bridging observations, theory and numerical simula.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/46AGWTT9/Sonnewald et al. - 2021 - Bridging observations, theory and numerical simula.pdf:application/pdf},
}

@article{basdevant_spectral_1986,
	title = {Spectral and finite difference solutions of the {Burgers} equation},
	volume = {14},
	copyright = {Theory},
	issn = {00457930},
	shorttitle = {Spectral {Burgers}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0045793086900368},
	doi = {10.1016/0045-7930(86)90036-8},
	abstract = {Spectral methods (FourierGalerkin, Fourier pseudospectral,ChebyshevTau, Chebyshev collocation, spectral element) and standard finite differences are applied to solve the Burgers equation with small viscofity ({\textasciitilde} = 1/100=). This equation admits a (nonsingular) thin internal layer that must be resolvedif accurate numerical solutions are to be obtained. From the reported computations, it appears that spectral schemes offer the best accuracy, especiallyif coordinate transformation or elemental subdivision is used to resolve the regions of large variation of the dependent variable.},
	language = {en},
	number = {1},
	urldate = {2023-04-28},
	journal = {Computers \& Fluids},
	author = {Basdevant, C and Deville, M and Haldenwang, P and Lacroix, J.M and Ouazzani, J and Peyret, R and Orlandi, P and Patera, A.T},
	month = jan,
	year = {1986},
	pages = {23--41},
	file = {Basdevant et al. - 1986 - Spectral and finite difference solutions of the Bu.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/TT9RYGEB/Basdevant et al. - 1986 - Spectral and finite difference solutions of the Bu.pdf:application/pdf;Basdevant et al. - 1986 - Spectral and finite difference solutions of the Bu.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/88DZKSUP/Basdevant et al. - 1986 - Spectral and finite difference solutions of the Bu.pdf:application/pdf;ScienceDirect Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/2YQLYTFM/0045793086900368.html:text/html;ScienceDirect Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/3QRXWTL7/0045793086900368.html:text/html},
}

@article{lee_neural_1990,
	title = {Neural algorithm for solving differential equations},
	volume = {91},
	issn = {00219991},
	shorttitle = {Neural algorithm for {ODE}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/002199919090007N},
	doi = {10.1016/0021-9991(90)90007-N},
	language = {en},
	number = {1},
	urldate = {2023-04-28},
	journal = {Journal of Computational Physics},
	author = {Lee, Hyuk and Kang, In Seok},
	month = nov,
	year = {1990},
	pages = {110--131},
	file = {Lee and Kang - 1990 - Neural algorithm for solving differential equation.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/NRM9AAWR/Lee and Kang - 1990 - Neural algorithm for solving differential equation.pdf:application/pdf},
}

@article{lagaris_neural-network_2000,
	title = {Neural-network methods for boundary value problems with irregular boundaries},
	volume = {11},
	issn = {10459227},
	shorttitle = {{ANN} \& boundaries},
	url = {http://ieeexplore.ieee.org/document/870037/},
	doi = {10.1109/72.870037},
	abstract = {Partial differential equations (PDEs) with boundary conditions (Dirichlet or Neumann) defined on boundaries with simple geometry have been successfully treated using sigmoidal multilayer perceptrons in previous works. This article deals with the case of complex boundary geometry, where the boundary is determined by a number of points that belong to it and are closely located, so as to offer a reasonable representation. Two networks are employed: a multilayer perceptron and a radial basis function network. The later is used to account for the exact satisfaction of the boundary conditions. The method has been successfully tested on two-dimensional and three-dimensional PDEs and has yielded accurate results.},
	language = {en},
	number = {5},
	urldate = {2023-04-28},
	journal = {IEEE Trans. Neural Netw.},
	author = {Lagaris, I.E. and Likas, A.C. and Papageorgiou, D.G.},
	month = sep,
	year = {2000},
	pages = {1041--1049},
	file = {Lagaris et al. - 2000 - Neural-network methods for boundary value problems.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/W59PPKV5/Lagaris et al. - 2000 - Neural-network methods for boundary value problems.pdf:application/pdf},
}

@article{baymani_artificial_2010,
	title = {Artificial {Neural} {Networks} {Approach} for {Solving} {Stokes} {Problem}},
	volume = {01},
	issn = {2152-7385, 2152-7393},
	shorttitle = {{PINN} {Stokes} problem},
	url = {http://www.scirp.org/journal/doi.aspx?DOI=10.4236/am.2010.14037},
	doi = {10.4236/am.2010.14037},
	abstract = {In this paper a new method based on neural network has been developed for obtaining the solution of the Stokes problem. We transform the mixed Stokes problem into three independent Poisson problems which by solving them the solution of the Stokes problem is obtained. The results obtained by this method, has been compared with the existing numerical method and with the exact solution of the problem. It can be observed that the current new approximation has higher accuracy. The number of model parameters required is less than conventional methods. The proposed new method is illustrated by an example.},
	language = {en},
	number = {04},
	urldate = {2023-04-28},
	journal = {AM},
	author = {Baymani, Modjtaba and Kerayechian, Asghar and Effati, Sohrab},
	year = {2010},
	pages = {288--292},
	file = {Baymani et al. - 2010 - Artificial Neural Networks Approach for Solving St.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/PHN5H9ST/Baymani et al. - 2010 - Artificial Neural Networks Approach for Solving St.pdf:application/pdf},
}

@article{chiaramonte_solving_2013,
	title = {Solving differential equations using neural networks},
	shorttitle = {{ODE} using {ANN}},
	url = {http://cs229.stanford.edu/proj2013/ChiaramonteKiener-SolvingDifferentialEquationsUsingNeuralNetworks.pdf},
	language = {en},
	author = {Chiaramonte, M M and Kiener, M},
	year = {2013},
	file = {Chiaramonte and Kiener - 2013 - Solving differential equations using neural networ.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/785JUM55/Chiaramonte and Kiener - 2013 - Solving differential equations using neural networ.pdf:application/pdf},
}

@article{rudd_solving_2013,
	title = {Solving {Partial} {Differential} {Equations} {Using} {Artificial} {Neural} {Networks}},
	copyright = {Thesis},
	shorttitle = {{PDE} \& {ANN}},
	url = {https://dukespace.lib.duke.edu/dspace/handle/10161/8197},
	language = {en},
	author = {Rudd, Keith},
	year = {2013},
	file = {Rudd - 2013 - Solving Partial Differential Equations Using Artif.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/CWQXRGKL/Rudd - 2013 - Solving Partial Differential Equations Using Artif.pdf:application/pdf},
}

@article{abadi_tensorflow_2016,
	title = {{TensorFlow}: {A} system for large-scale machine learning},
	shorttitle = {Tensorflow library},
	url = {https://arxiv.org/abs/1605.08695},
	abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataﬂow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataﬂow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, generalpurpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives ﬂexibility to the application developer: whereas in previous “parameter server” designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataﬂow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
	language = {en},
	author = {Abadi, Martın and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	year = {2016},
	file = {Abadi et al. - TensorFlow A system for large-scale machine learn.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/3ZZSB442/Abadi et al. - TensorFlow A system for large-scale machine learn.pdf:application/pdf},
}

@article{sirignano_dgm_2018,
	title = {{DGM}: {A} deep learning algorithm for solving partial differential equations},
	volume = {375},
	copyright = {DGM algorithm},
	issn = {00219991},
	shorttitle = {{DGM} algorithm},
	url = {http://arxiv.org/abs/1708.07469},
	doi = {10.1016/j.jcp.2018.08.029},
	abstract = {High-dimensional PDEs have been a longstanding computational challenge. We propose to solve highdimensional PDEs by approximating the solution with a deep neural network which is trained to satisfy the diﬀerential operator, initial condition, and boundary conditions. Our algorithm is meshfree, which is key since meshes become infeasible in higher dimensions. Instead of forming a mesh, the neural network is trained on batches of randomly sampled time and space points. The algorithm is tested on a class of high-dimensional free boundary PDEs, which we are able to accurately solve in up to 200 dimensions. The algorithm is also tested on a high-dimensional Hamilton-Jacobi-Bellman PDE and Burgers’ equation. The deep learning algorithm approximates the general solution to the Burgers’ equation for a continuum of diﬀerent boundary conditions and physical conditions (which can be viewed as a high-dimensional space). We call the algorithm a “Deep Galerkin Method (DGM)” since it is similar in spirit to Galerkin methods, with the solution approximated by a neural network instead of a linear combination of basis functions. In addition, we prove a theorem regarding the approximation power of neural networks for a class of quasilinear parabolic PDEs.},
	language = {en},
	urldate = {2023-04-28},
	journal = {Journal of Computational Physics},
	author = {Sirignano, Justin and Spiliopoulos, Konstantinos},
	month = dec,
	year = {2018},
	keywords = {Statistics - Machine Learning, Mathematics - Numerical Analysis, Quantitative Finance - Computational Finance, Quantitative Finance - Mathematical Finance},
	pages = {1339--1364},
	file = {Sirignano and Spiliopoulos - 2018 - DGM A deep learning algorithm for solving partial.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/SY9JSX9Q/Sirignano and Spiliopoulos - 2018 - DGM A deep learning algorithm for solving partial.pdf:application/pdf},
}

@article{long_pde-net_2018,
	title = {{PDE}-{Net}: {Learning} {PDEs} from {Data}},
	shorttitle = {{PDENet} model},
	url = {https://arxiv.org/abs/1710.09668},
	abstract = {Partial differential equations (PDEs) play a prominent role in many disciplines of science and engineering. PDEs are commonly derived based on empirical observations. However, with the rapid development of sensors, computational power, and data storage in the past decade, huge quantities of data can be easily collected and efﬁciently stored. Such vast quantity of data offers new opportunities for data-driven discovery of physical laws. Inspired by the latest development of neural network designs in deep learning, we propose a new feed-forward deep network, called PDENet, to fulﬁll two objectives at the same time: to accurately predict dynamics of complex systems and to uncover the underlying hidden PDE models. Comparing with existing approaches, our approach has the most ﬂexibility by learning both differential operators and the nonlinear response function of the underlying PDE model. A special feature of the proposed PDE-Net is that all ﬁlters are properly constrained, which enables us to easily identify the governing PDE models while still maintaining the expressive and predictive power of the network. These constrains are carefully designed by fully exploiting the relation between the orders of differential operators and the orders of sum rules of ﬁlters (an important concept originated from wavelet theory). Numerical experiments show that the PDE-Net has the potential to uncover the hidden PDE of the observed dynamics, and predict the dynamical behavior for a relatively long time, even in a noisy environment.},
	language = {en},
	author = {Long, Zichao and Lu, Yiping and Ma, Xianzhong and Dong, Bin},
	year = {2018},
	file = {Long et al. - 2018 - PDE-Net Learning PDEs from Data.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/TRKVN9RK/Long et al. - 2018 - PDE-Net Learning PDEs from Data.pdf:application/pdf},
}

@article{raissi_physics-informed_2019,
	title = {Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	volume = {378},
	issn = {00219991},
	shorttitle = {Raissi inverse problems},
	url = {https://doi.org/10.1016/j.jcp.2018.10.045},
	doi = {10.1016/j.jcp.2018.10.045},
	language = {en},
	urldate = {2023-04-28},
	journal = {Journal of Computational Physics},
	author = {Raissi, M. and Perdikaris, P. and Karniadakis, G.E.},
	month = feb,
	year = {2019},
	pages = {686--707},
	file = {Raissi et al. - 2019 - Physics-informed neural networks A deep learning .pdf:/home/x/Documents/inpe/library-ml-zotero/storage/JRKAET65/Raissi et al. - 2019 - Physics-informed neural networks A deep learning .pdf:application/pdf},
}

@article{al-aradi_solving_2018,
	title = {Solving {Nonlinear} and {High}-{Dimensional} {Partial} {Differential} {Equations} via {Deep} {Learning}},
	copyright = {Book},
	shorttitle = {{PDE} {Book}},
	url = {http://utstat.toronto.edu/~ali/papers/PDEandDeepLearning.pdf},
	language = {en},
	author = {Al-Aradi, Ali and Correia, Adolfo and Naiff, Danilo and Jardim, Gabriel and Vargas, Fundacao Getulio and Saporito, Yuri},
	year = {2018},
	file = {Al-Aradi et al. - Solving Nonlinear and High-Dimensional Partial Dif.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/BIY4K6GY/Al-Aradi et al. - Solving Nonlinear and High-Dimensional Partial Dif.pdf:application/pdf},
}

@misc{dwivedi_distributed_2019,
	title = {Distributed physics informed neural network for data-efficient solution to partial differential equations},
	copyright = {Distributed PINN},
	shorttitle = {Distributed {PINN}},
	url = {http://arxiv.org/abs/1907.08967},
	abstract = {The physics informed neural network (PINN) is evolving as a viable method to solve partial diﬀerential equations. In the recent past PINNs have been successfully tested and validated to ﬁnd solutions to both linear and non-linear partial diﬀerential equations (PDEs). However, the literature lacks detailed investigation of PINNs in terms of their representation capability. In this work, we ﬁrst test the original PINN method in terms of its capability to represent a complicated function. Further, to address the shortcomings of the PINN architecture, we propose a novel distributed PINN, named DPINN. We ﬁrst perform a direct comparison of the proposed DPINN approach against PINN to solve a non-linear PDE (Burgers’ equation). We show that DPINN not only yields a more accurate solution to the Burgers’ equation, but it is found to be more data-eﬃcient as well. At last, we employ our novel DPINN to two-dimensional steady-state Navier-Stokes equation, which is a system of non-linear PDEs. To the best of the authors’ knowledge, this is the ﬁrst such attempt to directly solve the Navier-Stokes equation using a physics informed neural network.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Dwivedi, Vikas and Parashar, Nishant and Srinivasan, Balaji},
	month = jul,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics, Statistics - Machine Learning},
	file = {Dwivedi et al. - 2019 - Distributed physics informed neural network for da.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/HFANDIYD/Dwivedi et al. - 2019 - Distributed physics informed neural network for da.pdf:application/pdf},
}

@misc{hasan_learning_2019,
	title = {Learning {Partial} {Differential} {Equations} from {Data} {Using} {Neural} {Networks}},
	shorttitle = {{ODE} \& {ANN}},
	url = {http://arxiv.org/abs/1910.10262},
	abstract = {We develop a framework for estimating unknown partial differential equations (PDEs) from noisy data, using a deep learning approach. Given noisy samples of a solution to an unknown PDE, our method interpolates the samples using a neural network, and extracts the PDE by equating derivatives of the neural network approximation. Our method applies to PDEs which are linear combinations of user-deﬁned dictionary functions, and generalizes previous methods that only consider parabolic PDEs. We introduce a regularization scheme that prevents the function approximation from overﬁtting the data and forces it to be a solution of the underlying PDE. We validate the model on simulated data generated by the known PDEs and added Gaussian noise, and we study our method under different levels of noise. We also compare the error of our method with a Cramer-Rao lower bound for an ordinary differential equation (ODE). Our results indicate that our method outperforms other methods in estimating PDEs, especially in the low signal-to-noise (SNR) regime.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Hasan, Ali and Pereira, João M. and Ravier, Robert and Farsiu, Sina and Tarokh, Vahid},
	month = oct,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Hasan et al. - 2019 - Learning Partial Differential Equations from Data .pdf:/home/x/Documents/inpe/library-ml-zotero/storage/93LRSDEQ/Hasan et al. - 2019 - Learning Partial Differential Equations from Data .pdf:application/pdf},
}

@article{lu_deepxde_2021,
	title = {{DeepXDE}: {A} {Deep} {Learning} {Library} for {Solving} {Differential} {Equations}},
	volume = {63},
	copyright = {DeepXDE},
	issn = {0036-1445, 1095-7200},
	shorttitle = {{DeepXDE} {PINN} {Library} (1)},
	url = {https://epubs.siam.org/doi/10.1137/19M1274067},
	doi = {10.1137/19M1274067},
	abstract = {Deep learning has achieved remarkable success in diverse applications; however, its use in solving partial diﬀerential equations (PDEs) has emerged only recently. Here, we present an overview of physics-informed neural networks (PINNs), which embed a PDE into the loss of the neural network using automatic diﬀerentiation. The PINN algorithm is simple, and it can be applied to diﬀerent types of PDEs, including integro-diﬀerential equations, fractional PDEs, and stochastic PDEs. Moreover, from the implementation point of view, PINNs solve inverse problems as easily as forward problems. We propose a new residual-based adaptive reﬁnement (RAR) method to improve the training eﬃciency of PINNs. For pedagogical reasons, we compare the PINN algorithm to a standard ﬁnite element method. We also present a Python library for PINNs, DeepXDE, which is designed to serve both as an education tool to be used in the classroom as well as a research tool for solving problems in computational science and engineering. Speciﬁcally, DeepXDE can solve forward problems given initial and boundary conditions, as well as inverse problems given some extra measurements. DeepXDE supports complex-geometry domains based on the technique of constructive solid geometry, and enables the user code to be compact, resembling closely the mathematical formulation. We introduce the usage of DeepXDE and its customizability, and we also demonstrate the capability of PINNs and the user-friendliness of DeepXDE for ﬁve diﬀerent examples. More broadly, DeepXDE contributes to the more rapid development of the emerging Scientiﬁc Machine Learning ﬁeld.},
	language = {en},
	number = {1},
	urldate = {2023-04-28},
	journal = {SIAM Rev.},
	author = {Lu, Lu and Meng, Xuhui and Mao, Zhiping and Karniadakis, George Em},
	month = jan,
	year = {2021},
	pages = {208--228},
	file = {Lu et al. - 2021 - DeepXDE A Deep Learning Library for Solving Diffe.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/VEWH8LN3/Lu et al. - 2021 - DeepXDE A Deep Learning Library for Solving Diffe.pdf:application/pdf;Lu et al. - 2021 - DeepXDE A Deep Learning Library for Solving Diffe.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/DSHESTFT/Lu et al. - 2021 - DeepXDE A Deep Learning Library for Solving Diffe.pdf:application/pdf},
}

@article{lu_deepxde_2020,
	title = {{DeepXDE}: {A} {Deep} {Learning} {Library} for {Solving} {Forward} and {Inverse} {Differential} {Equations}},
	shorttitle = {{DeepXDE} {PINN} {Library} (2)},
	url = {https://lululxvi.github.io/files/talks/2020SIAMMDS_MS70.pdf},
	language = {en},
	journal = {Opt Express},
	author = {Lu, Lu},
	year = {2020},
	file = {2020 DeepXDE (slides) - Lu.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/B9FJSCVJ/2020 DeepXDE (slides) - Lu.pdf:application/pdf;Lu - 2020 - DeepXDE A Deep Learning Library for Solving Forwa.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/S5VWGMPR/Lu - 2020 - DeepXDE A Deep Learning Library for Solving Forwa.pdf:application/pdf},
}

@misc{xu_distributed_2020,
	title = {Distributed {Machine} {Learning} for {Computational} {Engineering} using {MPI}},
	copyright = {NN, PDE, MPI},
	shorttitle = {{ANN}, {PDE}, and {MPI}},
	url = {http://arxiv.org/abs/2011.01349},
	abstract = {We propose a framework for training neural networks that are coupled with partial diﬀerential equations (PDEs) in a parallel computing environment. Unlike most distributed computing frameworks for deep neural networks, our focus is to parallelize both numerical solvers and deep neural networks in forward and adjoint computations. Our parallel computing model views data communication as a node in the computational graph for numerical simulations. The advantage of our model is that data communication and computing are cleanly separated and thus provide better ﬂexibility, modularity, and testability. We demonstrate using various large-scale problems that we can achieve substantial acceleration by using parallel solvers for PDEs in training deep neural networks that are coupled with PDEs.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Xu, Kailai and Zhu, Weiqiang and Darve, Eric},
	month = nov,
	year = {2020},
	keywords = {Mathematics - Numerical Analysis, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Xu et al. - 2020 - Distributed Machine Learning for Computational Eng.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/4VZ3P2Z5/Xu et al. - 2020 - Distributed Machine Learning for Computational Eng.pdf:application/pdf},
}

@misc{chen_learning_2021,
	title = {Learning {Neural} {Event} {Functions} for {Ordinary} {Differential} {Equations}},
	shorttitle = {Neural {Event} {ODE}},
	url = {http://arxiv.org/abs/2011.03902},
	abstract = {The existing Neural ODE formulation relies on an explicit knowledge of the termination time. We extend Neural ODEs to implicitly deﬁned termination criteria modeled by neural event functions, which can be chained together and differentiated through. Neural Event ODEs are capable of modeling discrete and instantaneous changes in a continuous-time system, without prior knowledge of when these changes should occur or how many such changes should exist. We test our approach in modeling hybrid discrete- and continuous- systems such as switching dynamical systems and collision in multi-body systems, and we propose simulation-based training of point processes with applications in discrete control.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Chen, Ricky T. Q. and Amos, Brandon and Nickel, Maximilian},
	month = oct,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Chen et al. - 2021 - Learning Neural Event Functions for Ordinary Diffe.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/G7HBIGXW/Chen et al. - 2021 - Learning Neural Event Functions for Ordinary Diffe.pdf:application/pdf},
}

@article{karniadakis_physics-informed_2021,
	title = {Physics-informed {Machine} {Learning}},
	volume = {3},
	issn = {2522-5820},
	shorttitle = {Physics-informed {ML}},
	url = {https://www.nature.com/articles/s42254-021-00314-5},
	doi = {10.1038/s42254-021-00314-5},
	abstract = {Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-d imensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-t ime domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-b ased regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-d imensional problems.},
	language = {en},
	number = {6},
	urldate = {2023-04-28},
	journal = {Nat Rev Phys},
	author = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
	month = may,
	year = {2021},
	pages = {422--440},
	file = {Karniadakis et al. - 2021 - Physics-informed machine learning.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/JN2EGE8J/Karniadakis et al. - 2021 - Physics-informed machine learning.pdf:application/pdf},
}

@misc{markidis_old_2021,
	title = {The {Old} and the {New}: {Can} {Physics}-{Informed} {Deep}-{Learning} {Replace} {Traditional} {Linear} {Solvers}?},
	shorttitle = {{PINN} to replace linear solvers},
	url = {http://arxiv.org/abs/2103.09655},
	abstract = {Physics-Informed Neural Networks (PINN) are neural networks encoding the problem governing equations, such as Partial Differential Equations (PDE), as a part of the neural network. PINNs have emerged as a new essential tool to solve various challenging problems, including computing linear systems arising from PDEs, a task for which several traditional methods exist. In this work, we focus first on evaluating the potential of PINNs as linear solvers in the case of the Poisson equation, an omnipresent equation in scientific computing. We characterize PINN linear solvers in terms of accuracy and performance under different network configurations (depth, activation functions, input data set distribution). We highlight the critical role of transfer learning. Our results show that low-frequency components of the solution converge quickly as an effect of the F-principle. In contrast, an accurate solution of the high frequencies requires an exceedingly long time. To address this limitation, we propose integrating PINNs into traditional linear solvers. We show that this integration leads to the development of new solvers whose performance is on par with other high-performance solvers, such as PETSc conjugate gradient linear solvers, in terms of performance and accuracy. Overall, while the accuracy and computational performance are still a limiting factor for the direct use of PINN linear solvers, hybrid strategies combining old traditional linear solver approaches with new emerging deep-learning techniques are among the most promising methods for developing a new class of linear solvers.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Markidis, Stefano},
	month = jul,
	year = {2021},
	keywords = {Physics - Computational Physics, Mathematics - Numerical Analysis, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv.org Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/HQMITYQ8/2103.html:text/html;Markidis - 2021 - The Old and the New Can Physics-Informed Deep-Lea.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/L9HLM5XW/Markidis - 2021 - The Old and the New Can Physics-Informed Deep-Lea.pdf:application/pdf},
}

@article{wang_eigenvector_2021,
	title = {On the eigenvector bias of {Fourier} feature networks: {From} regression to solving multi-scale {PDEs} with physics-informed neural networks},
	volume = {384},
	copyright = {Eigenvector Bias of Fourier},
	issn = {00457825},
	shorttitle = {Eigenvector {Bias} of {Fourier}},
	url = {http://arxiv.org/abs/2012.10047},
	doi = {10.1016/j.cma.2021.113938},
	abstract = {Physics-informed neural networks (PINNs) are demonstrating remarkable promise in integrating physical models with gappy and noisy observational data, but they still struggle in cases where the target functions to be approximated exhibit high-frequency or multi-scale features. In this work we investigate this limitation through the lens of Neural Tangent Kernel (NTK) theory and elucidate how PINNs are biased towards learning functions along the dominant eigen-directions of their limiting NTK. Using this observation, we construct novel architectures that employ spatio-temporal and multi-scale random Fourier features, and justify how such coordinate embedding layers can lead to robust and accurate PINN models. Numerical examples are presented for several challenging cases where conventional PINN models fail, including wave propagation and reaction-diffusion dynamics, illustrating how the proposed methods can be used to effectively tackle both forward and inverse problems involving partial differential equations with multi-scale behavior. All code an data accompanying this manuscript will be made publicly available at https://github.com/ PredictiveIntelligenceLab/MultiscalePINNs.},
	language = {en},
	urldate = {2023-04-28},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Wang, Sifan and Wang, Hanwen and Perdikaris, Paris},
	month = oct,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {113938},
	file = {Wang et al. - 2021 - On the eigenvector bias of Fourier feature network.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/5J73HG28/Wang et al. - 2021 - On the eigenvector bias of Fourier feature network.pdf:application/pdf},
}

@article{niaki_physics-informed_2021,
	title = {Physics-{Informed} {Neural} {Network} for {Modelling} the {Thermochemical} {Curing} {Process} of {Composite}-{Tool} {Systems} {During} {Manufacture}},
	volume = {384},
	copyright = {Thermochemical},
	issn = {00457825},
	shorttitle = {{PINN} {Thermochemical}},
	url = {http://arxiv.org/abs/2011.13511},
	doi = {10.1016/j.cma.2021.113959},
	abstract = {We present a Physics-Informed Neural Network (PINN) to simulate the thermochemical evolution of a composite material on a tool undergoing cure in an autoclave. In particular, we solve the governing coupled system of diﬀerential equations—including conductive heat transfer and resin cure kinetics—by optimizing the parameters of a deep neural network (DNN) using a physics-based loss function. To account for the vastly diﬀerent behaviour of thermal conduction and resin cure, we design a PINN consisting of two disconnected subnetworks, and develop a sequential training algorithm that mitigates instability present in traditional training methods. Further, we incorporate explicit discontinuities into the DNN at the composite-tool interface and enforce known physical behaviour directly in the loss function to improve the solution near the interface. We train the PINN with a technique that automatically adapts the weights on the loss terms corresponding to PDE, boundary, interface, and initial conditions. Finally, we demonstrate that one can include problem parameters as an input to the model—resulting in a surrogate that provides real-time simulation for a range of problem settings—and that one can use transfer learning to signiﬁcantly reduce the training time for problem settings similar to that of an initial trained model. The performance of the proposed PINN is demonstrated in multiple scenarios with diﬀerent material thicknesses and thermal boundary conditions.},
	language = {en},
	urldate = {2023-04-28},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Niaki, Sina Amini and Haghighat, Ehsan and Campbell, Trevor and Poursartip, Anoush and Vaziri, Reza},
	month = oct,
	year = {2021},
	keywords = {Computer Science - Machine Learning, 35Q79, 74A40, 68T07, 65M22,, J.2},
	pages = {113959},
	file = {Niaki et al. - 2021 - Physics-Informed Neural Network for Modelling the .pdf:/home/x/Documents/inpe/library-ml-zotero/storage/U4E4ZMP3/Niaki et al. - 2021 - Physics-Informed Neural Network for Modelling the .pdf:application/pdf},
}

@article{feng_physics-informed_2023,
	title = {Physics-informed neural networks of the {Saint}-{Venant} equations for downscaling a large-scale river model},
	volume = {59},
	copyright = {Saint-Venant},
	issn = {0043-1397, 1944-7973},
	shorttitle = {Saint-{Venant}},
	url = {http://arxiv.org/abs/2210.03240},
	doi = {10.1029/2022WR033168},
	abstract = {Large-scale river models are being reﬁned over coastal regions to improve the scientiﬁc understanding of coastal processes, hazards and responses to climate change. However, coarse mesh resolutions and approximations in physical representations of tidal rivers limit the performance of such models at resolving the complex ﬂow dynamics especially near the river-ocean interface, resulting in inaccurate simulations of ﬂood inundation. In this research, we propose a machine learning (ML) framework based on the state-of-the-art physics-informed neural network (PINN) to simulate the downscaled ﬂow at the subgrid scale. First, we demonstrate that PINN is able to assimilate observations of various types and solve the one-dimensional (1-D) Saint-Venant equations (SVE) directly. We perform the ﬂow simulations over a ﬂoodplain and along an open channel in several synthetic case studies. The PINN performance is evaluated against analytical solutions and numerical models. Our results indicate that the PINN solutions of water depth have satisfactory accuracy with limited observations assimilated. In the case of ﬂood wave propagation induced by storm surge and tide, a new neural network architecture is proposed based on Fourier feature embeddings that seamlessly encodes the periodic tidal boundary condition in the PINN’s formulation. Furthermore, we show that the PINNbased downscaling can produce more reasonable subgrid solutions of the along-channel water depth by assimilating observational data. The PINN solution outperforms the simple linear interpolation in resolving the topography and dynamic ﬂow regimes at the subgrid scale. This study provides a promising path towards improving emulation capabilities in large-scale models to characterize ﬁne-scale coastal processes.},
	language = {en},
	number = {2},
	urldate = {2023-04-28},
	journal = {Water Resources Research},
	author = {Feng, Dongyu and Tan, Zeli and He, QiZhi},
	month = feb,
	year = {2023},
	keywords = {Physics - Atmospheric and Oceanic Physics, Physics - Fluid Dynamics},
	file = {Feng et al. - 2023 - Physics-informed neural networks of the Saint-Vena.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/ISHBQBBB/Feng et al. - 2023 - Physics-informed neural networks of the Saint-Vena.pdf:application/pdf},
}

@misc{das_state---art_2022,
	title = {State-of-the-{Art} {Review} of {Design} of {Experiments} for {Physics}-{Informed} {Deep} {Learning}},
	copyright = {Review},
	shorttitle = {{PINN} state-of-the-art review},
	abstract = {This paper presents a comprehensive review of the design of experiments used in the surrogate models. In particular, this study demonstrates the necessity of the design of experiment schemes for the Physics-Informed Neural Network (PINN), which belongs to the supervised learning class. Many complex partial diﬀerential equations (PDEs) do not have any analytical solution; only numerical methods are used to solve the equations, which is computationally expensive. In recent decades, PINN has gained popularity as a replacement for numerical methods to reduce the computational budget. PINN uses physical information in the form of diﬀerential equations to enhance the performance of the neural networks. Though it works eﬃciently, the choice of the design of experiment scheme is important as the accuracy of the predicted responses using PINN depends on the training data. In this study, ﬁve diﬀerent PDEs are used for numerical purposes, i.e., viscous Burger’s equation, Shro¨dinger equation, heat equation, Allen-Cahn equation, and Korteweg-de Vries equation. A comparative study is performed to establish the necessity of the selection of a DoE scheme. It is seen that the Hammersley sampling-based PINN performs better than other DoE sample strategies.},
	language = {en},
	publisher = {arXiv},
	author = {Das, Sourav and Tesfamariam, Solomon},
	month = feb,
	year = {2022},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Das and Tesfamariam - 2022 - State-of-the-Art Review of Design of Experiments f.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/QF4QLMJC/Das and Tesfamariam - 2022 - State-of-the-Art Review of Design of Experiments f.pdf:application/pdf},
}

@article{lucor_simple_2022,
	title = {Simple computational strategies for more effective physics-informed neural networks modeling of turbulent natural convection},
	volume = {456},
	copyright = {Convection},
	issn = {00219991},
	shorttitle = {Convection {PINN}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999122000845},
	doi = {10.1016/j.jcp.2022.111022},
	abstract = {Recent works have explored the potential of machine learning as data-driven turbulence closures for RANS and LES techniques. Beyond these advances, the high expressivity and agility of physics-informed neural networks (PINNs) make them promising candidates for full ﬂuid ﬂow PDE modeling. An important question is whether this new paradigm, exempt from the traditional notion of discretization of the underlying operators very much connected to the ﬂow scales resolution, is capable of sustaining high levels of turbulence characterized by multi-scale features? We investigate the use of PINNs surrogate modeling for turbulent Rayleigh-B´enard (RB) convection ﬂows in rough and smooth rectangular cavities, mainly relying on DNS temperature data from the ﬂuid bulk. We carefully quantify the computational requirements under which the formulation is capable of accurately recovering the ﬂow hidden quantities. We then propose a new padding technique to distribute some of the scattered coordinates - at which PDE residuals are minimized - around the region of labeled data acquisition. We show how it comes to play as a regularization close to the training boundaries which are zones of poor accuracy for standard PINNs and results in a noticeable global accuracy improvement at iso-budget. Finally, we propose for the ﬁrst time to relax the incompressibility condition in such a way that it drastically beneﬁts the optimization search and results in a much improved convergence of the composite loss function. The RB results obtained at high Rayleigh number Ra = 2 · 109 are particularly impressive: the predictive accuracy of the surrogate over the entire half a billion DNS coordinates yields errors for all ﬂow variables ranging between [0.3\% − 4\%] in the relative L2 norm, with a training relying only on 1.6\% of the DNS data points.},
	language = {en},
	urldate = {2023-04-28},
	journal = {Journal of Computational Physics},
	author = {Lucor, Didier and Agrawal, Atul and Sergent, Anne},
	month = may,
	year = {2022},
	pages = {111022},
	file = {Lucor et al. - 2022 - Simple computational strategies for more effective.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/B5KNX8RF/Lucor et al. - 2022 - Simple computational strategies for more effective.pdf:application/pdf},
}

@book{nocedal_numerical_2006,
	address = {New York},
	edition = {2nd ed},
	series = {Springer series in operations research},
	title = {Numerical {Optimization}},
	copyright = {Book},
	isbn = {978-0-387-30303-1},
	shorttitle = {{BFGS} {Method}},
	url = {http://www.ime.unicamp.br/~pulino/MT404/TextosOnline/NocedalJ.pdf},
	language = {en},
	publisher = {Springer},
	author = {Nocedal, Jorge and Wright, Stephen J.},
	year = {2006},
	note = {OCLC: ocm68629100},
	keywords = {Mathematical optimization},
	file = {Nocedal and Wright - 2006 - Numerical optimization.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/JHIHCI26/Nocedal and Wright - 2006 - Numerical optimization.pdf:application/pdf},
}

@article{sukumar_exact_2022,
	title = {Exact imposition of boundary conditions with distance functions in physics-informed deep neural networks},
	volume = {389},
	issn = {00457825},
	shorttitle = {{PINN} boundary conditions},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0045782521006186},
	doi = {10.1016/j.cma.2021.114333},
	abstract = {In this paper, we introduce a new approach based on distance ﬁelds to exactly impose boundary conditions in physicsinformed deep neural networks. The challenges in satisfying Dirichlet boundary conditions in meshfree and particle methods are well-known. This issue is also pertinent in the development of physics informed neural networks (PINN) for the solution of partial diﬀerential equations. We introduce geometry-aware trial functions in artiﬁcal neural networks to improve the training in deep learning for partial diﬀerential equations. To this end, we use concepts from constructive solid geometry (R-functions) and generalized barycentric coordinates (mean value potential ﬁelds) to construct φ(x), an approximate distance function to the boundary of a domain in Rd. To exactly impose homogeneous Dirichlet boundary conditions, the trial function is taken as φ(x) multiplied by the PINN approximation, and its generalization via transﬁnite interpolation is used to a priori satisfy inhomogeneous Dirichlet (essential), Neumann (natural), and Robin boundary conditions on complex geometries. In doing so, we eliminate modeling error associated with the satisfaction of boundary conditions in a collocation method and ensure that kinematic admissibility is met pointwise in a Ritz method. With this new ansatz, the training for the neural network is simpliﬁed: sole contribution to the loss function is from the residual error at interior collocation points where the governing equation is required to be satisﬁed. Numerical solutions are computed using strong form collocation and Ritz minimization. To convey the main ideas and to assess the accuracy of the approach, we present numerical solutions for linear and nonlinear boundary-value problems over convex and nonconvex polygonal domains as well as over domains with curved boundaries. Benchmark problems in one dimension for linear elasticity, advection-diﬀusion, and beam bending; and in two dimensions for the steady-state heat equation, Laplace equation, biharmonic equation (Kirchhoﬀ plate bending), and the nonlinear Eikonal equation are considered. The construction of approximate distance functions using R-functions extends to higher dimensions, and we showcase its use by solving a Poisson problem with homogeneous Dirichlet boundary conditions over the four-dimensional hypercube. The proposed approach consistently outperforms a standard PINN-based collocation method, which underscores the importance of exactly (a priori) satisfying the boundary condition when constructing a loss function in PINN. This study provides a pathway for meshfree analysis to be conducted on the exact geometry without domain discretization.},
	language = {en},
	urldate = {2023-04-28},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Sukumar, N. and Srivastava, Ankit},
	month = feb,
	year = {2022},
	pages = {114333},
	file = {Sukumar and Srivastava - 2022 - Exact imposition of boundary conditions with dista.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/4FW4XVSQ/Sukumar and Srivastava - 2022 - Exact imposition of boundary conditions with dista.pdf:application/pdf},
}

@article{ma_preliminary_2022,
	title = {A {Preliminary} {Study} on the {Resolution} of {Electro}-{Thermal} {Multi}-{Physics} {Coupling} {Problem} {Using} {Physics}-{Informed} {Neural} {Network} ({PINN})},
	volume = {15},
	copyright = {With heat sources},
	issn = {1999-4893},
	shorttitle = {Electro-{Thermal} {PINN}},
	url = {https://www.mdpi.com/1999-4893/15/2/53},
	doi = {10.3390/a15020053},
	abstract = {The problem of electro-thermal coupling is widely present in the integrated circuit (IC). The accuracy and efﬁciency of traditional solution methods, such as the ﬁnite element method (FEM), are tightly related to the quality and density of mesh construction. Recently, PINN (physics-informed neural network) was proposed as a method for solving differential equations. This method is mesh free and generalizes the process of solving PDEs regardless of the equations’ structure. Therefore, an experiment is conducted to explore the feasibility of PINN in solving electro-thermal coupling problems, which include the electrokinetic ﬁeld and steady-state thermal ﬁeld. We utilize two neural networks in the form of sequential training to approximate the electric ﬁeld and the thermal ﬁeld, respectively. The experimental results show that PINN provides good accuracy in solving electro-thermal coupling problems.},
	language = {en},
	number = {2},
	urldate = {2023-04-28},
	journal = {Algorithms},
	author = {Ma, Yaoyao and Xu, Xiaoyu and Yan, Shuai and Ren, Zhuoxiang},
	month = feb,
	year = {2022},
	pages = {53},
	file = {Ma et al. - 2022 - A Preliminary Study on the Resolution of Electro-T.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/C5JH25BP/Ma et al. - 2022 - A Preliminary Study on the Resolution of Electro-T.pdf:application/pdf},
}

@article{katsikis_gentle_2022,
	title = {A {Gentle} {Introduction} to {Physics}-{Informed} {Neural} {Networks}, with {Applications} in {Static} {Rod} and {Beam} {Problems}},
	volume = {9},
	issn = {2409-5761},
	shorttitle = {{PINN} {Static} {Rod} and {Beam}},
	url = {https://avantipublishers.com/index.php/jaacm/article/view/1246},
	doi = {10.15377/2409-5761.2022.09.8},
	abstract = {A modern approach to solving mathematical models involving differential equations, the so-called Physics-Informed Neural Network (PINN), is based on the techniques which include the use of artificial neural networks and the method of fitting the governing differential equations at collocation points. In this paper, training of the PINN with an application of optimization techniques is performed on simple onedimensional mechanical problems of elasticity, namely rods and beams. Different boundary conditions are considered.},
	language = {en},
	urldate = {2023-04-28},
	journal = {J. Adv. App. Comput. Math.},
	author = {Katsikis, Dimitrios and Muradova, Aliki D. and Stavroulakis, Georgios E.},
	month = may,
	year = {2022},
	pages = {103--128},
	file = {Katsikis et al. - 2022 - A Gentle Introduction to Physics-Informed Neural N.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/H4LPNVKC/Katsikis et al. - 2022 - A Gentle Introduction to Physics-Informed Neural N.pdf:application/pdf},
}

@inproceedings{martin_reinforcement_2022,
	address = {San Diego, CA \& Virtual},
	title = {Reinforcement {Learning} and {Orbit}-{Discovery} {Enhanced} by {Small}-{Body} {Physics}-{Informed} {Neural} {Network} {Gravity} {Models}},
	isbn = {978-1-62410-631-6},
	shorttitle = {{PINN} gravity model},
	url = {https://arc.aiaa.org/doi/10.2514/6.2022-2272},
	doi = {10.2514/6.2022-2272},
	language = {en},
	urldate = {2023-04-28},
	booktitle = {{AIAA} {SCITECH} 2022 {Forum}},
	publisher = {American Institute of Aeronautics and Astronautics},
	author = {Martin, John and Schaub, Hanspeter},
	month = jan,
	year = {2022},
	file = {Martin and Schaub - 2022 - Reinforcement Learning and Orbit-Discovery Enhance.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/VUGXYVAN/Martin and Schaub - 2022 - Reinforcement Learning and Orbit-Discovery Enhance.pdf:application/pdf},
}

@article{han_solving_2018,
	title = {Solving high-dimensional partial differential equations using deep learning},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	shorttitle = {{ODE} \& deep learning},
	url = {https://pnas.org/doi/full/10.1073/pnas.1718942115},
	doi = {10.1073/pnas.1718942115},
	abstract = {Significance
            Partial differential equations (PDEs) are among the most ubiquitous tools used in modeling problems in nature. However, solving high-dimensional PDEs has been notoriously difficult due to the “curse of dimensionality.” This paper introduces a practical algorithm for solving nonlinear PDEs in very high (hundreds and potentially thousands of) dimensions. Numerical results suggest that the proposed algorithm is quite effective for a wide variety of problems, in terms of both accuracy and speed. We believe that this opens up a host of possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their interrelationships.
          , 
            Developing algorithms for solving high-dimensional partial differential equations (PDEs) has been an exceedingly difficult task for a long time, due to the notoriously difficult problem known as the “curse of dimensionality.” This paper introduces a deep learning-based approach that can handle general high-dimensional parabolic PDEs. To this end, the PDEs are reformulated using backward stochastic differential equations and the gradient of the unknown solution is approximated by neural networks, very much in the spirit of deep reinforcement learning with the gradient acting as the policy function. Numerical results on examples including the nonlinear Black–Scholes equation, the Hamilton–Jacobi–Bellman equation, and the Allen–Cahn equation suggest that the proposed algorithm is quite effective in high dimensions, in terms of both accuracy and cost. This opens up possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their interrelationships.},
	language = {en},
	number = {34},
	urldate = {2023-04-28},
	journal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Han, Jiequn and Jentzen, Arnulf and E, Weinan},
	month = aug,
	year = {2018},
	pages = {8505--8510},
	file = {Han et al. - 2018 - Solving high-dimensional partial differential equa.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/BLFE32CV/Han et al. - 2018 - Solving high-dimensional partial differential equa.pdf:application/pdf},
}

@article{cai_physics-informed_2021,
	title = {Physics-{Informed} {Neural} {Networks} for {Heat} {Transfer} {Problems}},
	volume = {143},
	issn = {0022-1481},
	shorttitle = {{PINN} for heat transfer},
	url = {https://doi.org/10.1115/1.4050542},
	doi = {10.1115/1.4050542},
	abstract = {Physics-informed neural networks (PINNs) have gained popularity across different engineering fields due to their effectiveness in solving realistic problems with noisy data and often partially missing physics. In PINNs, automatic differentiation is leveraged to evaluate differential operators without discretization errors, and a multitask learning problem is defined in order to simultaneously fit observed data while respecting the underlying governing laws of physics. Here, we present applications of PINNs to various prototype heat transfer problems, targeting in particular realistic conditions not readily tackled with traditional computational methods. To this end, we first consider forced and mixed convection with unknown thermal boundary conditions on the heated surfaces and aim to obtain the temperature and velocity fields everywhere in the domain, including the boundaries, given some sparse temperature measurements. We also consider the prototype Stefan problem for two-phase flow, aiming to infer the moving interface, the velocity and temperature fields everywhere as well as the different conductivities of a solid and a liquid phase, given a few temperature measurements inside the domain. Finally, we present some realistic industrial applications related to power electronics to highlight the practicality of PINNs as well as the effective use of neural networks in solving general heat transfer problems of industrial complexity. Taken together, the results presented herein demonstrate that PINNs not only can solve ill-posed problems, which are beyond the reach of traditional computational methods, but they can also bridge the gap between computational and experimental heat transfer.},
	number = {6},
	urldate = {2023-04-28},
	journal = {Journal of Heat Transfer},
	author = {Cai, Shengze and Wang, Zhicheng and Wang, Sifan and Perdikaris, Paris and Karniadakis, George Em},
	month = apr,
	year = {2021},
	file = {Cai et al. - 2021 - Physics-Informed Neural Networks for Heat Transfer.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/SQ2FCBZM/Cai et al. - 2021 - Physics-Informed Neural Networks for Heat Transfer.pdf:application/pdf;Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/Q23CTBDY/Physics-Informed-Neural-Networks-for-Heat-Transfer.html:text/html},
}

@phdthesis{alhubail_application_2021,
	type = {Thesis},
	title = {Application of {Physics}-{Informed} {Neural} {Networks} to {Solve} 2-{D} {Single}-phase {Flow} in {Heterogeneous} {Porous} {Media}},
	copyright = {Thesis},
	shorttitle = {{PINN} {2D} single-phase flow},
	url = {https://repository.kaust.edu.sa/handle/10754/670174},
	abstract = {Neural networks have recently seen tremendous advancements in applicability in
many areas, one of which is their utilization in solving physical problems governed by
partial differential equations and the constraints of these equations. Physics-informed
neural networks is the name given to such neural networks. They are different from
typical neural networks in that they include loss terms that represent the physics
of the problem. These terms often include partial derivatives of the neural network
outputs with respect to its inputs, and these derivatives are found through the use of
automatic differentiation.
The purpose of this thesis is to showcase the ability of physics-informed neural
networks to solve basic fluid flow problems in homogeneous and heterogeneous porous
media. This is done through the utilization of the pressure equation under a set of
assumptions as well as the inclusion of Dirichlet and Neumann boundary conditions.
The goal is to create a surrogate model that allows for finding the pressure and
velocity profiles everywhere inside the domain of interest.
In the homogeneous case, minimization of the loss function that included the
boundary conditions term and the partial differential equation term allowed for producing
results that show good agreement with the results from a numerical simulator.
However, in the case of heterogeneous media where there are sharp discontinuities in
hydraulic conductivity inside the domain, the model failed to produce accurate results.
To resolve this issue, extended physics-informed neural networks were used.
This method involves the decomposition of the domain into multiple homogeneous
sub-domains. Each sub-domain has its own physics informed neural network structure,
equation parameters, and equation constraints. To allow the sub-domains to
communicate, interface conditions are placed on the interfaces that separate the different
sub-domains. The results from this method matched well with the results of
the simulator. In both the homogeneous and heterogeneous cases, neural networks
with only one hidden layer with thirty nodes were used. Even with this simple structure
for the neural networks, the computations are expensive and a large number of
training iterations is required to converge.},
	language = {en},
	urldate = {2023-04-28},
	author = {Alhubail, Ali},
	month = jul,
	year = {2021},
	doi = {10.25781/KAUST-37Z86},
	file = {Full Text PDF:/home/x/Documents/inpe/library-ml-zotero/storage/ZJXYNTV5/Alhubail - 2021 - Application of Physics-Informed Neural Networks to.pdf:application/pdf},
}

@article{zhang_data_2020,
	title = {Data {Driven} {Solutions} and {Discoveries} in {Mechanics} {Using} {Physics} {Informed} {Neural} {Network}},
	copyright = {Heat 2D},
	shorttitle = {{PINN} {Mechanics}},
	url = {https://www.preprints.org/manuscript/202006.0258/v1},
	doi = {10.20944/preprints202006.0258.v1},
	abstract = {Deep learning has achieved remarkable success in diverse computer science applications, however, its use in other traditional engineering fields has emerged only recently. In this project, we solved several mechanics problems governed by differential equations, using physics informed neural networks (PINN). The PINN embeds the differential equations into the loss of the neural network using automatic differentiation. We present our developments in the context of solving two main classes of problems: data-driven solutions and data-driven discoveries, and we compare the results with either analytical solutions or numerical solutions using the finite element method. The remarkable achievements of the PINN model shown in this report suggest the bright prospect of the physics-informed surrogate models that are fully differentiable with respect to all input coordinates and free parameters. More broadly, this study shows that PINN provides an attractive alternative to solve traditional engineering problems.},
	urldate = {2023-04-28},
	author = {Zhang, Qi and Chen, Yilin and Yang, Ziyi},
	month = jun,
	year = {2020},
	file = {Full Text PDF:/home/x/Documents/inpe/library-ml-zotero/storage/Y9K69FD9/Zhang et al. - 2020 - Data Driven Solutions and Discoveries in Mechanics.pdf:application/pdf},
}

@article{lagaris_artificial_1998,
	title = {Artificial {Neural} {Networks} for {Solving} {Ordinary} and {Partial} {Differential} {Equations}},
	volume = {9},
	issn = {10459227},
	shorttitle = {{ANN} \& {PDE}},
	url = {http://arxiv.org/abs/physics/9705023},
	doi = {10.1109/72.712178},
	abstract = {We present a method to solve initial and boundary value problems using artificial neural networks. A trial solution of the differential equation is written as a sum of two parts. The first part satisfies the boundary (or initial) conditions and contains no adjustable parameters. The second part is constructed so as not to affect the boundary conditions. This part involves a feedforward neural network, containing adjustable parameters (the weights). Hence by construction the boundary conditions are satisfied and the network is trained to satisfy the differential equation. The applicability of this approach ranges from single ODE's, to systems of coupled ODE's and also to PDE's. In this article we illustrate the method by solving a variety of model problems and present comparisons with finite elements for several cases of partial differential equations.},
	number = {5},
	urldate = {2023-04-28},
	journal = {IEEE Trans. Neural Netw.},
	author = {Lagaris, I. E. and Likas, A. and Fotiadis, D. I.},
	month = sep,
	year = {1998},
	keywords = {Physics - Computational Physics, Nonlinear Sciences - Cellular Automata and Lattice Gases, Quantum Physics},
	pages = {987--1000},
	file = {arXiv Fulltext PDF:/home/x/Documents/inpe/library-ml-zotero/storage/9Q7J6W8I/Lagaris et al. - 1998 - Artificial Neural Networks for Solving Ordinary an.pdf:application/pdf;arXiv.org Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/NLE728IW/9705023.html:text/html;Ôö et al. - Department of Computer Science University of Ioann.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/JZWJLJKI/Ôö et al. - Department of Computer Science University of Ioann.pdf:application/pdf},
}

@unpublished{jeffrey_wong_lecture_2018,
	title = {Lecture {Notes} on {PDEs}, part {I}: {The} heat equation and the eigenfunction method},
	copyright = {Theory},
	shorttitle = {Heat equation (lecture)},
	url = {https://services.math.duke.edu/~jtwong/},
	author = {{Jeffrey Wong}},
	year = {2018},
	file = {Lecture Notes on PDEs.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/JESBBLDD/Lecture Notes on PDEs.pdf:application/pdf},
}

@phdthesis{cerqueira_state---art_2021,
	title = {A state-of-the-art of physics-informed neural networks in engineering},
	copyright = {Thesis},
	shorttitle = {{PINN} in engineering},
	url = {http://pantheon.ufrj.br/handle/11422/15774},
	abstract = {Machine learning techniques have gained space in the industrial scenario as a tool to convert the increasing flux of information (data) in process improvement. Among these techniques, neural networks has got much attention due to their universal approximators capacity, of which performance can be improved by providing previous physical knowledge: one has, therefore, the development of the so called Physicsinformed neural networks (PINN). In such context and having noticed a “gap” in the works related on this topics and in the diffusion of this theme in the School of Chemistry, this work proposes a state-of-the-art of the mentioned technique. Particular interesting concerning PINN in fluid mechanics and heat transfer has been noticed. Moreover, PINN have been pointed as important tools for solving forward and inverse problems. Finally, through practical examples, this work has shown the use of neural networks for solving one particular example in chemical engineering without informing the physics of the problem (obtaining the friction factor) and using the differential equation that describes it (solving the 1D heat diffusion equation).},
	language = {eng},
	urldate = {2023-04-28},
	author = {Cerqueira, Pedro Henrique da Silva Singue},
	month = aug,
	year = {2021},
	file = {Cerqueira - 2021 - A state-of-the-art of physics-informed neural netw.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/BK2J3BU7/Cerqueira - 2021 - A state-of-the-art of physics-informed neural netw.pdf:application/pdf;PINN (monografia) 2021 - Cerqueira.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/DZ2WIMHD/PINN (monografia) 2021 - Cerqueira.pdf:application/pdf},
}

@article{laneryd_physics_2022,
	series = {10th {Vienna} {International} {Conference} on {Mathematical} {Modelling} {MATHMOD} 2022},
	title = {Physics {Informed} {Neural} {Networks} for {Power} {Transformer} {Dynamic} {Thermal} {Modelling}},
	volume = {55},
	issn = {2405-8963},
	shorttitle = {{PINN} thermal modeling},
	url = {https://www.sciencedirect.com/science/article/pii/S2405896322012526},
	doi = {10.1016/j.ifacol.2022.09.070},
	abstract = {The emerging methodology of Physics Informed Neural Networks (PINNs) promises to combine available data and physical knowledge to achieve high accuracy and fast evaluation. Dynamic thermal modelling of power transformers is an application specifically set to benefit from these characteristics. Data collected during typical operation is not representative of extreme loading scenarios and the number of thermal sensors is limited. The detailed geometry is often not known by the asset owner which creates high uncertainty for physics-based simulation models. In this study, the transformer is modeled by the one-dimensional heat diffusion equation. PINN is constructed with a loss function including both data-based and physics-based terms. A time-dependent source term from a time series of measurement is also part of the PINN. The result is compared with a finite volume solution demonstrating good agreement. The PINN approach will be useful for further development in thermal modelling for power transformers.},
	language = {en},
	number = {20},
	urldate = {2023-04-28},
	journal = {IFAC-PapersOnLine},
	author = {Laneryd, Tor and Bragone, Federica and Morozovska, Kateryna and Luvisotto, Michele},
	month = jan,
	year = {2022},
	keywords = {artificial intelligence for modelling, Comparison of methods, Energy Systems, Environmental systems, Finite Volume Method, Machine learning, Physics-Informed Neural Networks},
	pages = {49--54},
	file = {Full Text:/home/x/Documents/inpe/library-ml-zotero/storage/TNCGYJHJ/Bragone - 2021 - Physics-informed machine learning in power transfo.pdf:application/pdf;PINN Power Transformer - Bragone.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/ZVLBXVYI/PINN Power Transformer - Bragone.pdf:application/pdf;PINN Power Transformer - Bragone.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/GYM23752/PINN Power Transformer - Bragone.pdf:application/pdf;ScienceDirect Full Text PDF:/home/x/Documents/inpe/library-ml-zotero/storage/8T6JE5SH/Laneryd et al. - 2022 - Physics Informed Neural Networks for Power Transfo.pdf:application/pdf;ScienceDirect Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/KP92EI5H/S2405896322012526.html:text/html},
}

@unpublished{daileda_two_2012,
	title = {The two dimensional heat equation},
	copyright = {Theory},
	shorttitle = {Heat equation {2D} (lecture)},
	url = {http://ramanujan.math.trinity.edu/rdaileda/teach/s12/m3357/lectures/lecture_3_6_short.pdf},
	author = {Daileda, Ryan C.},
	year = {2012},
	file = {Daileda - The two dimensional heat equation.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/YACRWCL9/Daileda - The two dimensional heat equation.pdf:application/pdf},
}

@misc{dagrada_introduction_2022,
	title = {Introduction to {Physics}-informed {Neural} {Networks}},
	copyright = {website},
	shorttitle = {Introduction to {PINN} (website)},
	url = {https://towardsdatascience.com/solving-differential-equations-with-neural-networks-afdcf7b8bcc4},
	abstract = {A hands-on tutorial with PyTorch},
	language = {en},
	urldate = {2023-04-28},
	journal = {Medium},
	author = {Dagrada, Mario},
	year = {2022},
	note = {Sources: https://github.com/madagra/basic-pinn},
	file = {Dagrada - 2022 - Introduction to Physics-informed Neural Networks.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/PSXPY2M8/Dagrada - 2022 - Introduction to Physics-informed Neural Networks.pdf:application/pdf;Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/R88GZFHH/solving-differential-equations-with-neural-networks-afdcf7b8bcc4.html:text/html},
}

@misc{hannay_differential_2020,
	title = {Differential {Equations} as a {Neural} {Network} {Layer}},
	copyright = {website},
	shorttitle = {{ODA} as {NN} layer (website)},
	url = {https://towardsdatascience.com/differential-equations-as-a-neural-network-layer-ac3092632255},
	abstract = {A first step to adding domain knowledge to your neural network models},
	language = {en},
	urldate = {2023-04-28},
	journal = {Medium},
	author = {Hannay, Kevin},
	month = apr,
	year = {2020},
	file = {2020 DE as a Neural Network Layers - HANNAY.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/7TCPKV3Q/2020 DE as a Neural Network Layers - HANNAY.pdf:application/pdf;Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/QX4VVYR5/differential-equations-as-a-neural-network-layer-ac3092632255.html:text/html},
}

@misc{jacquier_tensorflow_2019,
	title = {{TensorFlow} 2.0 implementation of {Maziar} {Raissi}'s {Physics} {Informed} {Neural} {Networks} ({PINNs})},
	copyright = {Software},
	shorttitle = {{PINN} {Raissi} \& {TF2} (website)},
	url = {https://github.com/pierremtb/PINNs-TF2.0},
	author = {Jacquier, Pierre},
	year = {2019},
}

@article{wu_symmetric_1998,
	title = {Symmetric functional differential equations and neural networks with memory},
	volume = {350},
	issn = {0002-9947, 1088-6850},
	shorttitle = {Symmetric {PINN}},
	url = {https://www.ams.org/tran/1998-350-12/S0002-9947-98-02083-2/},
	doi = {10.1090/S0002-9947-98-02083-2},
	abstract = {Advancing research. Creating connections.},
	language = {en},
	number = {12},
	urldate = {2023-04-28},
	journal = {Trans. Amer. Math. Soc.},
	author = {Wu, Jianhong},
	year = {1998},
	keywords = {delay differential equation, equivariant degree, global bifurcation., neural network, Periodic solution, symmetry, wave},
	pages = {4799--4838},
	file = {Full Text PDF:/home/x/Documents/inpe/library-ml-zotero/storage/PF6YD3IW/Wu - 1998 - Symmetric functional differential equations and ne.pdf:application/pdf},
}

@article{berg_unified_2018,
	title = {A unified deep artificial neural network approach to partial differential equations in complex geometries},
	volume = {317},
	issn = {0925-2312},
	shorttitle = {{PINN} complex geometries},
	url = {https://www.sciencedirect.com/science/article/pii/S092523121830794X},
	doi = {10.1016/j.neucom.2018.06.056},
	abstract = {In this paper, we use deep feedforward artificial neural networks to approximate solutions to partial differential equations in complex geometries. We show how to modify the backpropagation algorithm to compute the partial derivatives of the network output with respect to the space variables which is needed to approximate the differential operator. The method is based on an ansatz for the solution which requires nothing but feedforward neural networks and an unconstrained gradient based optimization method such as gradient descent or a quasi-Newton method. We show an example where classical mesh based methods cannot be used and neural networks can be seen as an attractive alternative. Finally, we highlight the benefits of deep compared to shallow neural networks and device some other convergence enhancing techniques.},
	language = {en},
	urldate = {2023-04-28},
	journal = {Neurocomputing},
	author = {Berg, Jens and Nyström, Kaj},
	month = nov,
	year = {2018},
	keywords = {Advection, Complex geometries, Deep neural networks, Diffusion, Partial differential equations},
	pages = {28--41},
	file = {ScienceDirect Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/LTU5M2TW/S092523121830794X.html:text/html;Submitted Version:/home/x/Documents/inpe/library-ml-zotero/storage/5MXE2DMS/Berg and Nyström - 2018 - A unified deep artificial neural network approach .pdf:application/pdf},
}

@inproceedings{rubanova_latent_2019,
	title = {Latent {Ordinary} {Differential} {Equations} for {Irregularly}-{Sampled} {Time} {Series}},
	volume = {32},
	shorttitle = {{RNN} \& {ODE}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/42a6845a557bef704ad8ac9cb4461d43-Abstract.html},
	abstract = {Time series with non-uniform intervals occur in many applications, and are difficult to model using standard recurrent neural networks (RNNs). We generalize RNNs to have continuous-time hidden dynamics defined by ordinary differential equations (ODEs), a model we call ODE-RNNs. Furthermore, we use ODE-RNNs to replace the recognition network of the recently-proposed Latent ODE model. Both ODE-RNNs and Latent ODEs can naturally handle arbitrary time gaps between observations, and can explicitly model the probability of observation times using Poisson processes. We show experimentally that these ODE-based models outperform their RNN-based counterparts on irregularly-sampled data.},
	urldate = {2023-04-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Rubanova, Yulia and Chen, Ricky T. Q. and Duvenaud, David K},
	year = {2019},
	file = {Full Text PDF:/home/x/Documents/inpe/library-ml-zotero/storage/HI64FF2Z/Rubanova et al. - 2019 - Latent Ordinary Differential Equations for Irregul.pdf:application/pdf},
}

@misc{rackauckas_diffeqfluxjl_2019,
	title = {{DiffEqFlux}.jl - {A} {Julia} {Library} for {Neural} {Differential} {Equations}},
	shorttitle = {{DiffEqFlux}.jl library},
	url = {http://arxiv.org/abs/1902.02376},
	doi = {10.48550/arXiv.1902.02376},
	abstract = {DiffEqFlux.jl is a library for fusing neural networks and differential equations. In this work we describe differential equations from the viewpoint of data science and discuss the complementary nature between machine learning models and differential equations. We demonstrate the ability to incorporate DifferentialEquations.jl-defined differential equation problems into a Flux-defined neural network, and vice versa. The advantages of being able to use the entire DifferentialEquations.jl suite for this purpose is demonstrated by counter examples where simple integration strategies fail, but the sophisticated integration strategies provided by the DifferentialEquations.jl library succeed. This is followed by a demonstration of delay differential equations and stochastic differential equations inside of neural networks. We show high-level functionality for defining neural ordinary differential equations (neural networks embedded into the differential equation) and describe the extra models in the Flux model zoo which includes neural stochastic differential equations. We conclude by discussing the various adjoint methods used for backpropogation of the differential equation solvers. DiffEqFlux.jl is an important contribution to the area, as it allows the full weight of the differential equation solvers developed from decades of research in the scientific computing field to be readily applied to the challenges posed by machine learning and data science.},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Rackauckas, Chris and Innes, Mike and Ma, Yingbo and Bettencourt, Jesse and White, Lyndon and Dixit, Vaibhav},
	month = feb,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/x/Documents/inpe/library-ml-zotero/storage/LG9T9JLZ/Rackauckas et al. - 2019 - DiffEqFlux.jl - A Julia Library for Neural Differe.pdf:application/pdf;arXiv.org Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/RB3DQ6DB/1902.html:text/html},
}

@article{samaniego_energy_2020,
	title = {An energy approach to the solution of partial differential equations in computational mechanics via machine learning: {Concepts}, implementation and applications},
	volume = {362},
	issn = {0045-7825},
	shorttitle = {{PDE} {DNN}},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782519306826},
	doi = {10.1016/j.cma.2019.112790},
	abstract = {Partial Differential Equations (PDEs) are fundamental to model different phenomena in science and engineering mathematically. Solving them is a crucial step towards a precise knowledge of the behavior of natural and engineered systems. In general, in order to solve PDEs that represent real systems to an acceptable degree, analytical methods are usually not enough. One has to resort to discretization methods. For engineering problems, probably the best-known option is the finite element method (FEM). However, powerful alternatives such as mesh-free methods and Isogeometric Analysis (IGA) are also available. The fundamental idea is to approximate the solution of the PDE by means of functions specifically built to have some desirable properties. In this contribution, we explore Deep Neural Networks (DNNs) as an option for approximation. They have shown impressive results in areas such as visual recognition. DNNs are regarded here as function approximation machines. There is great flexibility to define their structure and important advances in the architecture and the efficiency of the algorithms to implement them make DNNs a very interesting alternative to approximate the solution of a PDE. We concentrate on applications that have an interest for Computational Mechanics. Most contributions explore this possibility have adopted a collocation strategy. In this work, we concentrate on mechanical problems and analyze the energetic format of the PDE. The energy of a mechanical system seems to be the natural loss function for a machine learning method to approach a mechanical problem. In order to prove the concepts, we deal with several problems and explore the capabilities of the method for applications in engineering.},
	language = {en},
	urldate = {2023-04-28},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Samaniego, E. and Anitescu, C. and Goswami, S. and Nguyen-Thanh, V. M. and Guo, H. and Hamdia, K. and Zhuang, X. and Rabczuk, T.},
	month = apr,
	year = {2020},
	keywords = {Deep neural networks, Energy approach, Physics informed},
	pages = {112790},
	file = {ScienceDirect Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/CYTA4PSZ/S0045782519306826.html:text/html;Submitted Version:/home/x/Documents/inpe/library-ml-zotero/storage/FST388A3/Samaniego et al. - 2020 - An energy approach to the solution of partial diff.pdf:application/pdf},
}

@article{chen_neurodiffeq_2020,
	title = {{NeuroDiffEq}: {A} {Python} package for solving differential equations with neural networks},
	volume = {5},
	copyright = {NeurIPS},
	issn = {2475-9066},
	shorttitle = {{NeurIPS} {Library}},
	url = {https://joss.theoj.org/papers/10.21105/joss.01931},
	doi = {10.21105/joss.01931},
	abstract = {Chen et al., (2020). NeuroDiffEq: A Python package for solving differential equations with neural networks. Journal of Open Source Software, 5(46), 1931, https://doi.org/10.21105/joss.01931},
	language = {en},
	number = {46},
	urldate = {2023-04-28},
	journal = {Journal of Open Source Software},
	author = {Chen, Feiyu and Sondak, David and Protopapas, Pavlos and Mattheakis, Marios and Liu, Shuheng and Agarwal, Devansh and Giovanni, Marco Di},
	month = feb,
	year = {2020},
	pages = {1931},
	file = {2018 Neural ODE (NeurIPS) - Chen.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/FKUISQSD/2018 Neural ODE (NeurIPS) - Chen.pdf:application/pdf;2018 Neural ODE (NeurIPS) (inclui apendices) - Chen.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/TATENQR2/2018 Neural ODE (NeurIPS) (inclui apendices) - Chen.pdf:application/pdf;Chen et al. - 2020 - NeuroDiffEq A Python package for solving differen.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/R5IAHBA6/Chen et al. - 2020 - NeuroDiffEq A Python package for solving differen.pdf:application/pdf},
}

@misc{lou_neural_2020,
	title = {Neural {Manifold} {Ordinary} {Differential} {Equations}},
	shorttitle = {Neural {Manifold} {ODE}},
	url = {http://arxiv.org/abs/2006.10254},
	doi = {10.48550/arXiv.2006.10254},
	abstract = {To better conform to data geometry, recent deep generative modelling techniques adapt Euclidean constructions to non-Euclidean spaces. In this paper, we study normalizing flows on manifolds. Previous work has developed flow models for specific cases; however, these advancements hand craft layers on a manifold-by-manifold basis, restricting generality and inducing cumbersome design constraints. We overcome these issues by introducing Neural Manifold Ordinary Differential Equations, a manifold generalization of Neural ODEs, which enables the construction of Manifold Continuous Normalizing Flows (MCNFs). MCNFs require only local geometry (therefore generalizing to arbitrary manifolds) and compute probabilities with continuous change of variables (allowing for a simple and expressive flow construction). We find that leveraging continuous manifold dynamics produces a marked improvement for both density estimation and downstream tasks.},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Lou, Aaron and Lim, Derek and Katsman, Isay and Huang, Leo and Jiang, Qingxuan and Lim, Ser-Nam and De Sa, Christopher},
	month = jun,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Differential Geometry},
	file = {arXiv Fulltext PDF:/home/x/Documents/inpe/library-ml-zotero/storage/M6LDWDUA/Lou et al. - 2020 - Neural Manifold Ordinary Differential Equations.pdf:application/pdf;arXiv.org Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/I7IMRLSY/2006.html:text/html},
}

@inproceedings{pal_opening_2021,
	title = {Opening the {Blackbox}: {Accelerating} {Neural} {Differential} {Equations} by {Regularizing} {Internal} {Solver} {Heuristics}},
	shorttitle = {Accelerating {NDE}},
	url = {https://proceedings.mlr.press/v139/pal21a.html},
	abstract = {Democratization of machine learning requires architectures that automatically adapt to new problems. Neural Differential Equations (NDEs) have emerged as a popular modeling framework by removing the need for ML practitioners to choose the number of layers in a recurrent model. While we can control the computational cost by choosing the number of layers in standard architectures, in NDEs the number of neural network evaluations for a forward pass can depend on the number of steps of the adaptive ODE solver. But, can we force the NDE to learn the version with the least steps while not increasing the training cost? Current strategies to overcome slow prediction require high order automatic differentiation, leading to significantly higher training time. We describe a novel regularization method that uses the internal cost heuristics of adaptive differential equation solvers combined with discrete adjoint sensitivities to guide the training process towards learning NDEs that are easier to solve. This approach opens up the blackbox numerical analysis behind the differential equation solver’s algorithm and directly uses its local error estimates and stiffness heuristics as cheap and accurate cost estimates. We incorporate our method without any change in the underlying NDE framework and show that our method extends beyond Ordinary Differential Equations to accommodate Neural Stochastic Differential Equations. We demonstrate how our approach can halve the prediction time and, unlike other methods which can increase the training time by an order of magnitude, we demonstrate similar reduction in training times. Together this showcases how the knowledge embedded within state-of-the-art equation solvers can be used to enhance machine learning.},
	language = {en},
	urldate = {2023-04-28},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Pal, Avik and Ma, Yingbo and Shah, Viral and Rackauckas, Christopher V.},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {8325--8335},
	file = {Full Text PDF:/home/x/Documents/inpe/library-ml-zotero/storage/EIS8RBUM/Pal et al. - 2021 - Opening the Blackbox Accelerating Neural Differen.pdf:application/pdf},
}

@misc{qiu_accuracy_2021,
	title = {Accuracy and {Architecture} {Studies} of {Residual} {Neural} {Network} solving {Ordinary} {Differential} {Equations}},
	shorttitle = {{ODE} {Accuracy}},
	url = {http://arxiv.org/abs/2101.03583},
	doi = {10.48550/arXiv.2101.03583},
	abstract = {In this paper we consider utilizing a residual neural network (ResNet) to solve ordinary differential equations. Stochastic gradient descent method is applied to obtain the optimal parameter set of weights and biases of the network. We apply forward Euler, Runge-Kutta2 and Runge-Kutta4 finite difference methods to generate three sets of targets training the ResNet and carry out the target study. The well trained ResNet behaves just as its counterpart of the corresponding one-step finite difference method. In particular, we carry out (1) the architecture study in terms of number of hidden layers and neurons per layer to find the optimal ResNet structure; (2) the target study to verify the ResNet solver behaves as accurate as its finite difference method counterpart; (3) solution trajectory simulation. Even the ResNet solver looks like and is implemented in a way similar to forward Euler scheme, its accuracy can be as high as any one step method. A sequence of numerical examples are presented to demonstrate the performance of the ResNet solver.},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Qiu, Changxin and Bendickson, Aaron and Kalyanapu, Joshua and Yan, Jue},
	month = jan,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Mathematics - Dynamical Systems},
	file = {arXiv Fulltext PDF:/home/x/Documents/inpe/library-ml-zotero/storage/TC3TSVUG/Qiu et al. - 2021 - Accuracy and Architecture Studies of Residual Neur.pdf:application/pdf;arXiv.org Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/7FTPMD6S/2101.html:text/html},
}

@misc{kidger_neural_2022,
	title = {On {Neural} {Differential} {Equations}},
	shorttitle = {On {NDE}},
	url = {http://arxiv.org/abs/2202.02435},
	doi = {10.48550/arXiv.2202.02435},
	abstract = {The conjoining of dynamical systems and deep learning has become a topic of great interest. In particular, neural differential equations (NDEs) demonstrate that neural networks and differential equation are two sides of the same coin. Traditional parameterised differential equations are a special case. Many popular neural network architectures, such as residual networks and recurrent networks, are discretisations. NDEs are suitable for tackling generative problems, dynamical systems, and time series (particularly in physics, finance, ...) and are thus of interest to both modern machine learning and traditional mathematical modelling. NDEs offer high-capacity function approximation, strong priors on model space, the ability to handle irregular data, memory efficiency, and a wealth of available theory on both sides. This doctoral thesis provides an in-depth survey of the field. Topics include: neural ordinary differential equations (e.g. for hybrid neural/mechanistic modelling of physical systems); neural controlled differential equations (e.g. for learning functions of irregular time series); and neural stochastic differential equations (e.g. to produce generative models capable of representing complex stochastic dynamics, or sampling from complex high-dimensional distributions). Further topics include: numerical methods for NDEs (e.g. reversible differential equations solvers, backpropagation through differential equations, Brownian reconstruction); symbolic regression for dynamical systems (e.g. via regularised evolution); and deep implicit models (e.g. deep equilibrium models, differentiable optimisation). We anticipate this thesis will be of interest to anyone interested in the marriage of deep learning with dynamical systems, and hope it will provide a useful reference for the current state of the art.},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Kidger, Patrick},
	month = feb,
	year = {2022},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Numerical Analysis, Mathematics - Dynamical Systems, Mathematics - Classical Analysis and ODEs},
	file = {arXiv Fulltext PDF:/home/x/Documents/inpe/library-ml-zotero/storage/DF8V2MGA/Kidger - 2022 - On Neural Differential Equations.pdf:application/pdf;arXiv.org Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/QICQGJ6Q/2202.html:text/html},
}

@article{bolten_sde_2019,
	title = {An {SDE} waveform-relaxation method with application in distributed neural network simulations},
	volume = {19},
	issn = {1617-7061},
	shorttitle = {Distributed {NN}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pamm.201900373},
	doi = {10.1002/pamm.201900373},
	abstract = {Waveform-relaxation methods are a set of iterative methods to solve systems of differential equations by dividing them into subsystems. Several of these methods, such as the Jacobi waveform-relaxation method, enable potential for parallelization across the system and are for that reason interesting in applications with a highly parallel setting. Here we present an SDE waveform-relaxation methods with applications in the fields of computational neuroscience. We give a short overview how and where the application of the method can speed up the simulation of functionally inspired rate-based units in a distributed neural network simulator that was originally designed for biologically grounded spiking neuron models.},
	language = {en},
	number = {1},
	urldate = {2023-04-28},
	journal = {PAMM},
	author = {Bolten, Matthias and Hahne, Jan},
	year = {2019},
	pages = {e201900373},
	file = {Bolten and Hahne - 2019 - An SDE waveform-relaxation method with application.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/DXBDE4V3/Bolten and Hahne - 2019 - An SDE waveform-relaxation method with application.pdf:application/pdf;Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/E6JH6VLX/pamm.html:text/html},
}

@article{wang_massive_2019,
	title = {Massive computational acceleration by using neural networks to emulate mechanism-based biological models},
	volume = {10},
	issn = {2041-1723},
	shorttitle = {{ANN} performance},
	url = {https://www.nature.com/articles/s41467-019-12342-y},
	doi = {10.1038/s41467-019-12342-y},
	abstract = {For many biological applications, exploration of the massive parametric space of a mechanism-based model can impose a prohibitive computational demand. To overcome this limitation, we present a framework to improve computational efficiency by orders of magnitude. The key concept is to train a neural network using a limited number of simulations generated by a mechanistic model. This number is small enough such that the simulations can be completed in a short time frame but large enough to enable reliable training. The trained neural network can then be used to explore a much larger parametric space. We demonstrate this notion by training neural networks to predict pattern formation and stochastic gene expression. We further demonstrate that using an ensemble of neural networks enables the self-contained evaluation of the quality of each prediction. Our work can be a platform for fast parametric space screening of biological models with user defined objectives.},
	language = {en},
	number = {1},
	urldate = {2023-04-28},
	journal = {Nat Commun},
	author = {Wang, Shangying and Fan, Kai and Luo, Nan and Cao, Yangxiaolu and Wu, Feilun and Zhang, Carolyn and Heller, Katherine A. and You, Lingchong},
	month = sep,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Machine learning, High-throughput screening, Synthetic biology, Systems analysis},
	pages = {4354},
	file = {Full Text PDF:/home/x/Documents/inpe/library-ml-zotero/storage/Q7THB8MN/Wang et al. - 2019 - Massive computational acceleration by using neural.pdf:application/pdf},
}

@article{white_using_2021,
	title = {Using neural networks to reduce communication in numerical solution of partial differential equations},
	shorttitle = {{PINN} communication},
	url = {https://ml4physicalsciences.github.io/2021/files/NeurIPS_ML4PS_2021_12.pdf},
	author = {White, Laurent and Dasika, Ganesh and Rama, Saketh},
	year = {2021},
	file = {Full Text:/home/x/Documents/inpe/library-ml-zotero/storage/F4P29JFN/White et al. - Using neural networks to reduce communication in n.pdf:application/pdf},
}

@inproceedings{stiller_large-scale_2020,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Large-{Scale} {Neural} {Solvers} for {Partial} {Differential} {Equations}},
	isbn = {978-3-030-63393-6},
	shorttitle = {Large-scale {PINN}},
	url = {https://link.springer.com/chapter/10.1007/978-3-030-63393-6_2},
	doi = {10.1007/978-3-030-63393-6_2},
	abstract = {Solving partial differential equations (PDE) is an indispensable part of many branches of science as many processes can be modelled in terms of PDEs. However, recent numerical solvers require manual discretization of the underlying equation as well as sophisticated, tailored code for distributed computing. Scanning the parameters of the underlying model significantly increases the runtime as the simulations have to be cold-started for each parameter configuration. Machine Learning based surrogate models denote promising ways for learning complex relationship among input, parameter and solution. However, recent generative neural networks require lots of training data, i.e. full simulation runs making them costly. In contrast, we examine the applicability of continuous, mesh-free neural solvers for partial differential equations, physics-informed neural networks (PINNs) solely requiring initial/boundary values and validation points for training but no simulation data. The induced curse of dimensionality is approached by learning a domain decomposition that steers the number of neurons per unit volume and significantly improves runtime. Distributed training on large-scale cluster systems also promises great utilization of large quantities of GPUs which we assess by a comprehensive evaluation study. Finally, we discuss the accuracy of GatedPINN with respect to analytical solutions- as well as state-of-the-art numerical solvers, such as spectral solvers.},
	language = {en},
	booktitle = {Driving {Scientific} and {Engineering} {Discoveries} {Through} the {Convergence} of {HPC}, {Big} {Data} and {AI}},
	publisher = {Springer International Publishing},
	author = {Stiller, Patrick and Bethke, Friedrich and Böhme, Maximilian and Pausch, Richard and Torge, Sunna and Debus, Alexander and Vorberger, Jan and Bussmann, Michael and Hoffmann, Nico},
	editor = {Nichols, Jeffrey and Verastegui, Becky and Maccabe, Arthur ‘Barney’ and Hernandez, Oscar and Parete-Koon, Suzanne and Ahearn, Theresa},
	year = {2020},
	pages = {20--34},
	file = {Submitted Version:/home/x/Documents/inpe/library-ml-zotero/storage/94YVREYS/Stiller et al. - 2020 - Large-Scale Neural Solvers for Partial Differentia.pdf:application/pdf},
}

@inproceedings{dumont_hyppo_2021,
	title = {{HYPPO}: {A} {Surrogate}-{Based} {Multi}-{Level} {Parallelism} {Tool} for {Hyperparameter} {Optimization}},
	shorttitle = {{HYPPO} library},
	url = {https://ieeexplore.ieee.org/document/9653176},
	doi = {10.1109/MLHPC54614.2021.00013},
	abstract = {We present a new software, HYPPO, that enables the automatic tuning of hyperparameters of various deep learning (DL) models. Unlike other hyperparameter optimization (HPO) methods, HYPPO uses adaptive surrogate models and directly accounts for uncertainty in model predictions to find accurate and reliable models that make robust predictions. Using asynchronous nested parallelism, we are able to significantly alleviate the computational burden of training complex architectures and quantifying the uncertainty. HYPPO is implemented in Python and can be used with both TensorFlow and PyTorch libraries. We demonstrate various software features on time-series prediction and image classification problems as well as a scientific application in computed tomography image reconstruction. Finally, we show that (1) we can reduce by an order of magnitude the number of evaluations necessary to find the most optimal region in the hyperparameter space and (2) we can reduce by two orders of magnitude the throughput for such HPO process to complete.},
	booktitle = {2021 {IEEE}/{ACM} {Workshop} on {Machine} {Learning} in {High} {Performance} {Computing} {Environments} ({MLHPC})},
	author = {Dumont, Vincent and Garner, Casey and Trivedi, Anuradha and Jones, Chelsea and Ganapati, Vidya and Mueller, Juliane and Perciano, Talita and Kiran, Mariam and Day, Marc},
	month = nov,
	year = {2021},
	note = {ISSN: 2768-4253},
	keywords = {Adaptation models, Computational modeling, Parallel processing, Predictive models, Stochastic processes, Training, Uncertainty},
	pages = {81--93},
	file = {IEEE Xplore Abstract Record:/home/x/Documents/inpe/library-ml-zotero/storage/XU4BY87M/9653176.html:text/html;Submitted Version:/home/x/Documents/inpe/library-ml-zotero/storage/89ERCXHN/Dumont et al. - 2021 - HYPPO A Surrogate-Based Multi-Level Parallelism T.pdf:application/pdf},
}

@phdthesis{lakshmiranganatha_hpc_2021,
	title = {{HPC} and {Machine} {Learning} {Techniques} for {Reducing} the {Computation} {Burden} of {Determining} {Time}-{Evolution} of {Complex} {Dynamic} {Systems}},
	copyright = {thesis},
	shorttitle = {{HPC} \& {ML}},
	url = {https://www.proquest.com/openview/7b44755c06084f2e3f628d0801664333/1},
	abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.},
	language = {en},
	urldate = {2023-04-28},
	author = {Lakshmiranganatha, Sumathi},
	year = {2021},
	file = {Lakshmiranganatha - 2021 - HPC and Machine Learning Techniques for Reducing t.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/JXRGSC8B/Lakshmiranganatha - 2021 - HPC and Machine Learning Techniques for Reducing t.pdf:application/pdf;Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/RT6NB4V9/1.html:text/html},
}

@phdthesis{corsini_deep_2021,
	title = {Deep learning methods for inverse supercontinuum generation},
	copyright = {Thesis},
	shorttitle = {Inverse supercontinuum ({Thesis})},
	url = {https://www.politesi.polimi.it/handle/10589/183786},
	urldate = {2023-04-28},
	author = {Corsini, Andrea},
	year = {2021},
	file = {Deep learning methods for inverse supercontinuum generation:/home/x/Documents/inpe/library-ml-zotero/storage/DXWPK935/183786.html:text/html},
}

@phdthesis{ramzi_advanced_2022,
	type = {phdthesis},
	title = {Advanced deep neural networks for {MRI} image reconstruction from highly undersampled data in challenging acquisition settings},
	copyright = {Thesis},
	shorttitle = {{ANN} \& {MRI}},
	url = {https://theses.hal.science/tel-03623570},
	abstract = {Magnetic Resonance Imaging (MRI) is one of the most prominent imaging techniques in the world. Its main purpose is to probe soft tissues in a non-invasive and non-ionizing way. However, its wider adoption is hindered by an overall high scan time. In order to reduce this duration, several approaches have been proposed, among which Parallel Imaging (PI) and Compressed Sensing (CS) are the most important. Using these techniques, MR data can be acquired in a highly compressed way which allows the reduction of acquisition times. However, the algorithms typically used to reconstruct the MR images from these undersampled data are slow and underperform in highly accelerated scenarios. In order to address these issues, unrolled neural networks have been introduced. The core idea of these models is to unroll the iterations of classical reconstruction algorithms into a finite computation graph. The main objective of this PhD thesis is to propose new architecture designs for acquisition scenarios which deviate from the typical Cartesian 2D sampling. To this end, we first review a handful of neural networks for MRI reconstruction. After selecting the best performer, the PDNet, we extend it to two contexts: the fastMRI 2020 reconstruction challenge and the 3D non-Cartesian data problem. We also chose to adress the concerns of many regarding the clinical applicability of deep learning for medical imaging. We do so by proposing ways to build robust and inspectable models, but also by simply testing the trained networks in out-of-distribution settings. Finally, after noticing how the implicit deep learning framework can help implement deeper MRI reconstruction models, we introduce a new acceleration method (called SHINE) for the training of such models.},
	language = {en},
	urldate = {2023-04-28},
	school = {Université Paris-Saclay},
	author = {Ramzi, Zaccharie},
	month = feb,
	year = {2022},
	file = {Ramzi - 2022 - Advanced deep neural networks for MRI image recons.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/75QXRKVH/Ramzi - 2022 - Advanced deep neural networks for MRI image recons.pdf:application/pdf},
}

@misc{honchar_neural_2017,
	title = {Neural networks for solving differential equations},
	copyright = {website},
	shorttitle = {{ANN} \& {ODE} (website)},
	url = {https://becominghuman.ai/neural-networks-for-solving-differential-equations-fa230ac5e04c},
	abstract = {We mostly know neural networks as big hierarchical models that can learn patterns from data with complicated nature or distribution. That’s…},
	language = {en},
	urldate = {2023-04-28},
	journal = {Medium},
	author = {Honchar, Alex},
	year = {2017},
	file = {Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/M8DLWJYR/neural-networks-for-solving-differential-equations-fa230ac5e04c.html:text/html},
}

@misc{parkhill_solving_2021,
	title = {Solving multidimensional {PDEs} in {Pytorch}},
	copyright = {website},
	shorttitle = {{PDEs} in {Pytorch} (website)},
	url = {https://jparkhill.netlify.app/SolvingDiffusions/},
	abstract = {Solving multi-dimensional partial differential equations (PDE’s) is something I’ve spent most of my adult life doing. Most of them are somewhat similar to the heat equation:},
	language = {en},
	urldate = {2023-04-28},
	journal = {jparkhill.github.io},
	author = {Parkhill, John},
	month = apr,
	year = {2021},
	file = {Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/DTETFR6G/SolvingDiffusions.html:text/html},
}

@misc{black_how_2021,
	title = {How {Machine} {Learning} {Is} {Revolutionizing} {HPC} {Simulations}},
	copyright = {website},
	shorttitle = {{PINN} and {HPC} (website)},
	url = {https://insidehpc.com/2021/08/how-machine-learning-is-revolutionizing-hpc-simulations/},
	abstract = {Physics-based simulations, that staple of traditional HPC, may be evolving toward an emerging, AI-based technique that could radically accelerate simulation runs while cutting costs. Called “surrogate machine learning models,” the topic was a focal point in a keynote on Tuesday at the International Conference on Parallel Processing by Argonne National Lab’s Rick Stevens. Stevens, ANL’s […]},
	language = {en-US},
	urldate = {2023-04-28},
	journal = {High-Performance Computing News Analysis {\textbar} insideHPC},
	author = {Black, Doug},
	month = aug,
	year = {2021},
	file = {Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/AVLZ88I2/how-machine-learning-is-revolutionizing-hpc-simulations.html:text/html},
}

@misc{pareti_surrogate_2021,
	title = {Surrogate {Models} integrating an {HPC} solver and a {Machine} {Learning} component},
	copyright = {website},
	shorttitle = {Weather predictions (website)},
	url = {https://docs.google.com/document/d/1t8HJsiVelvTmsxFUxR9YHXnYgEpY0DX42i5tdARQ0Kk/edit},
	abstract = {Surrogate Models integrating an HPC Solver and a Machine Learning Component  Overview	1 Nvidia	1 Deep Learning for Turbulence Modeling	2 Large Eddy Simulation, or LES	3 PyTorch-based Lattice Boltzmann Framework	3 Conversation with the Authors of the LBM paper	3 SmartSim: Using Machine Learning a...},
	language = {en-GB},
	urldate = {2023-04-28},
	journal = {Google Docs},
	author = {Pareti, Joseph},
	year = {2021},
	file = {Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/5EPJHD3Y/edit.html:text/html},
}

@article{barker_monte_2008,
	title = {The {Monte} {Carlo} {Independent} {Column} {Approximation}: {An} {Assessment} {Using} {Several} {Global} {Atmospheric} {Models}},
	volume = {134},
	copyright = {Atmospheric},
	shorttitle = {Monte {Carlo} {Atmospheric} {Model}},
	number = {635},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Barker, H. W. and Cole, J. N. S. and Morcrette, J.-J. and Pincus, R. and Räisänen, P. and von Salzen, K. and Vaillancourt, P. A.},
	year = {2008},
	pages = {1463--1478},
	file = {Barker et al. - 2008 - The Monte Carlo Independent Column Approximation .pdf:/home/x/Documents/inpe/library-ml-zotero/storage/WYAEBPR3/Barker et al. - 2008 - The Monte Carlo Independent Column Approximation .pdf:application/pdf},
}

@article{burkardt_investigating_2013,
	title = {Investigating {Uncertain} {Parameters} in the {Burgers} {Equation}},
	shorttitle = {Burgers equation},
	url = {https://people.sc.fsu.edu/~jburkardt/presentations/presentations.html},
	author = {Burkardt, John},
	year = {2013},
	note = {Presentation: https://people.sc.fsu.edu/{\textasciitilde}jburkardt/presentations/burgers\_2013\_ajou.pdf},
	file = {Burkardt - 2013 - Investigating Uncertain Parameters in the Burgers .pdf:/home/x/Documents/inpe/library-ml-zotero/storage/A8QCY5FU/Burkardt - 2013 - Investigating Uncertain Parameters in the Burgers .pdf:application/pdf},
}

@book{chollet_deep_2018,
	title = {Deep {Learning} with {Python}},
	copyright = {Book},
	shorttitle = {Deep {Learning} (book)},
	publisher = {Manning Publications Co), oCLC: ocn982650571},
	author = {Chollet, F.},
	year = {2018},
}

@unpublished{noauthor_ecrad_2022,
	title = {{ECRAD} - {ECMWF} {Atmospheric} {Radiation} {Scheme}},
	shorttitle = {{ECRAD} (website)},
	url = {https://github.com/ecmwf-ifs/ecrad},
	abstract = {ECMWF atmospheric radiation scheme},
	urldate = {2022-10-08},
	month = sep,
	year = {2022},
}

@unpublished{noauthor_comite_nodate,
	title = {Comitê {Científico} {Do} {MONAN}},
	shorttitle = {{MONAN} (website)},
	url = {https://monanadmin.github.io/monan_cc_docs/},
	urldate = {2022-10-08},
}

@book{patterson_deep_2017,
	title = {Deep {Learning}: {A} {Practitioner}'s {Approach}},
	copyright = {Book},
	shorttitle = {Deel {Learning} (book)},
	publisher = {" O'Reilly Media, Inc."},
	author = {Patterson, Josh and Gibson, Adam},
	year = {2017},
}

@book{raschka_python_2019,
	title = {Python {Machine} {Learning}: {Machine} {Learning} and {Deep} {Learning} with {Python}, {Scikit}-{Learn}, and {TensorFlow} 2},
	copyright = {Book},
	shorttitle = {{ML} \& {Python} (book)},
	publisher = {Packt Publishing Ltd},
	author = {Raschka, Sebastian and Mirjalili, Vahid},
	year = {2019},
}

@misc{chen_neural_2019,
	title = {Neural {Ordinary} {Differential} {Equations}},
	shorttitle = {Neural {ODE} ({NIPS2018})},
	doi = {10.48550/arXiv.1806.07366},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
	publisher = {arXiv},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
	month = dec,
	year = {2019},
	note = {Sources: https://github.com/rtqichen/torchdiffeq},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/MYKG3RWG/1806.html:text/html;Chen et al. - 2019 - Neural Ordinary Differential Equations (1).pdf:/home/x/Documents/inpe/library-ml-zotero/storage/WMM4F4LE/Chen et al. - 2019 - Neural Ordinary Differential Equations (1).pdf:application/pdf;Chen et al. - 2019 - Neural Ordinary Differential Equations (2).pdf:/home/x/Documents/inpe/library-ml-zotero/storage/BVFGSLVI/Chen et al. - 2019 - Neural Ordinary Differential Equations (2).pdf:application/pdf},
}

@article{kasim_building_2022,
	title = {Building high accuracy emulators for scientific simulations with deep neural architecture search},
	volume = {3},
	copyright = {2 billion times speedup},
	issn = {2632-2153},
	shorttitle = {{ANN} {Performance}},
	url = {http://arxiv.org/abs/2001.08055},
	doi = {10.1088/2632-2153/ac3ffa},
	abstract = {Computer simulations are invaluable tools for scientific discovery. However, accurate simulations are often slow to execute, which limits their applicability to extensive parameter exploration, large-scale data analysis, and uncertainty quantification. A promising route to accelerate simulations by building fast emulators with machine learning requires large training datasets, which can be prohibitively expensive to obtain with slow simulations. Here we present a method based on neural architecture search to build accurate emulators even with a limited number of training data. The method successfully accelerates simulations by up to 2 billion times in 10 scientific cases including astrophysics, climate science, biogeochemistry, high energy density physics, fusion energy, and seismology, using the same super-architecture, algorithm, and hyperparameters. Our approach also inherently provides emulator uncertainty estimation, adding further confidence in their use. We anticipate this work will accelerate research involving expensive simulations, allow more extensive parameters exploration, and enable new, previously unfeasible computational discovery.},
	number = {1},
	urldate = {2023-05-01},
	journal = {Mach. Learn.: Sci. Technol.},
	author = {Kasim, M. F. and Watson-Parris, D. and Deaconu, L. and Oliver, S. and Hatfield, P. and Froula, D. H. and Gregori, G. and Jarvis, M. and Khatiwala, S. and Korenaga, J. and Topp-Mugglestone, J. and Viezzer, E. and Vinko, S. M.},
	month = mar,
	year = {2022},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics, Statistics - Machine Learning, Physics - Atmospheric and Oceanic Physics, Physics - Plasma Physics},
	pages = {015013},
	file = {arXiv.org Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/Z4I9DE9Y/2001.html:text/html;Kasim et al. - 2022 - Building high accuracy emulators for scientific si.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/FLICKXIF/Kasim et al. - 2022 - Building high accuracy emulators for scientific si.pdf:application/pdf},
}

@article{hogan_incorporating_2013,
	title = {Incorporating the {Effects} of {3D} {Radiative} {Transfer} in the {Presence} of {Clouds} into {Two}-{Stream} {Multilayer} {Radiation} {Schemes}},
	volume = {70},
	issn = {0022-4928, 1520-0469},
	shorttitle = {Incorporating {3D} radiation},
	url = {https://journals.ametsoc.org/doi/10.1175/JAS-D-12-041.1},
	doi = {10.1175/JAS-D-12-041.1},
	abstract = {Abstract
            This paper presents a new method for representing the important effects of horizontal radiation transport through cloud sides in two-stream radiation schemes. Ordinarily, the radiative transfer equations are discretized separately for the clear and cloudy regions within each model level, but here terms are introduced that represent the exchange of radiation laterally between regions and the resulting coupled equations are solved for each layer. This approach may be taken with both the direct incoming shortwave radiation, which is governed by Beer’s law, and the diffuse shortwave and longwave radiation, governed by the two-stream equations. The rate of lateral exchange is determined by the area of cloud “edge.” The validity of the method is demonstrated by comparing with rigorous 3D radiative transfer calculations in the literature for two cloud types in which the 3D effect is strong, specifically cumulus and aircraft contrails. The 3D effect on shortwave cloud radiative forcing varies between around −25\% and around +100\%, depending on solar zenith angle. Even with an otherwise very simplistic representation of the cloud, the new scheme exhibits good agreement with the rigorous calculations in the shortwave, opening the way for efficient yet accurate representation of this important effect in climate models.},
	language = {en},
	number = {2},
	urldate = {2023-05-01},
	journal = {Journal of the Atmospheric Sciences},
	author = {Hogan, Robin J. and Shonk, Jonathan K. P.},
	month = feb,
	year = {2013},
	pages = {708--724},
	file = {Hogan and Shonk - 2013 - Incorporating the Effects of 3D Radiative Transfer.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/MNCGIVQZ/Hogan and Shonk - 2013 - Incorporating the Effects of 3D Radiative Transfer.pdf:application/pdf},
}

@article{schafer_representing_2016,
	title = {Representing 3-{D} cloud radiation effects in two-stream schemes: 1. {Longwave} considerations and effective cloud edge length},
	volume = {121},
	copyright = {Radiation 3D},
	issn = {2169897X},
	shorttitle = {Representing {3D} radiation},
	url = {http://doi.wiley.com/10.1002/2016JD024876},
	doi = {10.1002/2016JD024876},
	language = {en},
	number = {14},
	urldate = {2023-05-01},
	journal = {J. Geophys. Res. Atmos.},
	author = {Schäfer, Sophia A. K. and Hogan, Robin J. and Klinger, Carolin and Chiu, J. Christine and Mayer, Bernhard},
	month = jul,
	year = {2016},
	pages = {8567--8582},
	file = {Schäfer et al. - 2016 - Representing 3-D cloud radiation effects in two-st.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/FJ36PZ27/Schäfer et al. - 2016 - Representing 3-D cloud radiation effects in two-st.pdf:application/pdf},
}

@article{shonk_tripleclouds_2008,
	title = {Tripleclouds: {An} {Efficient} {Method} for {Representing} {Horizontal} {Cloud} {Inhomogeneity} in {1D} {Radiation} {Schemes} by {Using} {Three} {Regions} at {Each} {Height}},
	volume = {21},
	copyright = {Radiation},
	issn = {1520-0442, 0894-8755},
	shorttitle = {Radiation {1D}},
	url = {http://journals.ametsoc.org/doi/10.1175/2007JCLI1940.1},
	doi = {10.1175/2007JCLI1940.1},
	abstract = {Abstract
            Radiation schemes in general circulation models currently make a number of simplifications when accounting for clouds, one of the most important being the removal of horizontal inhomogeneity. A new scheme is presented that attempts to account for the neglected inhomogeneity by using two regions of cloud in each vertical level of the model as opposed to one. One of these regions is used to represent the optically thinner cloud in the level, and the other represents the optically thicker cloud. So, along with the clear-sky region, the scheme has three regions in each model level and is referred to as “Tripleclouds.” In addition, the scheme has the capability to represent arbitrary vertical overlap between the three regions in pairs of adjacent levels. This scheme is implemented in the Edwards–Slingo radiation code and tested on 250 h of data from 12 different days. The data are derived from cloud retrievals using radar, lidar, and a microwave radiometer at Chilbolton, southern United Kingdom. When the data are grouped into periods equivalent in size to general circulation model grid boxes, the shortwave plane-parallel albedo bias is found to be 8\%, while the corresponding bias is found to be less than 1\% using Tripleclouds. Similar results are found for the longwave biases. Tripleclouds is then compared to a more conventional method of accounting for inhomogeneity that multiplies optical depths by a constant scaling factor, and Tripleclouds is seen to improve on this method both in terms of top-of-atmosphere radiative flux biases and internal heating rates.},
	language = {en},
	number = {11},
	urldate = {2023-05-01},
	journal = {Journal of Climate},
	author = {Shonk, Jonathan K. P. and Hogan, Robin J.},
	month = jun,
	year = {2008},
	pages = {2352--2370},
	file = {Shonk and Hogan - 2008 - Tripleclouds An Efficient Method for Representing.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/CC6VFYVQ/Shonk and Hogan - 2008 - Tripleclouds An Efficient Method for Representing.pdf:application/pdf},
}

@misc{bragone_physics-informed_2021,
	title = {Physics-informed machine learning in power transformer dynamic thermal modelling},
	shorttitle = {{PINN} thermal modelling},
	author = {Bragone, Federica},
	year = {2021},
	file = {Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/45IFHW7G/record.html:text/html},
}

@article{krestenitis_oil_2019,
	title = {Oil {Spill} {Identification} from {Satellite} {Images} {Using} {Deep} {Neural} {Networks}},
	volume = {11},
	copyright = {Palestra 230504},
	issn = {2072-4292},
	shorttitle = {Oil spill {ANN}},
	url = {https://www.mdpi.com/2072-4292/11/15/1762},
	doi = {10.3390/rs11151762},
	abstract = {Oil spill is considered one of the main threats to marine and coastal environments. Efficient monitoring and early identification of oil slicks are vital for the corresponding authorities to react expediently, confine the environmental pollution and avoid further damage. Synthetic aperture radar (SAR) sensors are commonly used for this objective due to their capability for operating efficiently regardless of the weather and illumination conditions. Black spots probably related to oil spills can be clearly captured by SAR sensors, yet their discrimination from look-alikes poses a challenging objective. A variety of different methods have been proposed to automatically detect and classify these dark spots. Most of them employ custom-made datasets posing results as non-comparable. Moreover, in most cases, a single label is assigned to the entire SAR image resulting in a difficulties when manipulating complex scenarios or extracting further information from the depicted content. To overcome these limitations, semantic segmentation with deep convolutional neural networks (DCNNs) is proposed as an efficient approach. Moreover, a publicly available SAR image dataset is introduced, aiming to consist a benchmark for future oil spill detection methods. The presented dataset is employed to review the performance of well-known DCNN segmentation models in the specific task. DeepLabv3+ presented the best performance, in terms of test set accuracy and related inference time. Furthermore, the complex nature of the specific problem, especially due to the challenging task of discriminating oil spills and look-alikes is discussed and illustrated, utilizing the introduced dataset. Results imply that DCNN segmentation models, trained and evaluated on the provided dataset, can be utilized to implement efficient oil spill detectors. Current work is expected to contribute significantly to the future research activity regarding oil spill identification and SAR image processing.},
	language = {en},
	number = {15},
	urldate = {2023-05-04},
	journal = {Remote Sensing},
	author = {Krestenitis, Marios and Orfanidis, Georgios and Ioannidis, Konstantinos and Avgerinakis, Konstantinos and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
	month = jan,
	year = {2019},
	keywords = {deep convolutional neural networks, oil spill detection, remote sensing, SAR imagery, semantic image segmentation},
	pages = {1762},
	file = {Full Text PDF:/home/x/Documents/inpe/library-ml-zotero/storage/THQ2FWE5/Krestenitis et al. - 2019 - Oil Spill Identification from Satellite Images Usi.pdf:application/pdf},
}

@misc{raval_neural_2019,
	title = {Neural {Differential} {Equations}},
	copyright = {Video},
	shorttitle = {{NDE} (video)},
	url = {https://youtu.be/AD3K8j12EIE},
	abstract = {This won the best paper award at NeurIPS (the biggest AI conference of the year) out of over 4800 other research papers! Neural Ordinary Differential Equations is the official name of the paper and in it the authors introduce a new type of neural network. This new network doesn't have any layers! Its framed as a differential equation, which allows us to use differential equation solvers on it to approximate the underlying function of time series data. Its very cool and will ultimately allow us to learn from irregular time series datasets more efficiently, which applies to many different industries. I'll cover all the prerequisites in this video and point to helpful resources down below. Enjoy! 

Code for this video:
https://github.com/llSourcell/Neural\_Differential\_Equations/

More learning resources:
   • Backpropagation i...  https://www.youtube.com/watch?v=q555kfIFUCM\&t=0s
   • Build a Neural Ne...  https://www.youtube.com/watch?v=h3l4qz76JhQ\&t=9s
   • The essence of ca...  https://www.youtube.com/watch?v=WUvTyaaNkzM\&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr
https://towardsdatascience.com/paper-summary-neural-ordinary-differential-equations-37c4e52df128
https://arxiv.org/abs/1806.07366
https://blog.acolyer.org/2019/01/09/neural-ordinary-differential-equations/
https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/},
	urldate = {2023-05-04},
	collaborator = {Raval, Siraj},
	month = jan,
	year = {2019},
	note = {Sources: https://github.com/llSourcell/Neural\_Differential\_Equations},
}

@misc{ali_mit_2023,
	title = {{MIT} {Deep} {Learning} {Book} (beautiful and flawless {PDF} version)},
	shorttitle = {{MIT} {Deep} {Learning}},
	url = {https://github.com/janishar/mit-deep-learning-book-pdf},
	abstract = {MIT Deep Learning Book in PDF format (complete and parts) by Ian Goodfellow, Yoshua Bengio and Aaron Courville},
	urldate = {2023-05-18},
	author = {Ali, Janishar},
	month = may,
	year = {2023},
	note = {https://www.deeplearningbook.org/},
	keywords = {book, chapter, clear, deep-learning, deeplearning, excercises, good, learning, lecture-notes, linear-algebra, machine, machine-learning, mit, neural-network, neural-networks, pdf, print, printable, thinking},
}

@inproceedings{yin_strategies_2022,
	address = {Lyon, France},
	title = {Strategies for {Integrating} {Deep} {Learning} {Surrogate} {Models} with {HPC} {Simulation} {Applications}},
	isbn = {978-1-66549-747-3},
	url = {https://ieeexplore.ieee.org/document/9835386/},
	doi = {10.1109/IPDPSW55747.2022.00222},
	abstract = {The emerging trend of the convergence of high performance computing (HPC), machine learning/deep learning (ML/DL), and big data analytics presents a host of challenges for large-scale computing campaigns that seek best practices to interleave traditional scientiﬁc simulation-based workloads with ML/DL models. A portfolio of systematic approaches to incorporate deep learning into modeling and simulation serves a vital need when we support AI for science at a computing facility. In this paper, we evaluate several strategies for deploying deep learning surrogate models in a representative physics application on supercomputers at the Oak Ridge Leadership Computing Facility (OLCF). We discuss a set of recommended deployment architectures and implementation approaches. We analyze and evaluate these alternatives and show their performance and scalability up to 1000 GPUs on two mainstream platforms equipped with different deep learning hardware and software stacks.},
	language = {en},
	urldate = {2023-05-18},
	booktitle = {2022 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} ({IPDPSW})},
	publisher = {IEEE},
	author = {Yin, Junqi and Wang, Feiyi and Shankar, Mallikarjun},
	month = may,
	year = {2022},
	pages = {01--10},
	file = {Yin et al. - 2022 - Strategies for Integrating Deep Learning Surrogate.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/MLLNYPQD/Yin et al. - 2022 - Strategies for Integrating Deep Learning Surrogate.pdf:application/pdf},
}

@misc{noauthor_online_nodate,
	title = {Online {Machine} {Learning} for {Exascale} {CFD} {\textbar} {Argonne} {Leadership} {Computing} {Facility}},
	url = {https://www.alcf.anl.gov/support-center/training-assets/online-machine-learning-exascale-cfd},
	urldate = {2023-05-18},
	file = {Online Machine Learning for Exascale CFD  Argonne.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/YFQT7RTQ/Online Machine Learning for Exascale CFD  Argonne.pdf:application/pdf;Online Machine Learning for Exascale CFD | Argonne Leadership Computing Facility:/home/x/Documents/inpe/library-ml-zotero/storage/BEMYBFUX/online-machine-learning-exascale-cfd.html:text/html},
}

@article{partee_using_2022,
	title = {Using {Machine} {Learning} at scale in numerical simulations with {SmartSim}: {An} application to ocean climate modeling},
	volume = {62},
	issn = {1877-7503},
	shorttitle = {Using {Machine} {Learning} at scale in numerical simulations with {SmartSim}},
	url = {https://www.sciencedirect.com/science/article/pii/S1877750322001065},
	doi = {10.1016/j.jocs.2022.101707},
	abstract = {We demonstrate the first climate-scale, numerical ocean simulations improved through distributed, online inference of Deep Neural Networks (DNN) using SmartSim. SmartSim is a library dedicated to enabling online analysis and Machine Learning (ML) for high performance, numerical simulations. In this paper, we detail the SmartSim architecture and provide benchmarks including online inference with a shared ML model, EKE-ResNet, on heterogeneous HPC systems. We demonstrate the capability of SmartSim by using it to run a 12-member ensemble of global-scale, high-resolution ocean simulations, each spanning 19 compute nodes, all communicating with the same ML architecture at each simulation timestep. In total, 970 billion inferences are collectively served by running the ensemble for a total of 120 simulated years. The inferences are used to predict the oceanic eddy kinetic energy (EKE), which is a variable that is used to tune different turbulence closures in the model and thus directly affects the simulation. The root-mean-square of the error in EKE (as compared to an eddy-resolving simulation) is 20\% lower when using the ML-prediction than the previous state of the art. This demonstration is an example of how machine learning methods can be integrated into traditional numerical simulations, replace prognostic equations, and preserve overall simulation stability without significantly affecting the time to solution.},
	language = {en},
	urldate = {2023-05-18},
	journal = {Journal of Computational Science},
	author = {Partee, Sam and Ellis, Matthew and Rigazzi, Alessandro and Shao, Andrew E. and Bachman, Scott and Marques, Gustavo and Robbins, Benjamin},
	month = jul,
	year = {2022},
	keywords = {Climate modeling, Deep learning, High performance computing, Numerical simulation, SmartSim},
	pages = {101707},
	file = {Partee et al. - 2021 - Preprint - Using Machine Learning at Scale in HPC Simulations.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/4GYATY9V/Partee et al. - 2021 - Preprint - Using Machine Learning at Scale in HPC Simulations.pdf:application/pdf;Partee et al. - 2022 - Using Machine Learning at scale in numerical simul.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/GSCH643S/Partee et al. - 2022 - Using Machine Learning at scale in numerical simul.pdf:application/pdf;ScienceDirect Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/ZGWVFPAL/S1877750322001065.html:text/html},
}

@misc{noauthor_online_nodate-1,
	title = {Online {Learning} with {SmartSim} {\textbar} {Argonne} {Leadership} {Computing} {Facility}},
	url = {https://www.alcf.anl.gov/support-center/training-assets/online-learning-smartsim},
	urldate = {2023-05-18},
	file = {Online Learning with SmartSim  Argonne Leadership.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/X3E4SLPB/Online Learning with SmartSim  Argonne Leadership.pdf:application/pdf;Online Learning with SmartSim | Argonne Leadership Computing Facility:/home/x/Documents/inpe/library-ml-zotero/storage/EJA9SF7J/online-learning-smartsim.html:text/html},
}

@misc{noauthor_argonne_2021,
	title = {Argonne {Annual} {Report}},
	year = {2021},
	file = {ALCF_2021AR.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/2XNADXEV/ALCF_2021AR.pdf:application/pdf},
}

@article{kurz_relexi_2022,
	title = {Relexi — {A} scalable open source reinforcement learning framework for high-performance computing},
	volume = {14},
	issn = {26659638},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2665963822001063},
	doi = {10.1016/j.simpa.2022.100422},
	abstract = {Relexi is an open source reinforcement learning (RL) framework written in Python and based on TensorFlow’s RL library TF-Agents. Relexi allows to employ RL for environments that require computationally intensive simulations like applications in computational fluid dynamics. For this, Relexi couples legacy simulation codes with the RL library TF-Agents at scale on modern high-performance computing (HPC) hardware using the SmartSim library. Relexi thus provides an easy way to explore the potential of RL for HPC applications.},
	language = {en},
	urldate = {2023-05-18},
	journal = {Software Impacts},
	author = {Kurz, Marius and Offenhäuser, Philipp and Viola, Dominic and Resch, Michael and Beck, Andrea},
	month = dec,
	year = {2022},
	pages = {100422},
	file = {Kurz et al. - 2022 - Relexi — A scalable open source reinforcement lear.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/DVWU88Q3/Kurz et al. - 2022 - Relexi — A scalable open source reinforcement lear.pdf:application/pdf},
}

@inproceedings{ward_colmena_2021,
	address = {St. Louis, MO, USA},
	title = {Colmena: {Scalable} {Machine}-{Learning}-{Based} {Steering} of {Ensemble} {Simulations} for {High} {Performance} {Computing}},
	isbn = {978-1-66541-124-0},
	shorttitle = {Colmena},
	url = {https://ieeexplore.ieee.org/document/9653177/},
	doi = {10.1109/MLHPC54614.2021.00007},
	abstract = {Scientiﬁc applications that involve simulation ensembles can be accelerated greatly by using experiment design methods to select the best simulations to perform. Methods that use machine learning (ML) to create proxy models of simulations show particular promise for guiding ensembles but are challenging to deploy because of the need to coordinate dynamic mixes of simulation and learning tasks. We present Colmena, an open-source Python framework that allows users to steer campaigns by providing just the implementations of individual tasks plus the logic used to choose which tasks to execute when. Colmena handles task dispatch, results collation, ML model invocation, and ML model (re)training, using Parsl to execute tasks on HPC systems. We describe the design of Colmena and illustrate its capabilities by applying it to electrolyte design, where it both scales to 65 536 CPUs and accelerates the discovery rate for high-performance molecules by a factor of 100 over unguided searches.},
	language = {en},
	urldate = {2023-05-18},
	booktitle = {2021 {IEEE}/{ACM} {Workshop} on {Machine} {Learning} in {High} {Performance} {Computing} {Environments} ({MLHPC})},
	publisher = {IEEE},
	author = {Ward, Logan and Sivaraman, Ganesh and Pauloski, J. Gregory and Babuji, Yadu and Chard, Ryan and Dandu, Naveen and Redfern, Paul C. and Assary, Rajeev S. and Chard, Kyle and Curtiss, Larry A. and Thakur, Rajeev and Foster, Ian},
	month = nov,
	year = {2021},
	pages = {9--20},
	file = {Ward et al. - 2021 - Colmena Scalable Machine-Learning-Based Steering .pdf:/home/x/Documents/inpe/library-ml-zotero/storage/TMFDN959/Ward et al. - 2021 - Colmena Scalable Machine-Learning-Based Steering .pdf:application/pdf},
}

@article{jansen_energy_2015,
	title = {Energy budget-based backscatter in an eddy permitting primitive equation model},
	volume = {94},
	issn = {1463-5003},
	url = {https://www.sciencedirect.com/science/article/pii/S1463500315001341},
	doi = {10.1016/j.ocemod.2015.07.015},
	abstract = {Increasing computational resources are starting to allow global ocean simulations at so-called “eddy-permitting” resolutions, at which the largest mesoscale eddies can be resolved explicitly. However, an adequate parameterization of the interactions with the unresolved part of the eddy energy spectrum remains crucial. Hyperviscous closures, which are commonly applied in eddy-permitting ocean models, cause spurious energy dissipation at these resolutions, leading to low levels of eddy kinetic energy (EKE) and weak eddy induced transports. It has recently been proposed to counteract the spurious energy dissipation of hyperviscous closures by an additional forcing term, which represents “backscatter” of energy from the un-resolved scales to the resolved scales. This study proposes a parameterization of energy backscatter based on an explicit sub-grid EKE budget. Energy dissipated by hyperviscosity acting on the resolved flow is added to the sub-grid EKE, while a backscatter term transfers energy back from the sub-grid EKE to the resolved flow. The backscatter term is formulated deterministically via a negative viscosity, which returns energy at somewhat larger scales than the hyperviscous dissipation, thus ensuring dissipation of enstrophy. The parameterization is tested in an idealized configuration of a primitive equation ocean model, and is shown to significantly improve the solutions of simulations at typical eddy-permitting resolutions.},
	language = {en},
	urldate = {2023-05-19},
	journal = {Ocean Modelling},
	author = {Jansen, Malte F. and Held, Isaac M. and Adcroft, Alistair and Hallberg, Robert},
	month = oct,
	year = {2015},
	keywords = {Backscatter, Eddy parameterization, Eddy-permitting, Energy budget, Mesoscale, Negative viscosity},
	pages = {15--26},
	file = {Jansen et al. - 2015 - Energy budget-based backscatter in an eddy permitt.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/7NE37LXI/Jansen et al. - 2015 - Energy budget-based backscatter in an eddy permitt.pdf:application/pdf;ScienceDirect Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/7TGEPW3K/S1463500315001341.html:text/html},
}

@article{monteiro_sympl_2018,
	title = {Sympl (v. 0.4.0) and climt (v. 0.15.3) - {Towards} a flexible framework for building model hierarchies in {Python}},
	volume = {11},
	doi = {10.5194/gmd-11-3781-2018},
	abstract = {sympl (System for Modelling Planets) and climt (Climate Modelling and Diagnostics Toolkit) are an attempt to rethink climate modelling frameworks from the ground up. The aim is to use expressive data structures available in the scientific Python ecosystem along with best practices in software design to allow scientists to easily and reliably combine model components to represent the climate system at a desired level of complexity and to enable users to fully understand what the model is doing.
sympl is a framework which formulates the model in terms of a state that gets evolved forward in time or modified within a specific time by well-defined components. sympl's design facilitates building models that are self-documenting, are highly interoperable, and provide fine-grained control over model components and behaviour. sympl components contain all relevant information about the input they expect and output that they provide. Components are designed to be easily interchanged, even when they rely on different units or array configurations. sympl provides basic functions and objects which could be used in any type of Earth system model.
climt is an Earth system modelling toolkit that contains scientific components built using sympl base objects. These include both pure Python components and wrapped Fortran libraries. climt provides functionality requiring model-specific assumptions, such as state initialization and grid configuration. climt's programming interface designed to be easy to use and thus appealing to a wide audience.
Model building, configuration and execution are performed through a Python script (or Jupyter Notebook), enabling researchers to build an end-to-end Python-based pipeline along with popular Python data analysis and visualization tools.},
	journal = {Geoscientific Model Development},
	author = {Monteiro, Joy and Mcgibbon, Jeremy and Caballero, Rodrigo},
	month = sep,
	year = {2018},
	pages = {3781--3794},
	file = {Monteiro et al. - 2018 - Sympl (v. 0.4.0) and climt (v. 0.15.3) - Towards a.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/HUR4ZP4T/Monteiro et al. - 2018 - Sympl (v. 0.4.0) and climt (v. 0.15.3) - Towards a.pdf:application/pdf},
}

@misc{ben-nun_productive_2022,
	title = {Productive {Performance} {Engineering} for {Weather} and {Climate} {Modeling} with {Python}},
	shorttitle = {Weather {Modeling} {Python}},
	url = {http://arxiv.org/abs/2205.04148},
	abstract = {Earth system models are developed with a tight coupling to target hardware, often containing specialized code predicated on processor characteristics. This coupling stems from using imperative languages that hard-code computation schedules and layout. We present a detailed account of optimizing the Finite Volume Cubed-Sphere Dynamical Core (FV3), improving productivity and performance. By using a declarative Pythonembedded stencil domain-speciﬁc language and data-centric optimization, we abstract hardware-speciﬁc details and deﬁne a semi-automated workﬂow for analyzing and optimizing weather and climate applications. The workﬂow utilizes both local and full-program optimization, as well as user-guided ﬁne-tuning. To prune the infeasible global optimization space, we automatically utilize repeating code motifs via a novel transfer tuning approach. On the Piz Daint supercomputer, we scale to 2,400 GPUs, achieving speedups of up to 3.92× over the tuned production implementation at a fraction of the original code.},
	language = {en},
	urldate = {2023-05-19},
	publisher = {arXiv},
	author = {Ben-Nun, Tal and Groner, Linus and Deconinck, Florian and Wicky, Tobias and Davis, Eddie and Dahm, Johann and Elbert, Oliver D. and George, Rhea and McGibbon, Jeremy and Trümper, Lukas and Wu, Elynn and Fuhrer, Oliver and Schulthess, Thomas and Hoefler, Torsten},
	month = aug,
	year = {2022},
	note = {arXiv:2205.04148 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Ben-Nun et al. - 2022 - Productive Performance Engineering for Weather and.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/DP5ZMFXY/Ben-Nun et al. - 2022 - Productive Performance Engineering for Weather and.pdf:application/pdf},
}

@article{mcgibbon_fv3gfs-wrapper_2021,
	title = {fv3gfs-wrapper: a {Python} wrapper of the {FV3GFS} atmospheric model},
	volume = {14},
	issn = {1991-9603},
	shorttitle = {fv3gfs-wrapper},
	url = {https://gmd.copernicus.org/articles/14/4401/2021/},
	doi = {10.5194/gmd-14-4401-2021},
	abstract = {Abstract. Simulation software in geophysics is traditionally written in Fortran or C++ due to the stringent performance requirements these codes have to satisfy. As a result, researchers who use high-productivity languages for exploratory work often find these codes hard to understand, hard to modify, and hard to integrate with their analysis tools. fv3gfs-wrapper is an open-source Python-wrapped version of the NOAA (National Oceanic and Atmospheric Administration) FV3GFS (Finite-Volume Cubed-Sphere Global Forecast System) global atmospheric model, which is coded in Fortran. The wrapper provides simple interfaces to progress the Fortran main loop and get or set variables used by the Fortran model. These interfaces enable a wide range of use cases such as modifying the behavior of the model, introducing online analysis code, or saving model variables and reading forcings directly to and from cloud storage. Model performance is identical to the fully compiled Fortran model, unless routines to copy the state in and out of the model are used. This copy overhead is well within an acceptable range of performance and could be avoided with modifications to the Fortran source code. The wrapping approach is outlined and can be applied similarly in other Fortran models to enable more productive scientific workflows.},
	language = {en},
	number = {7},
	urldate = {2023-05-19},
	journal = {Geosci. Model Dev.},
	author = {McGibbon, Jeremy and Brenowitz, Noah D. and Cheeseman, Mark and Clark, Spencer K. and Dahm, Johann P. S. and Davis, Eddie C. and Elbert, Oliver D. and George, Rhea C. and Harris, Lucas M. and Henn, Brian and Kwa, Anna and Perkins, W. Andre and Watt-Meyer, Oliver and Wicky, Tobias F. and Bretherton, Christopher S. and Fuhrer, Oliver},
	month = jul,
	year = {2021},
	pages = {4401--4409},
	file = {McGibbon et al. - 2021 - fv3gfs-wrapper a Python wrapper of the FV3GFS atm.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/2986GD7S/McGibbon et al. - 2021 - fv3gfs-wrapper a Python wrapper of the FV3GFS atm.pdf:application/pdf},
}

@article{dahm_pace_2023,
	title = {Pace v0.2: a {Python}-based performance-portable atmospheric model},
	volume = {16},
	issn = {1991-9603},
	shorttitle = {Pace v0.2},
	url = {https://gmd.copernicus.org/articles/16/2719/2023/},
	doi = {10.5194/gmd-16-2719-2023},
	abstract = {Abstract. Progress in leveraging current and emerging high-performance computing infrastructures using traditional weather and climate models has been slow. This has become known more broadly as the software productivity gap. With the end of Moore's law driving forward rapid specialization of hardware architectures, building simulation codes on a low-level language with hardware-specific optimizations is a significant risk. As a solution, we present Pace, an implementation of the nonhydrostatic FV3 dynamical core and GFDL cloud microphysics scheme which is entirely Python-based. In order to achieve high performance on a diverse set of hardware architectures, Pace is written using the GT4Py domain-specific language. We demonstrate that with this approach we can achieve portability and performance, while significantly improving the readability and maintainability of the code as compared to the Fortran reference implementation. We show that Pace can run at scale on leadership-class supercomputers and achieve performance speeds 3.5–4 times faster than the Fortran code on GPU-accelerated supercomputers. Furthermore, we demonstrate how a Python-based simulation code facilitates existing or enables entirely new use cases and workflows. Pace demonstrates how a high-level language can insulate us from disruptive changes, provide a more productive development environment, and facilitate the integration with new technologies such as machine learning.},
	language = {en},
	number = {9},
	urldate = {2023-05-19},
	journal = {Geosci. Model Dev.},
	author = {Dahm, Johann and Davis, Eddie and Deconinck, Florian and Elbert, Oliver and George, Rhea and McGibbon, Jeremy and Wicky, Tobias and Wu, Elynn and Kung, Christopher and Ben-Nun, Tal and Harris, Lucas and Groner, Linus and Fuhrer, Oliver},
	month = may,
	year = {2023},
	pages = {2719--2736},
	file = {Dahm et al. - 2023 - Pace v0.2 a Python-based performance-portable atm.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/R5UGKR2K/Dahm et al. - 2023 - Pace v0.2 a Python-based performance-portable atm.pdf:application/pdf},
}

@misc{raissi_physics_2017-2,
	title = {Physics {Informed} {Deep} {Learning} ({Part} {I}): {Data}-driven {Solutions} of {Nonlinear} {Partial} {Differential} {Equations}},
	shorttitle = {Physics {Informed} {Deep} {Learning} ({Part} {I})},
	url = {http://arxiv.org/abs/1711.10561},
	abstract = {We introduce physics informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial diﬀerential equations. In this two part treatise, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial diﬀerential equations. Depending on the nature and arrangement of the available data, we devise two distinct classes of algorithms, namely continuous time and discrete time models. The resulting neural networks form a new class of data-eﬃcient universal function approximators that naturally encode any underlying physical laws as prior information. In this ﬁrst part, we demonstrate how these networks can be used to infer solutions to partial diﬀerential equations, and obtain physics-informed surrogate models that are fully diﬀerentiable with respect to all input coordinates and free parameters.},
	language = {en},
	urldate = {2023-05-20},
	publisher = {arXiv},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	month = nov,
	year = {2017},
	note = {arXiv:1711.10561 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Numerical Analysis, Computer Science - Artificial Intelligence, Mathematics - Dynamical Systems},
}

@misc{keisler_forecasting_2022,
	title = {Forecasting {Global} {Weather} with {Graph} {Neural} {Networks}},
	shorttitle = {Forecastin {PINN}},
	url = {http://arxiv.org/abs/2202.07575},
	doi = {10.48550/arXiv.2202.07575},
	abstract = {We present a data-driven approach for forecasting global weather using graph neural networks. The system learns to step forward the current 3D atmospheric state by six hours, and multiple steps are chained together to produce skillful forecasts going out several days into the future. The underlying model is trained on reanalysis data from ERA5 or forecast data from GFS. Test performance on metrics such as Z500 (geopotential height) and T850 (temperature) improves upon previous data-driven approaches and is comparable to operational, full-resolution, physical models from GFS and ECMWF, at least when evaluated on 1-degree scales and when using reanalysis initial conditions. We also show results from connecting this data-driven model to live, operational forecasts from GFS.},
	urldate = {2023-05-25},
	publisher = {arXiv},
	author = {Keisler, Ryan},
	month = feb,
	year = {2022},
	note = {arXiv:2202.07575 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
	file = {arXiv Fulltext PDF:/home/x/Documents/inpe/library-ml-zotero/storage/2Q8QG7B8/Keisler - 2022 - Forecasting Global Weather with Graph Neural Netwo.pdf:application/pdf;arXiv.org Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/C7XFZCJV/2202.html:text/html},
}

@misc{ning_graph-based_2023,
	title = {Graph-{Based} {Deep} {Learning} for {Sea} {Surface} {Temperature} {Forecasts}},
	url = {http://arxiv.org/abs/2305.09468},
	doi = {10.48550/arXiv.2305.09468},
	abstract = {Sea surface temperature (SST) forecasts help with managing the marine ecosystem and the aquaculture impacted by anthropogenic climate change. Numerical dynamical models are resource intensive for SST forecasts; machine learning (ML) models could reduce high computational requirements and have been in the focus of the research community recently. ML models normally require a large amount of data for training. Environmental data are collected on regularly-spaced grids, so early work mainly used grid-based deep learning (DL) for prediction. However, both grid data and the corresponding DL approaches have inherent problems. As geometric DL has emerged, graphs as a more generalized data structure and graph neural networks (GNNs) have been introduced to the spatiotemporal domains. In this work, we preliminarily explored graph re-sampling and GNNs for global SST forecasts, and GNNs show better one month ahead SST prediction than the persistence model in most oceans in terms of root mean square errors.},
	urldate = {2023-05-26},
	publisher = {arXiv},
	author = {Ning, Ding and Vetrova, Varvara and Bryan, Karin R.},
	month = apr,
	year = {2023},
	note = {arXiv:2305.09468 [physics]
version: 1},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/x/Documents/inpe/library-ml-zotero/storage/2WTIARPE/Ning et al. - 2023 - Graph-Based Deep Learning for Sea Surface Temperat.pdf:application/pdf;arXiv.org Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/77XM47Y9/2305.html:text/html},
}

@misc{agrawal_machine_2023,
	title = {A {Machine} {Learning} {Outlook}: {Post}-processing of {Global} {Medium}-range {Forecasts}},
	shorttitle = {A {Machine} {Learning} {Outlook}},
	url = {http://arxiv.org/abs/2303.16301},
	doi = {10.48550/arXiv.2303.16301},
	abstract = {Post-processing typically takes the outputs of a Numerical Weather Prediction (NWP) model and applies linear statistical techniques to produce improve localized forecasts, by including additional observations, or determining systematic errors at a finer scale. In this pilot study, we investigate the benefits and challenges of using non-linear neural network (NN) based methods to post-process multiple weather features -- temperature, moisture, wind, geopotential height, precipitable water -- at 30 vertical levels, globally and at lead times up to 7 days. We show that we can achieve accuracy improvements of up to 12\% (RMSE) in a field such as temperature at 850hPa for a 7 day forecast. However, we recognize the need to strengthen foundational work on objectively measuring a sharp and correct forecast. We discuss the challenges of using standard metrics such as root mean squared error (RMSE) or anomaly correlation coefficient (ACC) as we move from linear statistical models to more complex non-linear machine learning approaches for post-processing global weather forecasts.},
	urldate = {2023-05-26},
	publisher = {arXiv},
	author = {Agrawal, Shreya and Carver, Rob and Gazen, Cenk and Maddy, Eric and Krasnopolsky, Vladimir and Bromberg, Carla and Ontiveros, Zack and Russell, Tyler and Hickey, Jason and Boukabara, Sid},
	month = mar,
	year = {2023},
	note = {arXiv:2303.16301 [physics]
version: 1},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
	file = {arXiv Fulltext PDF:/home/x/Documents/inpe/library-ml-zotero/storage/JJ7D3J2U/Agrawal et al. - 2023 - A Machine Learning Outlook Post-processing of Glo.pdf:application/pdf;arXiv.org Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/EZDDV64Z/2303.html:text/html},
}

@misc{li_towards_2023,
	title = {Towards {Spatio}-temporal {Sea} {Surface} {Temperature} {Forecasting} via {Static} and {Dynamic} {Learnable} {Personalized} {Graph} {Convolution} {Network}},
	url = {http://arxiv.org/abs/2304.09290},
	doi = {10.48550/arXiv.2304.09290},
	abstract = {Sea surface temperature (SST) is uniquely important to the Earth's atmosphere since its dynamics are a major force in shaping local and global climate and profoundly affect our ecosystems. Accurate forecasting of SST brings significant economic and social implications, for example, better preparation for extreme weather such as severe droughts or tropical cyclones months ahead. However, such a task faces unique challenges due to the intrinsic complexity and uncertainty of ocean systems. Recently, deep learning techniques, such as graphical neural networks (GNN), have been applied to address this task. Even though these methods have some success, they frequently have serious drawbacks when it comes to investigating dynamic spatiotemporal dependencies between signals. To solve this problem, this paper proposes a novel static and dynamic learnable personalized graph convolution network (SD-LPGC). Specifically, two graph learning layers are first constructed to respectively model the stable long-term and short-term evolutionary patterns hidden in the multivariate SST signals. Then, a learnable personalized convolution layer is designed to fuse this information. Our experiments on real SST datasets demonstrate the state-of-the-art performances of the proposed approach on the forecasting task.},
	urldate = {2023-05-26},
	publisher = {arXiv},
	author = {Li, Xiaohan and Zhang, Gaowei and Huang, Kai and He, Zhaofeng},
	month = apr,
	year = {2023},
	note = {arXiv:2304.09290 [physics]
version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Physics - Atmospheric and Oceanic Physics},
	file = {arXiv Fulltext PDF:/home/x/Documents/inpe/library-ml-zotero/storage/HR7L7Y2L/Li et al. - 2023 - Towards Spatio-temporal Sea Surface Temperature Fo.pdf:application/pdf;arXiv.org Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/NQXNPMGI/2304.html:text/html},
}

@misc{noauthor_answer_2019,
	title = {Answer to "{What} is the difference between the diffusion equation and the heat equation?"},
	shorttitle = {Answer to "{What} is the difference between the diffusion equation and the heat equation?},
	url = {https://physics.stackexchange.com/a/511882},
	urldate = {2023-05-26},
	journal = {Physics Stack Exchange},
	month = nov,
	year = {2019},
	file = {Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/9JFXRIRR/what-is-the-difference-between-the-diffusion-equation-and-the-heat-equation.html:text/html},
}

@inproceedings{vahid_partovi_nia_gauss-hermite_nodate,
	title = {Gauss-{Hermite} {Quadrature}: {Numerical} or {Statistical} {Method}?},
	abstract = {Gauss-Hermite Quadrature (GHQ) is often used for numerical approximation of integrals with Gaussian kernels. In generalized linear mixed models random eﬀects are assumed to have Gaussian distributions, but often the marginal likelihood, which has the key role in parameter estimation and inference, is analytically intractable. In addition to Monte Carlo methods, ﬁrst or second order Taylor expansion, Laplace approximation or GHQ are feasible tools for numerical evaluation of the integrals. In this paper we review the key ideas of GHQ. Nonparametric Maximum Likelihood (NPML) estimation is shown to be a ﬂexible version of GHQ. A binary nested random eﬀects model is ﬁtted to a real data set using GHQ.},
	language = {en},
	author = {{Vahid Partovi Nia}},
	file = {Nia - Gauss-Hermite Quadrature Numerical or Statistical.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/A52UXTTR/Nia - Gauss-Hermite Quadrature Numerical or Statistical.pdf:application/pdf},
}

@article{gulkac_numerical_2015,
	title = {Numerical {Solutions} of {Two}-{Dimensional} {Burgers}’ {Equations}},
	volume = {6},
	issn = {22295518},
	url = {http://www.ijser.org/onlineResearchPaperViewer.aspx?Numerical-Solutions-of-Two-Dimensional-Burgers-Equations.pdf},
	doi = {10.14299/ijser.2015.04.008},
	abstract = {Two-dimensional Burgers’ equations are reported various kinds of phenomena such as turbulence and viscous fluid. In this paper, we illustrate the LOD method for solving the two-dimensional coupled Burgers’ equations. We extend our earlier work [1] and a stability analysis by Fourier method of the LOD method is also investigated. The computational results obtained by present method are in excellent agreement with earlier results. Present method can be easily implemented for solving nonlinear problems evolving in several branches of engineering and science.},
	language = {en},
	number = {4},
	urldate = {2023-03-31},
	journal = {IJSER},
	author = {Gülkaç, Vildan},
	month = apr,
	year = {2015},
	note = {Number: 4},
	pages = {215--218},
	file = {Gülkaç - 2015 - Numerical Solutions of Two-Dimensional Burgers’ Eq.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/22Q8Y6MR/Gülkaç - 2015 - Numerical Solutions of Two-Dimensional Burgers’ Eq.pdf:application/pdf},
}

@article{fritsch_method_1984,
	title = {A {Method} for {Constructing} {Local} {Monotone} {Piecewise} {Cubic} {Interpolants}},
	volume = {5},
	issn = {0196-5204, 2168-3417},
	url = {http://epubs.siam.org/doi/10.1137/0905021},
	doi = {10.1137/0905021},
	abstract = {A method is described for producing monotone piecewise cubic interpolants to monotone data which is completely local and which is extremely simple to implement.},
	language = {en},
	number = {2},
	urldate = {2023-03-31},
	journal = {SIAM J. Sci. and Stat. Comput.},
	author = {Fritsch, F. N. and Butland, J.},
	month = jun,
	year = {1984},
	note = {Number: 2},
	pages = {300--304},
	file = {Fritsch and Butland - 1984 - A Method for Constructing Local Monotone Piecewise.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/56I7IWAG/Fritsch and Butland - 1984 - A Method for Constructing Local Monotone Piecewise.pdf:application/pdf},
}

@article{bajaj_recipes_2023,
	title = {Recipes for when physics fails: recovering robust learning of physics informed neural networks},
	volume = {4},
	issn = {2632-2153},
	shorttitle = {Recipes for when physics fails},
	url = {https://iopscience.iop.org/article/10.1088/2632-2153/acb416},
	doi = {10.1088/2632-2153/acb416},
	abstract = {Abstract
            Physics-informed neural networks (PINNs) have been shown to be effective in solving partial differential equations by capturing the physics induced constraints as a part of the training loss function. This paper shows that a PINN can be sensitive to errors in training data and overfit itself in dynamically propagating these errors over the domain of the solution of the PDE. It also shows how physical regularizations based on continuity criteria and conservation laws fail to address this issue and rather introduce problems of their own causing the deep network to converge to a physics-obeying local minimum instead of the global minimum. We introduce Gaussian process (GP) based smoothing that recovers the performance of a PINN and promises a robust architecture against noise/errors in measurements. Additionally, we illustrate an inexpensive method of quantifying the evolution of uncertainty based on the variance estimation of GPs on boundary data. Robust PINN performance is also shown to be achievable by choice of sparse sets of inducing points based on sparsely induced GPs. We demonstrate the performance of our proposed methods and compare the results from existing benchmark models in literature for time-dependent Schrödinger and Burgers’ equations.},
	language = {en},
	number = {1},
	urldate = {2023-03-31},
	journal = {Mach. Learn.: Sci. Technol.},
	author = {Bajaj, Chandrajit and McLennan, Luke and Andeen, Timothy and Roy, Avik},
	month = mar,
	year = {2023},
	note = {Number: 1},
	pages = {015013},
}

@book{kiusalaas_numerical_2013,
	title = {Numerical {Methods} in {Engineering} with {Python} 3},
	isbn = {978-1-107-03385-6},
	abstract = {This book is an introduction to numerical methods for students in engineering. It covers the usual topics found in an engineering course: solution of equations, interpolation and data fitting, solution of differential equations, eigenvalue problems, and optimization. The algorithms are implemented in Python 3, a high-level programming language that rivals MATLAB® in readability and ease of use. All methods include programs showing how the computer code is utilized in the solution of problems. The book is based on Numerical Methods in Engineering with Python, which used Python 2. This new text demonstrates the use of Python 3 and includes an introduction to the Python plotting package Matplotlib. This comprehensive book is enhanced by the addition of numerous examples and problems throughout.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Kiusalaas, Jaan},
	month = jan,
	year = {2013},
	note = {Google-Books-ID: aJkXoxxoCoUC},
	keywords = {Computers / Programming / General, Computers / Software Development \& Engineering / General, Mathematics / Applied, Mathematics / Numerical Analysis, Technology \& Engineering / Engineering (General), Technology \& Engineering / Environmental / General},
}

@misc{ongko_implementing_2022,
	title = {Implementing {Various} {Root}-{Finding} {Algorithms} in {Python}},
	url = {https://towardsdatascience.com/implementing-various-root-finding-algorithms-in-python-67917ef090b3},
	abstract = {With an actual application in data science and logistic regression},
	language = {en},
	urldate = {2023-04-02},
	journal = {Medium},
	author = {Ongko, Gerry Christian},
	month = apr,
	year = {2022},
	file = {Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/HJGP3B5V/implementing-various-root-finding-algorithms-in-python-67917ef090b3.html:text/html},
}

@misc{certik_sympy_2023,
	title = {{SymPy} is a {Python} library for symbolic mathematics. {It} aims to become a full-featured computer algebra system ({CAS}) while keeping the code as simple as possible in order to be comprehensible and easily extensible.},
	url = {https://www.sympy.org/en/index.html},
	urldate = {2023-04-02},
	author = {Čertík, Ondřej and Meurer, Aaron},
	month = apr,
	year = {2023},
	note = {Programmers: \_:n41},
	file = {SymPy:/home/x/Documents/inpe/library-ml-zotero/storage/IHN5UGEW/index.html:text/html},
}

@article{rosa_gradient_2000,
	title = {Gradient pattern analysis of {Swift}–{Hohenberg} dynamics: phase disorder characterization},
	volume = {283},
	issn = {0378-4371},
	shorttitle = {Gradient pattern analysis of {Swift}–{Hohenberg} dynamics},
	url = {https://www.sciencedirect.com/science/article/pii/S0378437100001448},
	doi = {10.1016/S0378-4371(00)00144-8},
	abstract = {In this paper, we analyze the onset of phase-dominant dynamics in a uniformly forced system. The study is based on the numerical integration of the Swift–Hohenberg equation and adresses the characterization of phase disorder detected from gradient computational operators as complex entropic form (CEF). The transition from amplitude to phase dynamics is well characterized by means of the variance of the CEF phase component.},
	language = {en},
	number = {1},
	urldate = {2023-04-04},
	journal = {Physica A: Statistical Mechanics and its Applications},
	author = {Rosa, R. R. and Pontes, J. and Christov, C. I. and Ramos, F. M. and Rodrigues\_Neto, C. and Rempel, E. L. and Walgraef, D.},
	month = aug,
	year = {2000},
	note = {Number: 1},
	keywords = {Complex Entropic form, Extended systems, Gradient dynamics, Pattern formation, Phase disorder},
	pages = {156--159},
	file = {ScienceDirect Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/7UHTNN8P/S0378437100001448.html:text/html},
}

@article{rosa_characterization_1999,
	title = {Characterization of asymmetric fragmentation patterns in spatially extended systems},
	volume = {10},
	issn = {0129-1831},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0129183199000103},
	doi = {10.1142/S0129183199000103},
	abstract = {Spatially extended systems yield complex patterns arising from the coupled dynamics of its different regions. In this paper we introduce a matrix computational operator, , for the characterization of asymmetric amplitude fragmentation in extended systems. For a given matrix of amplitudes this operation results in an asymmetric-triangulation field composed by L points and I straight lines. The parameter (I-L)/L is a new quantitative measure of the local complexity defined in terms of the asymmetry in the gradient field of the amplitudes. This asymmetric fragmentation parameter is a measure of the degree of structural complexity and characterizes the localized regions of a spatially extended system and symmetry breaking along the evolution of the system. For the case of a random field, in the real domain, which has total asymmetry, this asymmetric fragmentation parameter is expected to have the highest value and this is used to normalize the values for the other cases. Here, we present a detailed description of the operator  and some of the fundamental conjectures that arises from its application in spatio-temporal asymmetric patterns.},
	number = {01},
	urldate = {2023-04-04},
	journal = {Int. J. Mod. Phys. C},
	author = {Rosa, R. R. and Sharma, A. S. and Valdivia, J. A.},
	month = feb,
	year = {1999},
	note = {Number: 01
Publisher: World Scientific Publishing Co.},
	keywords = {Extended systems, Amplitude fragmentation, Pattern dynamics, Spatio-Temporal complexity, Symmetry breaking},
	pages = {147--163},
}

@misc{rubens_gradient_2022,
	title = {Gradient {Pattern} {Analysis}},
	url = {https://github.com/rsautter/GPA},
	abstract = {Gradient Pattern Analysis in Cython},
	urldate = {2023-04-04},
	author = {{Rubens}},
	month = dec,
	year = {2022},
	note = {Programmers: \_:n71
original-date: 2016-09-13T21:19:43Z},
}

@article{rosa_gradient_2003,
	title = {Gradient pattern analysis of structural dynamics: application to molecular system relaxation},
	volume = {33},
	issn = {0103-9733},
	shorttitle = {Gradient pattern analysis of structural dynamics},
	url = {http://www.scielo.br/scielo.php?script=sci_arttext&pid=S0103-97332003000300023},
	doi = {10.1590/S0103-97332003000300023},
	language = {en},
	number = {3},
	urldate = {2023-04-04},
	journal = {Braz. J. Phys.},
	author = {Rosa, R. R. and Campos, M. R. and Ramos, F. M. and Vijaykumar, N. L. and Fujiwara, S. and Sato, T.},
	month = sep,
	year = {2003},
	note = {Number: 3},
	pages = {605--610},
}

@article{rosa_gradient_2008,
	title = {Gradient pattern analysis of short solar radio bursts},
	volume = {42},
	issn = {02731177},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0273117707009027},
	doi = {10.1016/j.asr.2007.08.015},
	abstract = {We analyze the weak component of the localized temporal pattern variability of 3 GHz solar burst observed by the Ondrejov radiospectrograph. A complex, short and weak impulsive sample from the time series was analyzed by applying a method based on the gradient pattern analysis and discrete wavelet decomposition. By analyzing canonical temporal variability patterns we show that the new method can reliably characterize the phenomenological dynamical process of short time series (N 6 103 measurements) as the radio burst addressed here. In the narrowest sense, by estimating the mutual information distance in the gradient spectra, we show that the ﬂuctuation pattern of the short and weak 3 GHz impulsive solar burst, with energetic amplitudes {\textless}350 SFU, is closer to the intermittent and strong MHD turbulent variability pattern.},
	language = {en},
	number = {5},
	urldate = {2023-04-04},
	journal = {Advances in Space Research},
	author = {Rosa, R. R. and Karlický, M. and Veronese, T. B. and Vijaykumar, N. L. and Sawant, H. S. and Borgazzi, A. I. and Dantas, M. S. and Barbosa, E. B. M. and Sych, R. A. and Mendes, O.},
	month = sep,
	year = {2008},
	note = {Number: 5},
	pages = {844--851},
}

@book{ruggiero_calculo_1996,
	title = {Cálculo numérico: aspectos teóricos e computacionais},
	volume = {1},
	isbn = {978-85-346-0204-4},
	url = {https://worldcat.org/en/title/1042351138},
	publisher = {Pearson Makron Books},
	author = {Ruggiero, Marcia A. Gomes and Lopes, Vera Lucia Da Rocha},
	month = mar,
	year = {1996},
}

@book{hoffman_numerical_2018,
	title = {Numerical {Methods} for {Engineers} and {Scientists}},
	volume = {1},
	isbn = {978-1-4822-7060-0},
	publisher = {CRC Press},
	author = {Hoffman, Joe D. and Frankel, Steven},
	year = {2018},
}

@book{newman_computational_2012,
	title = {Computational {Physics}},
	volume = {1},
	isbn = {978-1-4801-4551-1},
	publisher = {CreateSpace Independent Publishing Platform},
	author = {Newman, Mark E. J.},
	month = nov,
	year = {2012},
}

@book{conte_elementary_1980,
	title = {Elementary {Numerical} {Analysis}: {An} {Algorithmic} {Approach}},
	volume = {1},
	isbn = {978-0-07-012447-9},
	publisher = {McGraw-Hill},
	author = {Conte, Samuel D. and Boor, Carl de},
	month = mar,
	year = {1980},
}

@book{nagar_introduction_2017,
	title = {Introduction to {Python} for {Engineers} and {Scientists}: {Open} {Source} {Solutions} for {Numerical} {Computation}},
	volume = {1},
	isbn = {978-1-4842-3203-3},
	publisher = {Apress},
	author = {Nagar, Sandeep},
	year = {2017},
}

@book{scherer_metodos_2005,
	title = {Métodos {Computacionais} da {Física}},
	volume = {1},
	isbn = {978-85-88325-35-7},
	publisher = {Livraria da Física},
	author = {Scherer, Cláudio},
	month = jan,
	year = {2005},
}

@book{press_numerical_2002,
	title = {Numerical {Recipes} in {C}++: {The} {Art} of {Scientific} {Computing}},
	volume = {1},
	isbn = {978-0-521-75033-2},
	publisher = {Cambridge University Press},
	author = {Press, William H. and Teukolsky, Saul A. and Vetterling, William T. and Flannery, Brian P.},
	year = {2002},
}

@book{kiusalaas_numerical_2010,
	title = {Numerical {Methods} in {Engineering} {With} {Python}},
	volume = {1},
	isbn = {978-0-521-19132-6},
	publisher = {Cambridge University Press},
	author = {Kiusalaas, Jaan},
	month = jan,
	year = {2010},
}

@book{proakis_digital_2013,
	title = {Digital {Signal} {Processing}},
	volume = {1},
	isbn = {978-1-292-02573-5},
	publisher = {Pearson},
	author = {Proakis, John G. and Manolakis, Dimitris G.},
	month = jul,
	year = {2013},
}

@book{conte_elementary_2017,
	title = {Elementary {Numerical} {Analysis}: {An} {Algorithmic} {Approach}},
	volume = {1},
	isbn = {978-1-61197-519-2},
	publisher = {SIAM},
	author = {Conte, Samuel D. and Boor, Carl de},
	year = {2017},
}

@phdthesis{roberts_non-oscillatory_2009,
	title = {Non-oscillatory interpolation for the {Semi}-{Lagrangian} scheme},
	url = {https://www.reading.ac.uk/maths-and-stats/publications/theses-and-dissertations/mathematics-msc-dissertations},
	author = {Roberts, Tomos Wyn},
	year = {2009},
	note = {Volume: 1},
}

@book{lucquin_introduction_1998,
	title = {Introduction to {Scientific} {Computing}},
	volume = {1},
	isbn = {978-0-471-97266-2},
	publisher = {Wiley},
	author = {Lucquin, Brigitte and Pironneau, Olivier},
	month = jun,
	year = {1998},
}

@book{press_numerical_2007,
	title = {Numerical {Recipes}: {The} {Art} of {Scientific} {Computing} [3 ed.]},
	volume = {1},
	isbn = {978-0-521-88068-8},
	publisher = {Cambridge University Press},
	author = {Press, William H. and Teukolsky, Saul A. and Vetterling, William T. and Flannery, Brian P.},
	year = {2007},
	keywords = {Mathematics / Applied, Mathematics / Numerical Analysis, Mathematics / General, Computers / Mathematical \& Statistical Software},
	file = {numerical.recipes/:/home/x/Documents/inpe/library-ml-zotero/storage/LHPBJQSH/numerical.recipes.html:text/html},
}

@book{moler_numerical_2010,
	title = {Numerical {Computing} {With} {MATLAB}},
	volume = {1},
	isbn = {978-0-89871-660-3},
	publisher = {SIAM},
	author = {Moler, Cleve B.},
	year = {2010},
}

@book{boyer_history_2011,
	title = {A {History} of {Mathematics}},
	volume = {1},
	isbn = {978-0-470-52548-7},
	publisher = {John Wiley \& Sons},
	author = {Boyer, Carl B. and Merzbach, Uta C.},
	month = jan,
	year = {2011},
}

@book{dugas_history_2012,
	title = {A {History} of {Mechanics}},
	volume = {1},
	isbn = {978-0-486-17337-5},
	publisher = {Dover Publications},
	author = {Dugas, René},
	year = {2012},
}

@misc{trefethen_who_2019,
	title = {Who invented the great numerical algorithms?},
	author = {Trefethen, Nick},
	year = {2019},
	note = {Volume: 1},
}

@article{hassan_fourier_2013,
	title = {Fourier {Spectral} {Methods} for {Solving} {Some} {Nonlinear} {Partial} {Differential} {Equations}},
	volume = {6},
	doi = {10.12816/0006177},
	abstract = {The spectral collocation or pseudospectral (PS) methods (Fourier transform methods) combined with temporal discretization techniques to numerically compute solutions of some partial differential equations (PDEs). In this paper, we solve the Korteweg-de Vries (KdV) equation using a Fourier spectral collocation method to discretize the space variable, leap frog and classical fourth-order Runge-Kutta scheme (RK4) for time dependence. Also, Boussinesq equation is solving by a Fourier spectral collocation method to discretize the space variable, finite difference and classical fourth-order Runge- Kutta scheme (RK4) for time dependence. Our implementation employs the Fast Fourier Transform (FFT) algorithm.},
	journal = {Int. J. Open Problems Compt. Math.},
	author = {Hassan, Hany and Saleh, Hassan},
	month = jun,
	year = {2013},
	pages = {144--179},
}

@article{chian_high-dimensional_2002,
	title = {High-dimensional interior crisis in the {Kuramoto}-{Sivashinsky} equation},
	volume = {65},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.65.035203},
	doi = {10.1103/PhysRevE.65.035203},
	abstract = {An investigation of interior crisis of high dimensions in an extended spatiotemporal system exemplified by the Kuramoto-Sivashinsky equation is reported. It is shown that unstable periodic orbits and their associated invariant manifolds in the Poincaré hyperplane can effectively characterize the global bifurcation dynamics of high-dimensional systems.},
	number = {3},
	urldate = {2023-04-06},
	journal = {Phys. Rev. E},
	author = {Chian, A. C.-L. and Rempel, E. L. and Macau, E. E. and Rosa, R. R. and Christiansen, F.},
	month = feb,
	year = {2002},
	note = {Number: 3
Publisher: American Physical Society},
	pages = {035203},
	file = {APS Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/38VVHRSE/PhysRevE.65.html:text/html},
}

@article{de_carvalho_investigating_2017,
	title = {Investigating the {Relation} between {Galaxy} {Properties} and the {Gaussianity} of the {Velocity} {Distribution} of {Groups} and {Clusters}},
	volume = {154},
	issn = {0004-6256},
	url = {https://ui.adsabs.harvard.edu/abs/2017AJ....154...96D},
	doi = {10.3847/1538-3881/aa7f2b},
	abstract = {We investigate the dependence of stellar population properties of galaxies on group dynamical stage for a subsample of the Yang catalog. We classify groups according to their galaxy velocity distribution into Gaussian (G) and Non-Gaussian (NG). Using two totally independent approaches, we have shown that our measurement of Gaussianity is robust and reliable. Our sample covers Yang’s groups in the redshift range 0.03 ≤slant z ≤slant 0.1, with mass ≥slant \{10\}14\{M\}⊙ . The new method, called Hellinger Distance, to determine whether a group has a velocity distribution Gaussian or NG is very effective in distinguishing between the two families. NG groups present halo masses higher than the G ones, confirming previous findings. Examining the skewness and kurtosis of the velocity distribution of G and NG groups, we find that faint galaxies in NG groups are mainly infalling, for the first time, into the groups. We show that considering only faint galaxies in the outskirts; those in NG groups are older and more metal-rich than those in G groups. Also, examining the Projected Phase Space of cluster galaxies, we see that bright and faint galactic systems in G groups are in dynamical equilibrium—which does not seem to be the case in NG groups. These findings suggest that NG systems have a higher infall rate, assembling more galaxies that have experienced preprocessing before entering the group.},
	urldate = {2023-04-06},
	journal = {The Astronomical Journal},
	author = {de Carvalho, R. R. and Ribeiro, A. L. B. and Stalder, D. H. and Rosa, R. R. and Costa, A. P. and Moura, T. C.},
	month = sep,
	year = {2017},
	note = {ADS Bibcode: 2017AJ....154...96D},
	keywords = {Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics, evolution, galaxies: clusters: general, galaxies: formation, galaxies: groups: general},
	pages = {96},
}

@book{kong_python_2021,
	address = {London},
	title = {Python programming and numerical methods: a guide for engineers and scientists},
	isbn = {978-0-12-819549-9},
	shorttitle = {Python programming and numerical methods},
	language = {eng},
	publisher = {Elsevier, Academic Press},
	author = {Kong, Qingkai and Siauw, Timmy and Bayen, Alexandre M.},
	year = {2021},
}

@misc{flor_metodo_2023,
	title = {Método dos {Mínimos} {Quadrados}},
	url = {http://www2.fis.ufba.br/dfg/fis2/Minimos_quadrados_MOD.pdf},
	urldate = {2023-04-10},
	publisher = {Instituto de Física {\textbar} Universidade Federal da Bahia},
	author = {{Flor}},
	month = apr,
	year = {2023},
	file = {Instituto de Física | Universidade Federal da Bahia:/home/x/Documents/inpe/library-ml-zotero/storage/SKLVH3ZA/www.fis.ufba.br.html:text/html},
}

@misc{noauthor_ordinary_2023,
	title = {Ordinary least squares},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Ordinary_least_squares&oldid=1147930427},
	abstract = {In statistics, ordinary least squares (OLS) is a type of linear least squares method for choosing the unknown parameters in a linear regression model (with fixed level-one effects of a linear function of a set of explanatory variables) by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the input dataset and the output of the (linear) function of the independent variable.
Geometrically, this is seen as the sum of the squared distances, parallel to the axis of the dependent variable, between each data point in the set and the corresponding point on the regression surface—the smaller the differences, the better the model fits the data. The resulting estimator can be expressed by a simple formula, especially in the case of a simple linear regression, in which there is a single regressor on the right side of the regression equation.
The OLS estimator is consistent for the level-one fixed effects when the regressors are exogenous and forms perfect colinearity (rank condition), consistent for the variance estimate of the residuals when regressors have finite fourth moments  and—by the Gauss–Markov theorem—optimal in the class of linear unbiased estimators when the errors are homoscedastic and serially uncorrelated. Under these conditions, the method of OLS provides minimum-variance mean-unbiased estimation when the errors have finite variances. Under the additional assumption that the errors are normally distributed with zero mean, OLS is the maximum likelihood estimator that outperforms any non-linear unbiased estimator.},
	language = {en},
	urldate = {2023-04-10},
	journal = {Wikipedia},
	month = apr,
	year = {2023},
	note = {Page Version ID: 1147930427},
	file = {Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/49JD6AUR/Ordinary_least_squares.html:text/html},
}

@misc{noauthor_multiple_2023,
	title = {Multiple {Linear} {Regression} ({MLR}) {Definition}, {Formula}, and {Example}},
	url = {https://www.investopedia.com/terms/m/mlr.asp},
	abstract = {Multiple linear regression (MLR) is a statistical technique that uses several explanatory variables to predict the outcome of a response variable.},
	language = {en},
	urldate = {2023-04-10},
	journal = {Investopedia},
	month = apr,
	year = {2023},
	file = {Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/3AM7NLE8/mlr.html:text/html},
}

@article{schmidheiny_multiple_2022,
	title = {The {Multiple} {Linear} {Regression} {Model}},
	url = {https://www.schmidheiny.name/teaching/ols.pdf},
	language = {en},
	author = {Schmidheiny, Kurt},
	year = {2022},
	file = {Schmidheiny - 2022 - The Multiple Linear Regression Model.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/6FGVLD8V/Schmidheiny - 2022 - The Multiple Linear Regression Model.pdf:application/pdf},
}

@misc{simoes_conversoes_2011,
	title = {Conversões entre {Sistemas}. {Códigos}. {Circuitos} {Digitais} {I}.},
	publisher = {UNESP},
	author = {Simões, Alexandre da S.},
	year = {2011},
}

@incollection{silva_topicos_2008,
	title = {Tópicos de inteligência computacional aplicados em tecnologias espaciais},
	isbn = {978-85-17-00037-9},
	shorttitle = {Book},
	url = {http://mtc-m16c.sid.inpe.br/rep-/sid.inpe.br/mtc-m18@80/2008/12.19.13.18},
	booktitle = {Computação e {Matemática} {Aplicada} às {Ciências} e {Tecnologias} {Espaciais}},
	author = {Silva, José Demísio Simões da},
	year = {2008},
}

@book{franco_calculo_2002,
	title = {Cálculo {Numérico}},
	url = {http://www.decom.ufop.br/bcc760/material_de_apoio/livros/livro_port.pdf},
	author = {Franco, Neide Maria Bertoldi},
	year = {2002},
}

@book{freitas_metodos_2000,
	title = {Métodos {Numéricos}},
	url = {http://www.decom.ufop.br/bcc760/material_de_apoio/livros/livro_port_1.pdf},
	author = {Freitas, Sergio Roberto de},
	year = {2000},
}

@book{gavala_calculo_2002,
	title = {Cálculo {Numérico}},
	url = {http://www.decom.ufop.br/bcc760/material_de_apoio/livros/ap_cal_num_esp.PDF},
	author = {Gavala, Javier Cobos},
	year = {2002},
}

@unpublished{carlos_listas_2023,
	title = {Listas de exercícios da matéria de matemática computacional ({CAP}-239-4), do curso de pós-graduação em {Computação} {Aplicada} do {Instituto} {Nacional} de {Pesquisas} {Espaciais} (in {Portuguese})},
	url = {https://fmenino-cap-239.netlify.app/},
	author = {Carlos, Felipe Menino},
	year = {2023},
}

@unpublished{nietto_sistemas_2019,
	title = {Sistemas de equações: algumas aplicações},
	url = {https://www.ime.unicamp.br/~apmat/},
	author = {Nietto, Stephanie and Martins, Ricardo Miranda},
	year = {2019},
}

@misc{santos_interpolacao_2021,
	title = {Interpolação - provocações},
	url = {https://www.youtube.com/@santoslblx},
	author = {Santos, Leonardo},
	year = {2021},
	note = {Presenters: \_:n233},
}

@article{fritsch_monotone_1980,
	title = {Monotone {Piecewise} {Cubic} {Interpolation}},
	volume = {17},
	url = {http://www.jstor.org/stable/2156610},
	abstract = {Necessary and sufficient conditions are derived for a cubic to be monotone on an interval. These conditions are used to develop an algorithm which constructs a visually pleasing monotone piecewise cubic interpolant to monotone data. Several examples are given which compare this algorithm with other interpolation methods.},
	language = {en},
	number = {2},
	journal = {SIAM Journal on Numerical Analysis},
	author = {Fritsch, F. N. and Carlson, R. E.},
	year = {1980},
	note = {Number: 2},
	pages = {238--246},
}

@article{dong_predicting_2011,
	title = {Predicting {Housekeeping} {Genes} {Based} on {Fourier} {Analysis}},
	volume = {6},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0021012},
	doi = {10.1371/journal.pone.0021012},
	abstract = {Housekeeping genes (HKGs) generally have fundamental functions in basic biochemical processes in organisms, and usually have relatively steady expression levels across various tissues. They play an important role in the normalization of microarray technology. Using Fourier analysis we transformed gene expression time-series from a Hela cell cycle gene expression dataset into Fourier spectra, and designed an effective computational method for discriminating between HKGs and nonHKGs using the support vector machine (SVM) supervised learning algorithm which can extract significant features of the spectra, providing a basis for identifying specific gene expression patterns. Using our method we identified 510 human HKGs, and then validated them by comparison with two independent sets of tissue expression profiles. Results showed that our predicted HKG set is more reliable than three previously identified sets of HKGs.},
	language = {en},
	number = {6},
	urldate = {2023-05-17},
	journal = {PLoS ONE},
	author = {Dong, Bo and Zhang, Peng and Chen, Xiaowei and Liu, Li and Wang, Yunfei and He, Shunmin and Chen, Runsheng},
	editor = {Chi, Jen-Tsan Ashley},
	month = jun,
	year = {2011},
	note = {Number: 6},
	pages = {e21012},
}

@unpublished{cox_computation_2014,
	title = {Computation {Modelling}},
	url = {https://www.southampton.ac.uk/~sjc/teaching/notes/comp_modelling_cox_latest.pdf},
	abstract = {Notes for FEEG1001 (Design and Computing)},
	author = {Cox, Simon J.},
	year = {2014},
}

@misc{burkardt_burgers_2015,
	title = {Burgers {Solution}},
	url = {https://people.sc.fsu.edu/~jburkardt/py_src/burgers_solution/burgers_solution.py},
	urldate = {2023-05-17},
	author = {Burkardt, John},
	year = {2015},
	note = {Programmers: \_:n272},
	file = {John Burkardt's Home Page:/home/x/Documents/inpe/library-ml-zotero/storage/AE9LCWAS/~jburkardt.html:text/html},
}

@inproceedings{mathias_augmenting_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Augmenting a {Physics}-{Informed} {Neural} {Network} for the {2D} {Burgers} {Equation} by {Addition} of {Solution} {Data} {Points}},
	isbn = {978-3-031-21689-3},
	doi = {10.1007/978-3-031-21689-3_28},
	abstract = {We implement a Physics-Informed Neural Network (PINN) for solving the two-dimensional Burgers equations. This type of model can be trained with no previous knowledge of the solution; instead, it relies on evaluating the governing equations of the system in points of the physical domain. It is also possible to use points with a known solution during training. In this paper, we compare PINNs trained with different amounts of governing equation evaluation points and known solution points Comparing models that were trained purely with known solution points to those that have also used the governing equations, we observe an improvement in the overall observance of the underlying physics in the latter. We also investigate how changing the number of each type of point affects the resulting models differently. Finally, we argue that the addition of the governing equations during training may provide a way to improve the overall performance of the model without relying on additional data, which is especially important for situations where the number of known solution points is limited.},
	language = {en},
	booktitle = {Intelligent {Systems}},
	publisher = {Springer International Publishing},
	author = {Mathias, Marlon S. and de Almeida, Wesley P. and Coelho, Jefferson F. and de Freitas, Lucas P. and Moreno, Felipe M. and Netto, Caio F. D. and Cozman, Fabio G. and Reali Costa, Anna Helena and Tannuri, Eduardo A. and Gomi, Edson S. and Dottori, Marcelo},
	editor = {Xavier-Junior, João Carlos and Rios, Ricardo Araújo},
	year = {2022},
	keywords = {Burgers equation, Physics-informed neural networks},
	pages = {388--401},
}

@mastersthesis{miranda_common_2022,
	address = {São José dos Campos},
	title = {Common {MPI}-based {HPC} {Approaches} in {Python} {Evaluated} for {Selected} {Test} {Cases}},
	url = {http://urlib.net/ibi/QABCDSTQQW/46C4U9H},
	abstract = {A number of the most common MPI-based high-performance computing approaches available in the Python programming environment of the LNCC Santos Dumont supercomputer are compared using three selected test cases. Python includes specific libraries, development tools, implementations, documentation and optimization or parallelization resources. It provides a straightforward way to allow programs to be written with a high level of abstraction, but the parallelization features to exploit multiple cores, processors or accelerators such as GPUs are diverse and may not be easily selectable by the programmer. This work compares common approaches in Python to increase computing performance for three test cases: a 2D heat transfer problem solved by the finite difference method, a 3D fast Fourier transform applied to synthetic data, and asteroid classification using a random forest. The corresponding serial and parallel implementations in Fortran 90 were taken as references to compare the computational performance. In addition to the performance results, a discussion of the trade-off between easiness of programming and computational performance is included. This work is intended as a primer for using parallel HPC resources in Python. RESUMO: Algumas das abordagens de computação de alto desempenho mais comuns baseadas em MPI disponíveis no ambiente de programação Python do supercomputador LNCC Santos Dumont são comparadas usando três casos de teste selecionados. Python inclui bibliotecas específicas, ferramentas de desenvolvimento, implementações, documentação e recursos de otimização ou paralelização. Ele fornece uma maneira direta de permitir que programas sejam escritos com um alto nível de abstração, mas os recursos de paralelização para explorar vários núcleos, processadores ou aceleradores, como GPUs, são diversos e podem não ser facilmente selecionáveis pelo programador. Este trabalho compara abordagens comuns em Python para se obter processamento de alto desempenho desempenho utilizando três casos de teste: um problema de transmissão de calor bidimensional resolvido por diferenças finitas, uma transformada rápida de Fourier tridimensional aplicada a dados sintéticos e uma classificação de asteróides por floresta aleatória. As correspondentes implementações seriais e paralelas em Fortran 90 foram tomadas como referência para comparação de desempenho nesses casos de teste. Além dos resultados de desempenho, inclui-se uma discussão sobre o compromisso entre facilidade de programação e desempenho de processamento. Este trabalho pretende ser uma introdução para o uso de recursos de processamento de alto desempenho baseados em MPI para Python.},
	language = {en},
	school = {National Institute for Space Research (INPE)},
	author = {Miranda, Eduardo Furlan},
	month = feb,
	year = {2022},
	keywords = {ambiente de programação Python, computação paralela., high performance computing, parallel computing, processamento de alto desempenho, Python programming environment},
	file = {Defesa de Mestrado de Eduardo Miranda\: Common MPI-based HPC Approaches in Python - YouTube:/home/x/Documents/inpe/library-ml-zotero/storage/DAUNA9PN/watch.html:text/html;Miranda - 2022 - Common MPI-based HPC Approaches in Python Evaluate.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/SXTSWVCL/Miranda - 2022 - Common MPI-based HPC Approaches in Python Evaluate.pdf:application/pdf},
}

@inproceedings{miranda_comparison_2021,
	title = {Comparison of {High}-performance {Computing} {Approaches} in the {Python} {Environment} for a {Five}-point {Stencil} {Test} {Problem}},
	url = {https://sol.sbc.org.br/index.php/bresci/article/view/15786},
	doi = {10.5753/bresci.2021.15786},
	booktitle = {{XV} {Brazilian} e-{Science} {Workshop}, at {XLI} {Congress} of the {Brazilian} {Computer} {Society} ({CSBC}-2021)},
	publisher = {SBC},
	author = {Miranda, Eduardo F and Stephany, Stephan},
	year = {2021},
	pages = {33--40},
	file = {Miranda and Stephany - 2021 - Comparison of High-performance Computing Approache.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/2XWGICBS/Miranda and Stephany - 2021 - Comparison of High-performance Computing Approache.pdf:application/pdf},
}

@article{souza_alise_2018,
	title = {Análise de {Desempenho} de {Comunicação} {Usando} a {Funcionalidade} de {Memória} {Compartilhada} do {MPI} 3.0},
	volume = {10},
	issn = {2175-7275},
	url = {http://ojs.unirg.edu.br/index.php/1/article/view/2276},
	abstract = {Resumo
					Na execução de um programa paralelizado com a biblioteca de comunicação por troca de mensagens MPI num nó computacional de memória compartilhada, a troca de mensagens entre processos pode ocasionar uma contenção pelo acesso à memória, prejudicando a escalabilidade do programa paralelo. A versão 3.0 do MPI implementou uma nova funcionalidade, a comunicação unilateral Shared Memory (SHM) que utiliza uma janela de memória comum aos processos executados no mesmo nó computacional na qual esses processos podem efetuar leituras e escritas diretamente, sem uso de funções MPI e sem armazenamento intermediário. Este trabalho avalia o desempenhocomputacional dessa nova funcionalidade do MPI na execução de um código de diferenças finitas em C e em Fortran 90 utilizando uma máquina paralela Cray. A comunicação unilateral SHM é comparada à comunicação bilateral convencional MPI.},
	language = {pt},
	number = {2},
	urldate = {2023-05-01},
	journal = {REVISTA CEREUS},
	author = {Souza, Carlos Renato and Panetta, Jairo and Stephany, Stephan},
	month = aug,
	year = {2018},
	note = {Number: 2},
}

@article{miranda_common_2021,
	title = {Common {HPC} {Approaches} in {Python} {Evaluated} for a {Scientific} {Computing} {Test} {Case}},
	volume = {13},
	copyright = {Copyright (c) 2021 REVISTA CEREUS},
	issn = {2175-7275},
	url = {http://www.ojs.unirg.edu.br/index.php/1/article/view/3408},
	abstract = {Resumo
					Várias das abordagens de processamento de alto desempenho mais comuns disponíveis no ambiente de programação Python do supercomputador LNCC Santos Dumont são comparadas usando um caso de teste específico. Python inclui bibliotecas específicas, ferramentas de desenvolvimento, implementações, recursos de documentação e otimização/paralelização, e fornece uma maneira direta de programar em um alto nível de abstração, porém os recursos de paralelização para explorar vários núcleos, processadores ou aceleradores como GPUs, são diversos e podem não ser facilmente selecionáveis ​​pelo programador. Este trabalho faz uma comparação de abordagens comuns em Python para aumentar o desempenho computacional. O caso de teste é um conhecido problema de transmissão de calor 2D modelado pela equação diferencial parcial de Poisson, que é resolvido por um método de diferenças finitas que requer o cálculo de um estêncil de 5 pontos na grade de domínio. As implementações seriais e paralelas em Fortran 90 foram tomadas como referência para comparar o desempenho com algumas implementações Python seriais e paralelas do mesmo algoritmo. Além dos resultados de desempenho, uma discussão sobre facilidade de programação e desempenho de processamento está incluída. Este trabalho pode ser usado como um ponto de partida para a utilização de recursos PAD em Python.},
	language = {pt},
	number = {2},
	urldate = {2023-05-20},
	journal = {REVISTA CEREUS},
	author = {Miranda, Eduardo Furlan and Stephany, Stephan},
	month = jul,
	year = {2021},
	note = {Number: 2},
	pages = {84--98},
}

@misc{baker_planet_1990,
	title = {Planet {Earth}: {The} {View} from {Space}},
	shorttitle = {Planet {Earth}},
	publisher = {Harvard University Press},
	author = {Baker, D. James},
	year = {1990},
	file = {Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/3BHJIF4L/html.html:text/html},
}

@misc{kiefer_remote_1994,
	title = {Remote {Sensing} and {Image} {Interpretation} by {Lillesand}},
	url = {https://www.biblio.com/book/remote-sensing-image-interpretation-lillesand-thomas/d/144003002},
	abstract = {Wiley.  Used - Good.  9780399151859. . Your purchase supports More Than Words, a nonprofit job training program for youth, empowering youth to take charge of their lives by…},
	language = {en},
	urldate = {2023-05-20},
	journal = {Biblio.com},
	author = {Kiefer, Ralph W},
	year = {1994},
	file = {Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/PN7YJWYS/144003002.html:text/html},
}

@book{noauthor_remote_1994,
	address = {Usa},
	title = {Remote {Sensing} and {Image} {Interpretation}},
	isbn = {978-0-471-57783-6},
	url = {https://www.biblio.com/book/remote-sensing-image-interpretation/d/1536581785},
	urldate = {2023-05-20},
	publisher = {Wiley},
	month = jan,
	year = {1994},
}

@book{santos_introducao_2013,
	title = {Introdução à programação orientada a objetos usando java 2a edição},
	isbn = {978-85-352-8429-4},
	abstract = {Este livro apresenta os conceitos básicos de orientação a objetos (modelos, classes e objetos, atributos e métodos, herança, classes abstratas) e conceitos de programação (estruturas de decisão e controle, arrays, strings e estruturas simples de dados) de forma clara, detalhada e gradativa, com exemplos mais práticos que puramente conceituais, enriquecidos por exercícios de diferentes níveis de complexidade. Esta segunda edição traz novos exemplos; inclusão dos conceitos de enumeradores, laços para iteração em arrays e coleções, argumentos variáveis, argumentos variáveis, tipos genéricos e autoboxing; um índice específico para as listagens, que foram separadas das figuras, entre outras atualizações.},
	language = {pt-BR},
	publisher = {Elsevier Brasil},
	author = {Santos, Rafael},
	month = aug,
	year = {2013},
	note = {Google-Books-ID: 2pfpCgAAQBAJ},
	keywords = {Computers / Programming / General},
}

@book{press_numerical_1996,
	title = {Numerical {Recipes} in {Fortran} 90: {Volume} 2, {Volume} 2 of {Fortran} {Numerical} {Recipes}: {The} {Art} of {Parallel} {Scientific} {Computing}},
	isbn = {978-0-521-57439-6},
	shorttitle = {Numerical {Recipes} in {Fortran} 90},
	abstract = {The second volume of the Fortran Numerical Recipes series, Numerical Recipes in Fortran 90 contains a detailed introduction to the Fortran 90 language and to the basic concepts of parallel programming, plus source code for all routines from the second edition of Numerical Recipes. This volume does not repeat any of the discussion of what individual programs actually do, the mathematical methods they utilize, or how to use them.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Press, William H. and Teukolsky, Saul A. and Vetterling, William T. and Flannery, Brian P.},
	month = sep,
	year = {1996},
	note = {Google-Books-ID: OIZaswEACAAJ},
	keywords = {Mathematics / Numerical Analysis, Mathematics / General},
}

@book{kahaner_numerical_1988,
	title = {Numerical {Methods} and {Software}},
	isbn = {978-0-13-627258-8},
	abstract = {This text recognizes developments in hardware and software and the way engineers and scientists communicate with computers. It discusses topics that are among the most important for engineers and scientists to know, along with applications.},
	language = {en},
	publisher = {Prentice Hall},
	author = {Kahaner, David and Moler, Cleve B. and Nash, Stephen},
	year = {1988},
	note = {Google-Books-ID: jipEAQAAIAAJ},
}

@article{emmanuel_review_2020,
	title = {Review of {Some} {Numerical} {Methods} for {Solving} {Initial} {Value} {Problems} for {Ordinary} {Differential} {Equations}},
	volume = {6},
	issn = {2575-5919},
	url = {http://www.sciencepublishinggroup.com/journal/paperinfo?journalid=322&doi=10.11648/j.ijamtp.20200601.12},
	doi = {10.11648/j.ijamtp.20200601.12},
	abstract = {Numerical analysis is a subject that is concerned with how to solve real life problems numerically. Numerical methods form an important part of solving differential equations emanated from real life situations, most especially in cases where there is no closed-form solution or difficult to obtain exact solutions. The main aim of this paper is to review some numerical methods for solving initial value problems of ordinary differential equations. The comparative study of the Third Order Convergence Numerical Method (FS), Adomian Decomposition Method (ADM) and Successive Approximation Method (SAM) in the context of the exact solution is presented. The methods will be compared in terms of convergence, accuracy and efficiency. Five illustrative examples/test problems were solved successfully. The results obtained show that the three methods are approximately the same in terms of accuracy and convergence in the case of first order linear ordinary differential equations. It is also observed that FS, ADM and SAM were found to be computationally efficient for the linear ordinary differential equations. In the case of the non-linear ordinary differential equations, SAM is found to be more accurate and converges faster to the exact solution than the FS and ADM. Hence, It is clearly seen that the ADM is found to be better than the FS and SAM in the case of non-linear differential equations in terms of computational efficiency.},
	language = {en},
	number = {1},
	urldate = {2023-05-31},
	journal = {IJAMTP},
	author = {Emmanuel, Fadugba Sunday and James, Adebayo Kayode and Nathaniel, Ogunyebi Segun and Temitayo, Okunlola Joseph},
	year = {2020},
	pages = {7},
	file = {Emmanuel et al. - 2020 - Review of Some Numerical Methods for Solving Initi.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/NZ8QHJCF/Emmanuel et al. - 2020 - Review of Some Numerical Methods for Solving Initi.pdf:application/pdf},
}

@article{chasnov_differential_2022,
	title = {Differential {Equations} for {Engineers}},
	url = {https://www.youtube.com/playlist?list=PLkZjai-2JcxlvaV9EUgtHj1KV7THMPw1w},
	language = {en},
	author = {Chasnov, Jeffrey R.},
	year = {2022},
	file = {Chasnov - Differential Equations for Engineers.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/XQ8WW7B8/Chasnov - Differential Equations for Engineers.pdf:application/pdf},
}

@misc{yadav_picards_2020,
	title = {Picard's method. {Numerical} solution of {ODE}.},
	url = {https://www.youtube.com/watch?v=-u39UKYxj3g},
	urldate = {2023-06-07},
	author = {Yadav, Barun Kumar},
	year = {2020},
	file = {Picard's method | Numerical solution of ODE | Part 5 | #barunmaths #picardmethod - YouTube:/home/x/Documents/inpe/library-ml-zotero/storage/WXE67X98/watch.html:text/html},
}

@misc{ricardo_picard_2021,
	title = {Picard {Iteration} - an overview},
	url = {https://www.sciencedirect.com/topics/mathematics/picard-iteration},
	urldate = {2023-06-07},
	author = {Ricardo, Henry J.},
	year = {2021},
	file = {Picard Iteration - an overview | ScienceDirect Topics:/home/x/Documents/inpe/library-ml-zotero/storage/IRAYN5N2/picard-iteration.html:text/html},
}

@book{ricardo_henry_j_modern_nodate,
	title = {A {Modern} {Introduction} to {Differential} {Equations}:},
	isbn = {978-0-12-823417-4},
	url = {https://www.amazon.com/Modern-Introduction-Differential-Equations/dp/0128234172},
	urldate = {2023-06-07},
	author = {{Ricardo, Henry J.}},
	file = {A Modern Introduction to Differential Equations\: Ricardo, Henry J.\: 9780128234174\: Amazon.com\: Books:/home/x/Documents/inpe/library-ml-zotero/storage/RWCACQMS/0128234172.html:text/html},
}

@book{ricardo_modern_2020,
	title = {A {Modern} {Introduction} to {Differential} {Equations}},
	isbn = {978-0-12-818218-5},
	abstract = {A Modern Introduction to Differential Equations, Third Edition, provides an introduction to the basic concepts of differential equations. The book begins by introducing the basic concepts of differential equations, focusing on the analytical, graphical and numerical aspects of first-order equations, including slope fields and phase lines. The comprehensive resource then covers methods of solving second-order homogeneous and nonhomogeneous linear equations with constant coefficients, systems of linear differential equations, the Laplace transform and its applications to the solution of differential equations and systems of differential equations, and systems of nonlinear equations.  Throughout the text, valuable pedagogical features support learning and teaching. Each chapter concludes with a summary of important concepts, and figures and tables are provided to help students visualize or summarize concepts. The book also includes examples and updated exercises drawn from biology, chemistry, and economics, as well as from traditional pure mathematics, physics, and engineering. Offers an accessible and highly readable resource to engage students Introduces qualitative and numerical methods early to build understanding Includes a large number of exercises from biology, chemistry, economics, physics and engineering Provides exercises that are labeled based on difficulty/sophistication and end-of-chapter summaries},
	language = {en},
	publisher = {Academic Press},
	author = {Ricardo, Henry J.},
	month = jan,
	year = {2020},
	note = {Google-Books-ID: QWXRDwAAQBAJ},
	keywords = {Mathematics / Differential Equations / General, Mathematics / Differential Equations / Ordinary},
}

@misc{pestourie_physics-enhanced_2022,
	title = {Physics-enhanced deep surrogates for {PDEs}},
	url = {http://arxiv.org/abs/2111.05841},
	doi = {10.48550/arXiv.2111.05841},
	abstract = {We present a ''physics-enhanced deep-surrogate'' (''PEDS'') approach towards developing fast surrogate models for complex physical systems, which is described by partial differential equations (PDEs) and similar models. Specifically, a unique combination of a low-fidelity, explainable physics simulator and a neural network generator is proposed, which is trained end-to-end to globally match the output of an expensive high-fidelity numerical solver. We consider low-fidelity models derived from coarser discretizations and/or by simplifying the physical equations, which are several orders of magnitude faster than a high-fidelity ''brute-force'' PDE solver. The neural network generates an approximate input, which is adaptively mixed with a downsampled guess and fed into the low-fidelity simulator. In this way, by incorporating the limited physical knowledge from the differentiable low-fidelity model ''layer'', we ensure that the conservation laws and symmetries governing the system are respected by the design of our hybrid system. Experiments on three test problems -- diffusion, reaction-diffusion, and electromagnetic scattering models -- show that a PEDS surrogate can be up to 3\${\textbackslash}times\$ more accurate than a ''black-box'' neural network with limited data (\${\textbackslash}approx 10{\textasciicircum}3\$ training points), and reduces the data needed by at least a factor of 100 for a target error of \$5{\textbackslash}\%\$, comparable to fabrication uncertainty. PEDS even appears to learn with a steeper asymptotic power law than black-box surrogates. In summary, PEDS provides a general, data-driven strategy to bridge the gap between a vast array of simplified physical models with corresponding brute-force numerical solvers, offering accuracy, speed, data efficiency, as well as physical insights into the process.},
	urldate = {2023-06-07},
	publisher = {arXiv},
	author = {Pestourie, Raphaël and Mroueh, Youssef and Rackauckas, Chris and Das, Payel and Johnson, Steven G.},
	month = nov,
	year = {2022},
	note = {arXiv:2111.05841 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Applied Physics},
	file = {arXiv Fulltext PDF:/home/x/Documents/inpe/library-ml-zotero/storage/PG3C33N6/Pestourie et al. - 2022 - Physics-enhanced deep surrogates for PDEs.pdf:application/pdf;arXiv.org Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/743CUBYR/2111.html:text/html},
}

@article{chen_improved_2021,
	title = {An improved data-free surrogate model for solving partial differential equations using deep neural networks},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-99037-x},
	doi = {10.1038/s41598-021-99037-x},
	abstract = {Partial differential equations (PDEs) are ubiquitous in natural science and engineering problems. Traditional discrete methods for solving PDEs are usually time-consuming and labor-intensive due to the need for tedious mesh generation and numerical iterations. Recently, deep neural networks have shown new promise in cost-effective surrogate modeling because of their universal function approximation abilities. In this paper, we borrow the idea from physics-informed neural networks (PINNs) and propose an improved data-free surrogate model, DFS-Net. Specifically, we devise an attention-based neural structure containing a weighting mechanism to alleviate the problem of unstable or inaccurate predictions by PINNs. The proposed DFS-Net takes expanded spatial and temporal coordinates as the input and directly outputs the observables (quantities of interest). It approximates the PDE solution by minimizing the weighted residuals of the governing equations and data-fit terms, where no simulation or measured data are needed. The experimental results demonstrate that DFS-Net offers a good trade-off between accuracy and efficiency. It outperforms the widely used surrogate models in terms of prediction performance on different numerical benchmarks, including the Helmholtz, Klein–Gordon, and Navier–Stokes equations.},
	language = {en},
	number = {1},
	urldate = {2023-06-07},
	journal = {Sci Rep},
	author = {Chen, Xinhai and Chen, Rongliang and Wan, Qian and Xu, Rui and Liu, Jie},
	month = sep,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Applied mathematics, Computational science, Computer science},
	pages = {19507},
	file = {Full Text PDF:/home/x/Documents/inpe/library-ml-zotero/storage/5EZ5Q4YZ/Chen et al. - 2021 - An improved data-free surrogate model for solving .pdf:application/pdf},
}

@misc{jenn_eulers_2016,
	title = {Euler's {Method} - {A} {Simple} {Table} {That} {Works} {Every} {Time} - {YouTube}},
	url = {https://www.youtube.com/watch?v=8cW_CQ77ayI},
	urldate = {2023-06-10},
	author = {{Jenn}},
	year = {2016},
	file = {Euler's Method - A Simple Table That Works Every Time - YouTube:/home/x/Documents/inpe/library-ml-zotero/storage/23MNDWCR/watch.html:text/html},
}

@misc{the_organic_chemistry_tutor_eulers_2017,
	title = {Euler's {Method} {Differential} {Equations}, {Examples}, {Numerical} {Methods}, {Calculus}},
	url = {https://www.youtube.com/watch?v=ukNbG7muKho},
	abstract = {This calculus video tutorial explains how to use euler's method to find the solution to a differential equation.  Euler's method is a numerical method that helps to estimate the y value of a function at some x value given the differential equation or the derivative of a function.  It's similar to the tangent line approximation but with midcourse correction.  This calculus video contains a few examples and practice problems using Euler's method and it has a few graphs to help explain the concept as well.},
	urldate = {2023-06-10},
	author = {{The Organic Chemistry Tutor}},
	month = feb,
	year = {2017},
}

@misc{noauthor_eulers_2014,
	title = {Euler's method. {Differential} equations.},
	url = {https://www.youtube.com/watch?v=q87L9R9v274},
	abstract = {Euler's method is a numerical tool for approximating values for solutions of differential equations. See how (and why) it works.Practice this lesson yourself...},
	language = {en-GB},
	urldate = {2023-06-10},
	year = {2014},
	file = {Snapshot:/home/x/Documents/inpe/library-ml-zotero/storage/UQZF2WBF/watch.html:text/html},
}

@misc{firefly_lectures_eulers_2013,
	title = {Euler's {Method} - {Example} 1},
	url = {https://www.youtube.com/watch?v=PwuZ3nir7d4},
	abstract = {Subscribe on YouTube: http://bit.ly/1bB9ILD

Leave some love on RateMyProfessor: http://bit.ly/1dUTHTw

Send us a comment/like on Facebook: http://on.fb.me/1eWN4Fn},
	urldate = {2023-06-10},
	author = {{Firefly Lectures}},
	month = apr,
	year = {2013},
}

@article{qureshi_discretization_2008,
	title = {Discretization of {Continuous} {Features} by {Resampling}},
	journal = {Proc. 8emes Journees fancophones Extraction et Gestion des Connaissances},
	author = {Qureshi, Taimur and Zighed, D. A.},
	year = {2008},
	file = {Qureshi and Zighed - Discretization of Continuous Features by Resamplin.pdf:/home/x/Documents/inpe/library-ml-zotero/storage/W79PZBR8/Qureshi and Zighed - Discretization of Continuous Features by Resamplin.pdf:application/pdf},
}

@inproceedings{nia_gauss-hermite_2006,
	title = {Gauss-hermite quadrature: {Numerical} or statistical method?},
	shorttitle = {Gauss-hermite quadrature},
	booktitle = {Proc. {Iranian} {Stat}. {Conf}},
	author = {Nia, Vahid Partovi},
	year = {2006},
	pages = {209--215},
	file = {Full Text:/home/x/Documents/inpe/library-ml-zotero/storage/HWZNNI6D/Nia - 2006 - Gauss-hermite quadrature Numerical or statistical.pdf:application/pdf},
}
